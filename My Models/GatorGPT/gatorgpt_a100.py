# -*- coding: utf-8 -*-
"""GatorGPT-A100.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v54xQhFv5awgwMaMzhtDahM4rcsKfk_W

# Data and imports

1. I am checking the GPU that is available to us, usually we go through A100.
2. I install the torchsummary and import it.
3. I am importing bunch of more libraries. Info about it in the cell itself.
4. Loading "" from huggingface datasets and getting both slpits. It is a pretraining corpus so we dont care about order of stories and stuff.
"""

# !nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
# %pip install torchinfo --quiet
from torchinfo import summary # To check params and layers and such

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader # To prepare and use datasets properly

from torch.nn.attention import SDPBackend, sdpa_kernel # To use in flash attention. (Not using as of now, not required)
from torch.amp import autocast # For GPU acceleration

import pandas as pd

import tiktoken # Tokenizer with p50k base vocab. Size - 50281
tok = tiktoken.get_encoding("p50k_base")

from datasets import load_dataset
dataset = load_dataset("roneneldan/TinyStories", split="validation") # 2.2M training stories and 22K Validation stories
data = pd.DataFrame(dataset) # Into a dataframe.

"""# Architecture
1. Rotary Positional Encodings is used for positional knowledge.
2. Grouped Query attention is ussed. 2 KV heads per query head.
3. Multi Layer Network with GeLU and 2.5x more neurons in hidden layer.
4. Transformer block that consist of 1, 2 and 3, as well as RMSNorm and all.
5. Dataset creation.
6. Entire GatorGPT structure !
"""

class Rope(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(Rope, self).__init__()

        # Store the embedding dimension (must be even for proper 2D rotations)
        assert d_model % 2 == 0, "d_mAodel must be even for proper RoPE implementation"
        self.d_model = d_model

        # Store the maximum sequence length this RoPE can handle
        self.max_len = max_len

        # Create position indices [0, 1, 2, ..., max_len-1] and add dimension for broadcasting
            # Shape: (max_len, 1) - each position gets its own row
        self.register_buffer('position_ids', torch.arange(max_len).unsqueeze(1))

        # Create frequency terms for rotation - these determine how fast each dimension pair rotates
            # Uses exponential decay: smaller indices rotate faster, larger indices rotate slower
            # torch.arange(0, d_model, 2) creates [0, 2, 4, 6, ...] (even indices only)
            # The formula creates frequencies: [1/10000^(0/d), 1/10000^(2/d), 1/10000^(4/d), ...]
            # This is equal to (10000 ** (-(2 * i) / d_model) for i in range(d_model // 2))
        self.register_buffer('div_term', torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model)))

    def forward(self, x):
        # Input shape: (batch_size, seq_len, d_model)
        batch_size, seq_len, d_model = x.shape

        # Get position indices for current sequence length (trim to actual sequence length)
            # If input has 100 tokens, this gets positions [0, 1, 2, ..., 99]
        position_ids = self.position_ids[:seq_len]  # Shape: (seq_len, 1)

        # Calculate rotation angles for each position and frequency based on 2017 paper
            # Multiply each position by each frequency term to get rotation angles
            # Shape: (seq_len, d_model//2)
            # This is basically: pos/(10000^(2i/d_model))
        angles = position_ids * self.div_term

        # Calculate sine and cosine values for rotation
            # Shape: (seq_len, d_model//2)
        cos_vals = torch.cos(angles)
        sin_vals = torch.sin(angles)

        # Reshape input to separate even and odd dimensions for 2D rotation
            # Split x into pairs: (x_0, x_1), (x_2, x_3), (x_4, x_5), ...
            # Shape: (batch_size, seq_len, d_model//2, 2)
        x_pairs = x.view(batch_size, seq_len, d_model // 2, 2)

        # Extract even and odd components
            # x_even contains x_0, x_2, x_4, ... (first element of each pair)
            # x_odd contains x_1, x_3, x_5, ... (second element of each pair)
        x_even = x_pairs[..., 0]  # Shape: (batch_size, seq_len, d_model//2)
        x_odd = x_pairs[..., 1]   # Shape: (batch_size, seq_len, d_model//2)

        # Apply 2D rotation to each pair of dimensions
            # Rotation matrix: [[cos, -sin], [sin, cos]]
            # For each pair (x_i, x_{i+1}), compute:
            # x_i' = x_i * cos - x_{i+1} * sin
            # x_{i+1}' = x_i * sin + x_{i+1} * cos
        rotated_even = x_even * cos_vals - x_odd * sin_vals
        rotated_odd = x_even * sin_vals + x_odd * cos_vals

        rotated_pairs = torch.stack([rotated_even, rotated_odd], dim=-1)
        rotated_x = rotated_pairs.view(batch_size, seq_len, d_model)

        return rotated_x

class GQA(nn.Module):
    def __init__(self,
                 d_model: int = 384,
                 n_heads: int = 8,
                 gqa_groups: int = 2,
                 max_len: int = 1024,
                ):
        super().__init__()  # initialize base Module
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"  # validate head split
        assert n_heads % gqa_groups == 0, "n_heads must be divisible by gqa_groups"  # validate grouping

        self.d_model = d_model  # store model dimension
        self.n_heads = n_heads  # store number of query heads
        self.gqa_groups = gqa_groups  # store number of groups for GQA
        self.head_dim = d_model // n_heads  # compute per-head dimension
        self.n_kv_heads = n_heads // gqa_groups  # compute number of K/V heads for GQA
        self.max_len = max_len  # store max sequence length

        # Define bias-free linear projections
        self.q_proj = nn.Linear(d_model, n_heads * self.head_dim, bias=False)  # Q projection: d_model -> H*D
        self.k_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)  # K projection: d_model -> H_kv*D
        self.v_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)  # V projection: d_model -> H_kv*D
        self.o_proj = nn.Linear(n_heads * self.head_dim, d_model, bias=False)  # Output projection: H*D -> d_model

        # Instantiate two RoPE modules with the exact composite dims requested
        self.rope_q = Rope(d_model=n_heads * self.head_dim, max_len=max_len)  # RoPE for Q (expects (B,T,H*D))
        self.rope_k = Rope(d_model=self.n_kv_heads * self.head_dim, max_len=max_len)  # RoPE for K (expects (B,T,H_kv*D))

    def forward(self,
                x: torch.Tensor,  # (B, T, d_model)
                attention_mask: torch.Tensor | None = None  # (B, T) 1=real,0=pad (ignored: no attention bias)
                ) -> torch.Tensor:  # returns (B, T, d_model)
        B, T, C = x.shape  # unpack input shape

        # Linear projections for Q, K, V
        q = self.q_proj(x)  # (B, T, H*D)
        k = self.k_proj(x)  # (B, T, H_kv*D)
        v = self.v_proj(x)  # (B, T, H_kv*D)

        # Apply RoPE to Q over the flattened head dimension
        q = self.rope_q(q)  # (B, T, H*D) with rotary positional encoding applied
        q = q.view(B, T, self.n_heads, self.head_dim).permute(0, 2, 1, 3).contiguous()  # (B, H, T, D)

        # Apply RoPE to K over the flattened head dimension
        k = self.rope_k(k)  # (B, T, H_kv*D) with rotary positional encoding applied
        k = k.view(B, T, self.n_kv_heads, self.head_dim).permute(0, 2, 1, 3).contiguous()  # (B, H_kv, T, D)

        # Reshape V to heads (no RoPE for V)
        v = v.view(B, T, self.n_kv_heads, self.head_dim).permute(0, 2, 1, 3).contiguous()  # (B, H_kv, T, D)

        # Expand K and V from n_kv_heads to n_heads via repeat_interleave on the head axis

        ########################################--Uncomment below code--################################
        expand_factor = self.n_heads // self.n_kv_heads  # compute replication factor
        k = k.repeat_interleave(expand_factor, dim=1)  # (B, H, T, D)
        v = v.repeat_interleave(expand_factor, dim=1)  # (B, H, T, D)
        ################################################################################################
        # Above thing converts [1,2,3,4] -> [1,1,1,2,2,2,3,3,3,4,4,4] when expand_factor is 3 and dim=0

        # GQA will not work in sdpa kernal with forced flash attention.
          # Rather I used expand factor to expand it and using it I did manual GQA and directly using Flash attention.

        # Compute SDPA with purely causal masking (no external attention bias, this uses flash attention
        # Removed sdpa_kernel context and enable_gqa=True to allow torch.compile to find a suitable kernel
        with sdpa_kernel(SDPBackend.FLASH_ATTENTION):
          out = F.scaled_dot_product_attention(
              q,
              k,
              v,
              attn_mask=None,
              is_causal=True,
              enable_gqa=False
          )  # (B, H, T, D)

        # Merge heads back to (B, T, H*D)
        out = out.permute(0, 2, 1, 3).contiguous().view(B, T, self.n_heads * self.head_dim)  # (B, T, d_model)

        # Project to output dimension
        out = self.o_proj(out)  # (B, T, d_model)

        return out  # return attended representations

class MLP(nn.Module):
    """
    SwiGLU MLP for a decoder-only Transformer block.

    - d_model: 384
    - d_ff: ~768 (≈2.5 × d_model)
    - Linear layers are bias-free
    - RMSNorm is applied outside this module
    - Input/Output shape: (batch, seq_len, d_model)
    - BF16-friendly: uses ops that preserve input dtype
    """
    def __init__(self, d_model: int = 384, d_ff: int = 768):
        super().__init__()
        # Fused "up" + "gate" projection to reduce matmuls: d_model -> 2*d_ff
        self.w1 = nn.Linear(d_model, 2 * d_ff, bias=False)
        # Down projection: d_ff -> d_model
        self.w2 = nn.Linear(d_ff, d_model, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, T, d_model)
        up, gate = self.w1(x).chunk(2, dim=-1)  # (B, T, d_ff) each

        # We split in two because SwiGLU works like that and it takes -
            # First half which is content
            # Second half which is how much of content in the context
        x = up * F.silu(gate)                   # SwiGLU: up ⊗ swish(gate)
        x = self.w2(x)                          # (B, T, d_model)
        return x

class Block(nn.Module):
    def __init__(
        self,
        d_model: int = 384,
        n_heads: int = 8,
        gqa_groups: int = 2,
        max_len: int = 1024,
        d_ff: int = 768,
        eps: float = 1e-5,
        dropout_p: float = 0.0,  # keep 0.0 for pretrain
    ):
        super().__init__()
        self.rms1 = nn.modules.normalization.RMSNorm(d_model, eps)
        self.rms2 = nn.modules.normalization.RMSNorm(d_model, eps)

        self.attn = GQA(d_model, n_heads, gqa_groups, max_len)  # should include proj_out
        self.mlp = MLP(d_model, d_ff)  # your SwiGLU MLP (bias-free)

        self.drop_attn = nn.Dropout(dropout_p)
        self.drop_mlp = nn.Dropout(dropout_p)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # pre-rms
        x = x + self.drop_attn(self.attn(self.rms1(x)))
        x = x + self.drop_mlp(self.mlp(self.rms2(x)))
        return x

class GatorGPTDataset(Dataset):
    def __init__(self, txt, tokenizer=tok, max_length=1024, stride=512):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            self.input_ids.append(torch.tensor(token_ids[i:i + max_length]))
            self.target_ids.append(torch.tensor(token_ids[i + 1: i + max_length + 1]))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]

class GatorGPT(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        d_model: int = 384,
        n_heads: int = 8,
        gqa_groups: int = 2,
        max_len: int = 1024,
        d_ff: int = 768,
        eps: float = 1e-5,
        dropout_p: float = 0.0,
        blocks: int = 10,
    ):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, d_model)
        self.unembed = nn.Linear(d_model, vocab_size, bias=False)

        self.final_rms = nn.modules.normalization.RMSNorm(d_model, eps)
        self.unembed.weight = self.embed.weight

        self.blocks = nn.ModuleList(
            [
                Block(
                    d_model=d_model,
                    n_heads=n_heads,
                    gqa_groups=gqa_groups,
                    max_len=max_len,
                    d_ff=d_ff,
                    eps=eps,
                    dropout_p=dropout_p,
                ) for _ in range(blocks)
            ]
        )

    def forward(self, x):
        """
        Forward method that takes in the tokens
        """
      # x: (batch, seq_len) of token ids
        h = self.embed(x)                 # (batch, seq_len, d_model)
        for block in self.blocks:         # run each transformer block
            h = block(h)
        h = self.final_rms(h)
        logits = self.unembed(h)          # (batch, seq_len, vocab_size)
        return logits

"""# Helper Functions

1. Function for creating data loaders.
2. Function for calculating batch loss and loader loss.
3. Generating simple text through autocast.
"""

###################################### TURNS TEXTS TO TOKENS
def text_to_token_ids(text, tokenizer):
  encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
  encoded_tensor = torch.tensor(encoded).unsqueeze(0)
  return encoded_tensor
############################################################################

###################################### TURN TOKENS TO TEXT
def token_ids_to_text(token_ids, tokenizer):
  decoded = tokenizer.decode(token_ids.squeeze(0).tolist())
  return decoded
############################################################################

###################################### CREATES DATA LOADERS
def create_dataloader(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True,
                         num_workers=0): # Changed default to 0

    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("p50k_base")

    # Create dataset
    dataset = GatorGPTDataset(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        drop_last=drop_last,
        num_workers=num_workers # Pass the potentially modified num_workers
    )

    return dataloader
############################################################################

###################################### LOSS FOR ONE BATCH
# Caluclates loss for a batch
def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)

    # For other devices
    # logits = model(input_batch)
    # loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())

    # For A100s - Corrected autocast usage
    with autocast("cuda", torch.bfloat16):
        logits = model(input_batch)
        loss = torch.nn.functional.cross_entropy(
            logits.flatten(0, 1),
            target_batch.flatten()
        )

    return loss
############################################################################

###################################### LOSS FOR ENTIRE LOADER
# Caluculates loss for ENTIRE data_loader which calls calc_loss_batch function inside itself
def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        # Reduce the number of batches to match the total number of batches in the data loader
        # if num_batches exceeds the number of batches in the data loader
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches
############################################################################

###################################### USE THIS TO EVALUATE MODEL DIRECLTY
def evaluate_model(model, train_loader, val_loader, device, eval_iter):
  # basically returns the losses for training and validation
  model.eval()
  with torch.no_grad():
    return calc_loss_loader(train_loader, model, device, eval_iter), calc_loss_loader(val_loader, model, device, eval_iter)
############################################################################

####################################### GENERATING NEW TOKENS
def generate_text_simple(model, idx, max_new_tokens, context_size):

  # idx is (batch, n_tokens) array of indices in current context
  for _ in range(max_new_tokens):
    # If LLM suports only 5 tokens, and the context size is 10, then we only use last 5 toens as context.
    idx_cond = idx[:, -context_size:]

    # Gettings the predictions
    with torch.no_grad():
      # Reshape idx_cond to (batch_size, sequence_length, emb_dim)
      # idx_cond = idx_cond.unsqueeze(-1).repeat(1 , 1, model.norm1.scale.shape[0]) # Or model.att.d_in to get the embedding dimension
      with autocast("cuda", torch.bfloat16):
        logits = model(idx_cond)

    # We take the last row. We dont do anything to the batches neither to the last dimension of the vocabularies, but take the last row
    logits = logits[:, -1, :] # (batch, vocab_size)

    # getting probablities from the logits. We can say something like 50% chances of this, 2% chances of this...
    probs = torch.softmax(logits, dim=-1) # (batch, vocab_size)

    # We see the highest value's index
    idx_next = torch.argmax(probs, dim=-1, keepdim=True) # (batch, 1)

    # Append the predicted token_id generated to the original index
    idx = torch.cat((idx, idx_next), dim=1) # (batch, num_tokens+1)

  return idx
############################################################################

###################################### GENERATING AND PRINTING SAMPLES
def generate_and_print_sample(model, tokenizer, device, start_context, context_size):
  # we print out, what the model is generating right now at the end of each epoch. Also, we print 50 items!
  model.eval()
  encoded = text_to_token_ids(start_context, tokenizer).to(device)

  with torch.no_grad():
    token_ids = generate_text_simple(
        model, encoded, 50, context_size
    )

  decoded = token_ids_to_text(token_ids, tokenizer)
  print(decoded.replace("\n", " "))
  model.train()
############################################################################

"""# Training loop

1. Function for training loop.
2. Preparing for training loop.
3. **Training**.
4. Inference example.
"""

from typing import Dict, List, Tuple, Any

def train_model_simple(
    model,
    train_loader,
    val_loader,
    optimizer,
    device,
    tokenizer,
    start_context: str,
    cfg: Dict[str, Any],
) -> Tuple[List[float], List[float], List[int]]:
    """
    Numeric hyperparameters expected in cfg:
      - "num_epochs": int
      - "eval_freq": int
      - "eval_iter": int
      - "patience": int
    Optional numeric params:
      - "sample_tokens": int (default 1024)
      - "progress_chunks": int (how many intra-epoch progress prints; default 5)
    """
    import time

    # Pull required numeric params from cfg (hard fail if missing)
    num_epochs   = cfg["num_epochs"]
    eval_freq    = cfg["eval_freq"]
    eval_iter    = cfg["eval_iter"]
    PATIENCE     = cfg["patience"]
    sample_tokens = cfg["sample_tokens"]
    progress_chunks = max(1, int(cfg["progress_chunks"]))

    # Tracking
    train_losses, val_losses, track_tokens_seen = [], [], []
    tokens_seen, global_step = 0, -1
    best_val_loss = float('inf')
    patience_counter = 0

    # Dataset info
    t0 = time.time()
    print("📊 Dataset info:")
    print(f"   Training batches: {len(train_loader)}")
    print(f"   Validation batches: {len(val_loader)}")
    print(f"   Total steps per epoch: {len(train_loader)}")
    print(f"   Evaluation every {eval_freq} steps")
    print("="*50)
    print(f"🕒 Printed above in {time.time() - t0:.3f}s")

    for epoch in range(num_epochs):
        t0 = time.time()
        print(f"🔁 Starting epoch {epoch+1}/{num_epochs}")
        print("-"*30)
        print(f"🕒 Printed above in {time.time() - t0:.3f}s")
        model.train()

        epoch_steps = 0
        epoch_loss = 0.0

        # how often to log intra-epoch progress (in steps)
        progress_every = max(1, len(train_loader) // progress_chunks)

        for input_batch, target_batch in train_loader:
            if patience_counter >= PATIENCE:
                break

            optimizer.zero_grad()
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()
            optimizer.step()

            tokens_seen += input_batch.numel()
            global_step += 1
            epoch_steps += 1
            epoch_loss += loss.item()

            # In-epoch progress
            if epoch_steps % progress_every == 0:
                t0 = time.time()
                avg_loss = epoch_loss / epoch_steps
                progress = epoch_steps / len(train_loader) * 100
                print(
                    f"   Step {epoch_steps}/{len(train_loader)} ({progress:.1f}%) - "
                    f"Current loss: {loss.item():.3f}, Avg loss: {avg_loss:.3f}, "
                    f"patience level: {patience_counter}/{PATIENCE}"
                )
                print(f"🕒 Printed above in {time.time() - t0:.3f}s")

            # Periodic evaluation
            if global_step % eval_freq == 0:
                t0 = time.time()
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter
                )
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                track_tokens_seen.append(tokens_seen)

                improvement = "✅" if val_loss < best_val_loss else "❌"
                print(
                    f"   Eval at step {global_step}: Train {train_loss:.3f}, "
                    f"Val {val_loss:.3f} {improvement}, "
                    f"patience level: {patience_counter}/{PATIENCE}"
                )
                print(f"🕒 Printed above in {time.time() - t0:.3f}s")

                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1

        # End of epoch
        t0 = time.time()
        avg_epoch_loss = epoch_loss / epoch_steps if epoch_steps > 0 else float('inf')
        print(f"✅ Epoch {epoch+1} complete: {epoch_steps} steps, avg loss: {avg_epoch_loss:.3f}")
        print("🎯 Generated sample:")
        generate_and_print_sample(model, tokenizer, device, start_context, sample_tokens)
        print("="*50)
        print(f"🕒 Printed above in {time.time() - t0:.3f}s")

        if patience_counter >= PATIENCE:
            print(f"🛑 Early stopping triggered at epoch {epoch+1} with patience {PATIENCE}.")
            break

    t0 = time.time()
    print(f"🏁 Training complete! Best validation loss: {best_val_loss:.3f}")
    print(f"🕒 Printed above in {time.time() - t0:.3f}s")
    return train_losses, val_losses, track_tokens_seen

device = "cuda" if torch.cuda.is_available() else "cpu"

model = GatorGPT(vocab_size=tok.n_vocab)
# To cuda, compiling before running
model = model.to(device)
model = torch.compile(model, fullgraph=True, mode="reduce-overhead")

optim = torch.optim.AdamW(model.parameters(), lr=0.001, eps=1e-08, weight_decay=0.01)

cfg = {
    "num_epochs": 10,
    "eval_freq": 200,
    "eval_iter": 200,
    "patience": 3,
    "sample_tokens": 1024,
    "progress_chunks": 5,
}

CONTEXT = "We ran across a field that was"

train_text = "\n".join(data[:int(0.9*len(data))]['text'])
val_text = "\n".join(data[int(0.9*len(data)):]['text'])

train_loader = create_dataloader(
    train_text,
    batch_size=8,
    max_length=256,
    stride=64,
    shuffle=True,
    drop_last=True,
    num_workers=0
)

val_loader = create_dataloader(
    val_text,
    batch_size=8,
    max_length=256,
    stride=64,
    shuffle=True,
    drop_last=True,
    num_workers=0
)

train_losses, val_losses, tokens_seen = train_model_simple(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    optimizer=optim,
    device=device,
    tokenizer=tok,
    start_context=CONTEXT,
    cfg=cfg,
)

token_ids_to_text(generate_text_simple(model, text_to_token_ids("Fascinated by the black", tok).to(device), 50, 1024), tok).replace("\n", " ").replace("\r\\", "").replace("\r", "")

"""# Debugging

1. Inspecting every layer and dtype there are.
"""

def inspect_everything(model, batch, device="cuda",
                       expect_dtype=torch.bfloat16,
                       prints_per_module: int = 1):
    """
    One-call tracer for dtype/shape/device across the whole model.
    - Works with batch as (inputs, targets), list/tuple, dict, or single tensor.
    - Logs first seen [IN]/[OUT] per module (limited by `prints_per_module`).
    - Enforces `expect_dtype` **only for floating-point tensors** (so int64 token IDs are fine).
    - Runs under torch.amp.autocast with bf16 by default.
    - Enables all SDP backends (incl. math) as a safety net.
    """
    import torch
    from contextlib import contextmanager

    def _first_tensor(x):
        if isinstance(x, torch.Tensor): return x
        if isinstance(x, (list, tuple)):
            for t in x:
                if isinstance(t, torch.Tensor): return t
        if isinstance(x, dict):
            for v in x.values():
                if isinstance(v, torch.Tensor): return v
        return None

    def _to_device(x, dev):
        if isinstance(x, torch.Tensor): return x.to(dev)
        if isinstance(x, (list, tuple)):
            seq = [t.to(dev) if isinstance(t, torch.Tensor) else t for t in x]
            return type(x)(seq) if isinstance(x, tuple) else seq
        if isinstance(x, dict):
            return {k: (v.to(dev) if isinstance(v, torch.Tensor) else v) for k, v in x.items()}
        return x

    def _extract_inputs(batch_like):
        if isinstance(batch_like, (list, tuple)) and len(batch_like) >= 1:
            return batch_like[0]
        return batch_like

    @contextmanager
    def _sdp_safety():
        prev = (
            torch.backends.cuda.flash_sdp_enabled(),
            torch.backends.cuda.mem_efficient_sdp_enabled(),
            torch.backends.cuda.math_sdp_enabled(),
        )
        torch.backends.cuda.enable_flash_sdp(True)
        torch.backends.cuda.enable_mem_efficient_sdp(True)
        torch.backends.cuda.enable_math_sdp(True)
        try:
            yield
        finally:
            f, m, a = prev
            torch.backends.cuda.enable_flash_sdp(f)
            torch.backends.cuda.enable_mem_efficient_sdp(m)
            torch.backends.cuda.enable_math_sdp(a)

    # Parameter/buffer scan (floating only)
    bad = []
    for n, p in model.named_parameters():
        if p.dtype.is_floating_point and p.dtype != expect_dtype:
            bad.append(("param", n, p.dtype))
    for n, b in model.named_buffers():
        if b.dtype.is_floating_point and b.dtype != expect_dtype:
            bad.append(("buffer", n, b.dtype))
    if bad:
        print("⚠️ Nonconforming floating params/buffers:")
        for kind, n, dt in bad:
            print(f"   - {kind:6s} {n}: {dt}")

    handles, counts = [], {}

    def _hook(name):
        def fn(mod, inp, out):
            c = counts.get(name, 0)
            if c >= prints_per_module: return
            counts[name] = c + 1

            ti = _first_tensor(inp if isinstance(inp, tuple) else (inp,))
            to = _first_tensor(out)

            if ti is not None:
                print(f"[IN ] {name:<48} dtype={ti.dtype} shape={tuple(ti.shape)} device={ti.device}")
                if ti.dtype.is_floating_point and ti.dtype != expect_dtype:
                    raise RuntimeError(f"[IN ] {name} expected {expect_dtype} for floating tensors, got {ti.dtype}")
            if to is not None:
                print(f"[OUT] {name:<48} dtype={to.dtype} shape={tuple(to.shape)} device={to.device}")
                if to.dtype.is_floating_point and to.dtype != expect_dtype:
                    raise RuntimeError(f"[OUT] {name} expected {expect_dtype} for floating tensors, got {to.dtype}")
        return fn

    for n, m in model.named_modules():
        if n == "":  # skip root
            continue
        handles.append(m.register_forward_hook(_hook(n)))

    try:
        model = model.to(device)
        batch = _to_device(batch, device)
        inputs = _extract_inputs(batch)

        print("SDP backends -> flash:",
              torch.backends.cuda.flash_sdp_enabled(),
              "mem_efficient:",
              torch.backends.cuda.mem_efficient_sdp_enabled(),
              "math:",
              torch.backends.cuda.math_sdp_enabled())

        from torch.amp import autocast
        with _sdp_safety(), autocast("cuda", expect_dtype):
            if isinstance(inputs, dict):
                _ = model(**inputs)
            elif isinstance(inputs, (list, tuple)):
                _ = model(*inputs)
            else:
                _ = model(inputs)
    finally:
        for h in handles:
            h.remove()

sample = next(iter(train_loader))  # yields (input_ids, target_ids)
model = GatorGPT(50257).to(device).to(torch.bfloat16)
inspect_everything(model, sample, device=device, expect_dtype=torch.bfloat16)