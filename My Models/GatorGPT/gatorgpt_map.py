# -*- coding: utf-8 -*-
"""GatorGPT_map.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qbvcONSrx5-qIMJ0cNIAgWMp6WZ5K0bh

# Data and imports

1. I am checking the GPU that is available to us, usually we go through A100.
2. I install the torchsummary and import it.
3. I am importing bunch of more libraries. Info about it in the cell itself.
4. Loading **roneneldan/TinyStories** from huggingface datasets and getting both slpits. It is a pretraining corpus so we dont care about order of stories and stuff.
"""

!nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
# %pip install torchinfo --quiet
from torchinfo import summary # To check params and layers and such

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader # To prepare and use datasets properly

from torch.nn.attention import SDPBackend, sdpa_kernel # To use in flash attention. (Not using as of now, not required)
from torch.amp import autocast # For GPU acceleration

import pandas as pd
import numpy as np
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp

import tiktoken # Tokenizer with p50k base vocab. Size - 50281
tok = tiktoken.get_encoding("p50k_base")

import wandb

from datasets import load_dataset
dataset = load_dataset("roneneldan/TinyStories", split="train") # 2.2M training stories

TRAINING_CUTOFF=100_00
data = pd.DataFrame(dataset)[:TRAINING_CUTOFF] # Into a dataframe.

"""# Architecture
1. Rotary Positional Encodings is used for positional knowledge.
2. Grouped Query attention is ussed. 2 KV heads per query head.
3. Multi Layer Network with GeLU and 2x more neurons in hidden layer.
4. Transformer block that consist of 1, 2 and 3, as well as RMSNorm and all.
5. Dataset creation.
6. Entire GatorGPT structure !
"""

class Rope(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(Rope, self).__init__()

        # Store the embedding dimension (must be even for proper 2D rotations)
        assert d_model % 2 == 0, "d_mAodel must be even for proper RoPE implementation"
        self.d_model = d_model

        # Store the maximum sequence length this RoPE can handle
        self.max_len = max_len

        # Create position indices [0, 1, 2, ..., max_len-1] and add dimension for broadcasting
            # Shape: (max_len, 1) - each position gets its own row
        self.register_buffer('position_ids', torch.arange(max_len).unsqueeze(1))

        # Create frequency terms for rotation - these determine how fast each dimension pair rotates
            # Uses exponential decay: smaller indices rotate faster, larger indices rotate slower
            # torch.arange(0, d_model, 2) creates [0, 2, 4, 6, ...] (even indices only)
            # The formula creates frequencies: [1/10000^(0/d), 1/10000^(2/d), 1/10000^(4/d), ...]
            # This is equal to (10000 ** (-(2 * i) / d_model) for i in range(d_model // 2))
        self.register_buffer('div_term', torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model)))

    def forward(self, x):
        # Input shape: (batch_size, seq_len, d_model)
        batch_size, seq_len, d_model = x.shape

        # Get position indices for current sequence length (trim to actual sequence length)
            # If input has 100 tokens, this gets positions [0, 1, 2, ..., 99]
        position_ids = self.position_ids[:seq_len]  # Shape: (seq_len, 1)

        # Calculate rotation angles for each position and frequency based on 2017 paper
            # Multiply each position by each frequency term to get rotation angles
            # Shape: (seq_len, d_model//2)
            # This is basically: pos/(10000^(2i/d_model))
        angles = position_ids * self.div_term

        # Calculate sine and cosine values for rotation
            # Shape: (seq_len, d_model//2)
        cos_vals = torch.cos(angles)
        sin_vals = torch.sin(angles)

        # Reshape input to separate even and odd dimensions for 2D rotation
            # Split x into pairs: (x_0, x_1), (x_2, x_3), (x_4, x_5), ...
            # Shape: (batch_size, seq_len, d_model//2, 2)
        x_pairs = x.view(batch_size, seq_len, d_model // 2, 2)

        # Extract even and odd components
            # x_even contains x_0, x_2, x_4, ... (first element of each pair)
            # x_odd contains x_1, x_3, x_5, ... (second element of each pair)
        x_even = x_pairs[..., 0]  # Shape: (batch_size, seq_len, d_model//2)
        x_odd = x_pairs[..., 1]   # Shape: (batch_size, seq_len, d_model//2)

        # Apply 2D rotation to each pair of dimensions
            # Rotation matrix: [[cos, -sin], [sin, cos]]
            # For each pair (x_i, x_{i+1}), compute:
            # x_i' = x_i * cos - x_{i+1} * sin
            # x_{i+1}' = x_i * sin + x_{i+1} * cos
        rotated_even = x_even * cos_vals - x_odd * sin_vals
        rotated_odd = x_even * sin_vals + x_odd * cos_vals

        rotated_pairs = torch.stack([rotated_even, rotated_odd], dim=-1)
        rotated_x = rotated_pairs.view(batch_size, seq_len, d_model)

        return rotated_x

class GQA(nn.Module):
    def __init__(self,
                 d_model: int = 384,
                 n_heads: int = 8,
                 gqa_groups: int = 2,
                 max_len: int = 1024,
                ):
        super().__init__()  # initialize base Module
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"  # validate head split
        assert n_heads % gqa_groups == 0, "n_heads must be divisible by gqa_groups"  # validate grouping

        self.d_model = d_model  # store model dimension
        self.n_heads = n_heads  # store number of query heads
        self.gqa_groups = gqa_groups  # store number of groups for GQA
        self.head_dim = d_model // n_heads  # compute per-head dimension
        self.n_kv_heads = n_heads // gqa_groups  # compute number of K/V heads for GQA
        self.max_len = max_len  # store max sequence length

        # Define bias-free linear projections
        self.q_proj = nn.Linear(d_model, n_heads * self.head_dim, bias=False)  # Q projection: d_model -> H*D
        self.k_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)  # K projection: d_model -> H_kv*D
        self.v_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)  # V projection: d_model -> H_kv*D
        self.o_proj = nn.Linear(n_heads * self.head_dim, d_model, bias=False)  # Output projection: H*D -> d_model

        # Instantiate two RoPE modules with the exact composite dims requested
        self.rope_q = Rope(d_model=n_heads * self.head_dim, max_len=max_len)  # RoPE for Q (expects (B,T,H*D))
        self.rope_k = Rope(d_model=self.n_kv_heads * self.head_dim, max_len=max_len)  # RoPE for K (expects (B,T,H_kv*D))

    def forward(self,
                x: torch.Tensor,  # (B, T, d_model)
                attention_mask: torch.Tensor | None = None  # (B, T) 1=real,0=pad (ignored: no attention bias)
                ) -> torch.Tensor:  # returns (B, T, d_model)
        B, T, C = x.shape  # unpack input shape

        # Linear projections for Q, K, V
        q = self.q_proj(x)  # (B, T, H*D)
        k = self.k_proj(x)  # (B, T, H_kv*D)
        v = self.v_proj(x)  # (B, T, H_kv*D)

        # Apply RoPE to Q over the flattened head dimension
        q = self.rope_q(q)  # (B, T, H*D) with rotary positional encoding applied
        q = q.view(B, T, self.n_heads, self.head_dim).permute(0, 2, 1, 3).contiguous()  # (B, H, T, D)

        # Apply RoPE to K over the flattened head dimension
        k = self.rope_k(k)  # (B, T, H_kv*D) with rotary positional encoding applied
        k = k.view(B, T, self.n_kv_heads, self.head_dim).permute(0, 2, 1, 3).contiguous()  # (B, H_kv, T, D)

        # Reshape V to heads (no RoPE for V)
        v = v.view(B, T, self.n_kv_heads, self.head_dim).permute(0, 2, 1, 3).contiguous()  # (B, H_kv, T, D)

        # Expand K and V from n_kv_heads to n_heads via repeat_interleave on the head axis

        ########################################--Uncomment below code--################################
        expand_factor = self.n_heads // self.n_kv_heads  # compute replication factor
        k = k.repeat_interleave(expand_factor, dim=1)  # (B, H, T, D)
        v = v.repeat_interleave(expand_factor, dim=1)  # (B, H, T, D)
        ################################################################################################
        # Above thing converts [1,2,3,4] -> [1,1,1,2,2,2,3,3,3,4,4,4] when expand_factor is 3 and dim=0

        # GQA will not work in sdpa kernal with forced flash attention.
          # Rather I used expand factor to expand it and using it I did manual GQA and directly using Flash attention.

        # Compute SDPA with purely causal masking (no external attention bias, this uses flash attention
        # Removed sdpa_kernel context and enable_gqa=True to allow torch.compile to find a suitable kernel
        with sdpa_kernel(SDPBackend.FLASH_ATTENTION):
          out = F.scaled_dot_product_attention(
              q,
              k,
              v,
              attn_mask=None,
              is_causal=True,
              enable_gqa=False
          )  # (B, H, T, D)

        # Merge heads back to (B, T, H*D)
        out = out.permute(0, 2, 1, 3).contiguous().view(B, T, self.n_heads * self.head_dim)  # (B, T, d_model)

        # Project to output dimension
        out = self.o_proj(out)  # (B, T, d_model)

        return out  # return attended representations

class MLP(nn.Module):
    """
    SwiGLU MLP for a decoder-only Transformer block.

    - d_model: 384
    - d_ff: ~768 (â‰ˆ2.5 Ã— d_model)
    - Linear layers are bias-free
    - RMSNorm is applied outside this module
    - Input/Output shape: (batch, seq_len, d_model)
    - BF16-friendly: uses ops that preserve input dtype
    """
    def __init__(self, d_model: int = 384, d_ff: int = 768):
        super().__init__()
        # Fused "up" + "gate" projection to reduce matmuls: d_model -> 2*d_ff
        self.w1 = nn.Linear(d_model, 2 * d_ff, bias=False)
        # Down projection: d_ff -> d_model
        self.w2 = nn.Linear(d_ff, d_model, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, T, d_model)
        up, gate = self.w1(x).chunk(2, dim=-1)  # (B, T, d_ff) each

        # We split in two because SwiGLU works like that and it takes -
            # First half which is content
            # Second half which is how much of content in the context
        x = up * F.silu(gate)                   # SwiGLU: up âŠ— swish(gate)
        x = self.w2(x)                          # (B, T, d_model)
        return x

class Block(nn.Module):
    def __init__(
        self,
        d_model: int = 384,
        n_heads: int = 8,
        gqa_groups: int = 2,
        max_len: int = 1024,
        d_ff: int = 768,
        eps: float = 1e-5,
        dropout_p: float = 0.0,  # keep 0.0 for pretrain
    ):
        super().__init__()
        self.rms1 = nn.modules.normalization.RMSNorm(d_model, eps)
        self.rms2 = nn.modules.normalization.RMSNorm(d_model, eps)

        self.attn = GQA(d_model, n_heads, gqa_groups, max_len)  # should include proj_out
        self.mlp = MLP(d_model, d_ff)  # your SwiGLU MLP (bias-free)

        self.drop_attn = nn.Dropout(dropout_p)
        self.drop_mlp = nn.Dropout(dropout_p)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # pre-rms
        x = x + self.drop_attn(self.attn(self.rms1(x)))
        x = x + self.drop_mlp(self.mlp(self.rms2(x)))
        return x

class FastDataset(Dataset):
    """Pre-computed sliding windows, numpy arrays for speed"""
    def __init__(self, tokens, max_length=256, stride=128):
        # Convert to numpy for faster slicing
        self.tokens = np.array(tokens, dtype=np.int32)
        self.max_length = max_length

        # Pre-compute all valid starts
        self.starts = np.arange(0, len(tokens) - max_length, stride)

    def __len__(self):
        return len(self.starts)

    def __getitem__(self, idx):
        start = self.starts[idx]
        end = start + self.max_length

        input_ids = torch.from_numpy(self.tokens[start:end].copy()).long()
        target_ids = torch.from_numpy(self.tokens[start+1:end+1].copy()).long()

        return input_ids, target_ids

class GatorGPT(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        d_model: int = 384,
        n_heads: int = 8,
        gqa_groups: int = 2,
        max_len: int = 1024,
        d_ff: int = 768,
        eps: float = 1e-5,
        dropout_p: float = 0.0,
        blocks: int = 10,
    ):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, d_model)
        self.unembed = nn.Linear(d_model, vocab_size, bias=False)

        self.final_rms = nn.modules.normalization.RMSNorm(d_model, eps)
        self.unembed.weight = self.embed.weight

        self.blocks = nn.ModuleList(
            [
                Block(
                    d_model=d_model,
                    n_heads=n_heads,
                    gqa_groups=gqa_groups,
                    max_len=max_len,
                    d_ff=d_ff,
                    eps=eps,
                    dropout_p=dropout_p,
                ) for _ in range(blocks)
            ]
        )

    def forward(self, x):
        """
        Forward method that takes in the tokens
        """
      # x: (batch, seq_len) of token ids
        h = self.embed(x)                 # (batch, seq_len, d_model)
        for block in self.blocks:         # run each transformer block
            h = block(h)
        h = self.final_rms(h)
        logits = self.unembed(h)          # (batch, seq_len, vocab_size)
        return logits

"""# Helper Functions

1. Function for creating data loaders.
2. Function for calculating batch loss and loader loss.
3. Generating simple text through autocast.
"""

###################################### TURNS TEXTS TO TOKENS
def text_to_token_ids(text, tokenizer):
  encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
  encoded_tensor = torch.tensor(encoded).unsqueeze(0)
  return encoded_tensor
############################################################################

###################################### TURN TOKENS TO TEXT
def token_ids_to_text(token_ids, tokenizer):
  decoded = tokenizer.decode(token_ids.squeeze(0).tolist())
  return decoded
############################################################################

###################################### CREATES DATA LOADERS
def create_fast_dataloader(tokens, batch_size=8, max_length=256, stride=128, shuffle=True):
    """Fast dataloader with proper A100 settings"""
    dataset = FastDataset(tokens, max_length, stride)

    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        drop_last=True,
        num_workers=4,  # Use CPU cores
        pin_memory=True,  # Faster GPU transfer
        prefetch_factor=2,
        persistent_workers=True
    )

############################################################################

###################################### LOSS FOR ONE BATCH
# Caluclates loss for a batch
def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device, non_blocking=True), target_batch.to(device, non_blocking=True)

    # For other devices
    # logits = model(input_batch)
    # loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())

    # For A100s - Corrected autocast usage
    with autocast("cuda", torch.bfloat16):
        logits = model(input_batch)
        loss = torch.nn.functional.cross_entropy(
            logits.flatten(0, 1),
            target_batch.flatten()
        )

    return loss
############################################################################

###################################### LOSS FOR ENTIRE LOADER
# Caluculates loss for ENTIRE data_loader which calls calc_loss_batch function inside itself
def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        # Reduce the number of batches to match the total number of batches in the data loader
        # if num_batches exceeds the number of batches in the data loader
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches
############################################################################

###################################### USE THIS TO EVALUATE MODEL DIRECLTY
def evaluate_model(model, val_loader, device, eval_iter):
  # basically returns the losses for training and validation
  model.eval()
  with torch.no_grad():
    return calc_loss_loader(val_loader, model, device, eval_iter)
############################################################################

####################################### GENERATING NEW TOKENS
def generate_text_simple(model, idx, max_new_tokens, context_size, top_k=5, temperature=0.75):
  for _ in range(max_new_tokens):
    idx_cond = idx[:, -context_size:]
    with torch.no_grad():
      with autocast("cuda", torch.bfloat16):
        logits = model(idx_cond)
    logits = logits[:, -1, :] / temperature
    topk_vals, topk_idx = torch.topk(logits, k=top_k, dim=-1)
    probs = torch.softmax(topk_vals, dim=-1)
    idx_next_local = torch.multinomial(probs, num_samples=1)
    idx_next = torch.gather(topk_idx, -1, idx_next_local)
    idx = torch.cat((idx, idx_next), dim=1)
  return idx
############################################################################

###################################### GENERATING AND PRINTING SAMPLES
def generate_and_print_sample(model, tokenizer, device, start_context, context_size, top_k=5, temperature=0.75):
  model.eval()
  encoded = text_to_token_ids(start_context, tokenizer).to(device)
  with torch.no_grad():
    token_ids = generate_text_simple(
        model, encoded, 50, context_size, top_k=top_k, temperature=temperature
    )
  decoded = token_ids_to_text(token_ids, tokenizer)
  print(decoded.replace("\n", " "))
  model.train()
############################################################################

########################################################### Tokenizing entire batch
def tokenize_batch(text_batch):
    """Tokenize a batch of texts"""
    return tok.encode("\n\n".join(text_batch), allowed_special={"<|endoftext|>"})
############################################################################

####################################################### Preparing the dataset efficiently
def fast_prepare_data(data, train_split=0.95, max_workers=None):
    """Pre-tokenize everything in parallel, avoid giant string joins"""
    if max_workers is None:
        max_workers = min(mp.cpu_count(), 8)

    split_idx = int(train_split * len(data))
    train_texts = data[:split_idx]['text']
    val_texts = data[split_idx:]['text']

    # Split into chunks for parallel processing
    chunk_size = 1000  # Process 1000 stories at a time

    def process_split(texts):
        chunks = [texts[i:i+chunk_size] for i in range(0, len(texts), chunk_size)]

        all_tokens = []
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            token_chunks = list(executor.map(tokenize_batch, chunks))

        # Flatten
        for chunk in token_chunks:
            all_tokens.extend(chunk)
        return all_tokens

    print("ðŸ”¥ Tokenizing training data...")
    train_tokens = process_split(train_texts)
    print("ðŸ”¥ Tokenizing validation data...")
    val_tokens = process_split(val_texts)

    return train_tokens, val_tokens
############################################################################

"""# Training loop

1. Function for training loop.
2. Preparing for training loop.
3. **Training**.
4. Inference example.
"""

from typing import Dict, List, Tuple, Any

def train_model_simple(
    model,
    train_loader,
    val_loader,
    optimizer,
    device,
    tokenizer,
    start_context: str,
    cfg: Dict[str, Any],
) -> Tuple[List[float], List[float], List[int]]:
    """
    Numeric hyperparameters expected in cfg:
      - "num_epochs": int
      - "eval_freq": int
      - "eval_iter": int
      - "patience": int
    Optional numeric params:
      - "sample_tokens": int (default 1024)
      - "progress_chunks": int (how many intra-epoch progress prints; default 5)
    """

    # Pull required numeric params from cfg (hard fail if missing)
    num_epochs   = cfg["num_epochs"]
    eval_freq    = cfg["eval_freq"]
    eval_iter    = cfg["eval_iter"]
    PATIENCE     = cfg["patience"]
    sample_tokens = cfg["sample_tokens"]
    progress_chunks = max(1, int(cfg["progress_chunks"]))

    # Training start notification
    print("ðŸš€ TRAINING STARTED!")
    print(f"ðŸ“Š Configuration: {num_epochs} epochs, {len(train_loader)} batches")
    print(f"ðŸŽ¯ Evaluation every {eval_freq} steps, patience: {PATIENCE}")
    print(f"ðŸ”® Sample tokens: {sample_tokens}")
    print(f"ðŸ“ˆ Progress logging every {progress_chunks} epochs")
    print(f"ðŸ”® Device: {device}")
    print("-" * 60)

    # Tracking
    train_losses, val_losses, track_tokens_seen = [], [], []
    tokens_seen = 0
    global_step = 0
    best_val_loss = float('inf')
    patience_counter = 0

    # Progress logging frequency (only if progress_chunks > 0)
    # How often to print progress during each epoch (0 = no progress prints)
    progress_every = max(1, len(train_loader) // progress_chunks) if progress_chunks > 0 else 0

    try:
        for epoch in range(num_epochs):
            model.train()
            epoch_steps = 0  # Batches processed in current epoch (resets each epoch)
            epoch_loss = 0.0

            for input_batch, target_batch in train_loader:
                if patience_counter >= PATIENCE:
                    break

                optimizer.zero_grad()
                loss = calc_loss_batch(input_batch, target_batch, model, device)
                loss.backward()
                optimizer.step()
                train_losses.append(loss.item())

                tokens_seen += input_batch.numel()
                global_step += 1  # Increment total step counter (never resets)
                epoch_steps += 1  # Increment current epoch step counter
                epoch_loss += loss.item()

                # Optional minimal progress logging
                if progress_every > 0 and epoch_steps % progress_every == 0:
                    avg_loss = epoch_loss / epoch_steps
                    progress = epoch_steps / len(train_loader) * 100
                    print(f"Epoch {epoch+1}/{num_epochs} - Step {epoch_steps}/{len(train_loader)} ({progress:.1f}%) - Loss: {avg_loss:.3f}")

                # Periodic evaluation
                if epoch_steps % eval_freq == 0:
                    val_loss = evaluate_model(
                        model, val_loader, device, eval_iter
                    )
                    val_losses.append(val_loss)
                    track_tokens_seen.append(tokens_seen)

                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        patience_counter = 0
                        print(f"Step {global_step}: Val loss improved to {val_loss:.3f}")
                    else:
                        patience_counter += 1

                    wandb.log({
                        "train_loss": loss.item(),
                        "val_loss": val_loss,
                        "tokens_seen": tokens_seen,
                    })

            # End of epoch - minimal output
            if epoch_steps > 0:
                avg_epoch_loss = epoch_loss / epoch_steps
                print(f"Epoch {epoch+1}/{num_epochs} complete - Avg loss: {avg_epoch_loss:.3f} - Best val: {best_val_loss:.3f}")

        print(f"ðŸŽ‰ Training complete! Best validation loss: {best_val_loss:.3f}")

    except KeyboardInterrupt:
        print(f"\nâš ï¸  Training interrupted by user at epoch {epoch+1}, step {epoch_steps}")
        print(f"ðŸ”„ Returning results collected so far...")
        print(f"ðŸ“Š Collected {len(train_losses)} training losses, {len(val_losses)} validation losses")
        print(f"ðŸ† Best validation loss so far: {best_val_loss:.3f}")

    return train_losses, val_losses, track_tokens_seen

device = "cuda" if torch.cuda.is_available() else "cpu"

model = GatorGPT(
    vocab_size=tok.n_vocab,
    d_model=448,
    n_heads=8,
    gqa_groups=2,
    max_len=1024,
    d_ff=896,
    eps=1e-5,
    dropout_p=0.1,
    blocks=10
)
model = model.to(device) # To cuda, compiling before running
model = torch.compile(model, fullgraph=True, mode="reduce-overhead")

optim = torch.optim.AdamW(model.parameters(), lr=0.01, eps=1e-08, weight_decay=0.01)

# wandb.init(project='GatorGPT', config={"num_epochs": 3, "eval_freq": 5000, "eval_iter": 1000, "patience": 3, "sample_tokens": 1024, "progress_chunks": 20})
wandb.init(project='GatorGPT', config={"num_epochs": 3, "eval_freq": 50, "eval_iter": 10, "patience": 3, "sample_tokens": 1024, "progress_chunks": 20})
cfg = wandb.config

CONTEXT = "We ran across a field that was"
train_tokens, val_tokens = fast_prepare_data(data)

train_loader = create_fast_dataloader(
    train_tokens,
    batch_size=32,
    max_length=512,
    stride=256,
    shuffle=True
)

val_loader = create_fast_dataloader(
    val_tokens,
    batch_size=32,
    max_length=512,
    stride=256,
    shuffle=False
)

print("Total Training tokens:", len(train_tokens))
print("Total Validation tokens:", len(val_tokens))
print("Total Paramteres: ", sum(p.numel() for p in model.parameters()))

"""# Training"""

train_losses, val_losses, tokens_seen = train_model_simple(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    optimizer=optim,
    device=device,
    tokenizer=tok,
    start_context=CONTEXT,
    cfg=cfg,
)

wandb.finish()

"""# Saving files to Huggingface"""

!pip install -q huggingface_hub

import torch, json, shutil
from pathlib import Path
from huggingface_hub import login, create_repo, upload_folder

# 1ï¸âƒ£ Login
login()

# 2ï¸âƒ£ Save dir
save_dir = Path("gator_gpt_final")
if save_dir.exists():
    shutil.rmtree(save_dir)
save_dir.mkdir(exist_ok=True)

# 3ï¸âƒ£ Save weights
torch.save(model.state_dict(), save_dir / "pytorch_model.bin")

# 4ï¸âƒ£ config.json
config = {
    "architectures": ["GatorModel"],
    "model_type": "gator-transformer",
    "hidden_size": 448,
    "num_attention_heads": 8,
    "num_hidden_layers": 10,
    "vocab_size": 50257,
    "max_position_embeddings": 1024,
    "auto_map": {
        "AutoConfig": "configuration_gator.GatorConfig",
        "AutoModelForCausalLM": "modeling_gator.GatorModel"
    }
}
(save_dir / "config.json").write_text(json.dumps(config, indent=2))

# 5ï¸âƒ£ tokenizer_manifest.json
tokenizer_manifest = {"library": "tiktoken", "encoding": "p50k_base"}
(save_dir / "tokenizer_manifest.json").write_text(json.dumps(tokenizer_manifest, indent=2))

# 6ï¸âƒ£ configuration_gator.py
(save_dir / "configuration_gator.py").write_text('''\
from transformers import PretrainedConfig

class GatorConfig(PretrainedConfig):
    model_type = "gator-transformer"
    def __init__(self, hidden_size=448, num_attention_heads=8, num_hidden_layers=10,
                 vocab_size=50257, max_position_embeddings=1024, **kwargs):
        super().__init__(**kwargs)
        self.hidden_size = hidden_size
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
''')

# 7ï¸âƒ£ modeling_gator.py (full architecture)
(save_dir / "modeling_gator.py").write_text('''\
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from transformers import PreTrainedModel
from .configuration_gator import GatorConfig

class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    def forward(self, x):
        norm = x.norm(2, dim=-1, keepdim=True) / math.sqrt(x.shape[-1])
        return self.weight * (x / (norm + self.eps))

class Rope(nn.Module):
    def __init__(self, d_model, max_len=1024):
        super().__init__()
        assert d_model % 2 == 0
        self.register_buffer("pos", torch.arange(max_len).unsqueeze(1))
        self.register_buffer("inv_freq", torch.exp(
            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)))
    def forward(self, x):
        t = x.size(1)
        freqs = self.pos[:t] * self.inv_freq
        cos, sin = torch.cos(freqs), torch.sin(freqs)
        x = x.view(*x.shape[:-1], -1, 2)
        x1, x2 = x[...,0], x[...,1]
        x_rot = torch.stack([x1*cos - x2*sin, x1*sin + x2*cos], dim=-1)
        return x_rot.view(*x.shape[:-2], -1)

class GQA(nn.Module):
    def __init__(self, d_model, n_heads, gqa_groups, max_len):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.n_kv = n_heads // gqa_groups
        self.q_proj = nn.Linear(d_model, n_heads*self.head_dim, bias=False)
        self.k_proj = nn.Linear(d_model, self.n_kv*self.head_dim, bias=False)
        self.v_proj = nn.Linear(d_model, self.n_kv*self.head_dim, bias=False)
        self.o_proj = nn.Linear(d_model, d_model, bias=False)
        self.rope_q = Rope(n_heads*self.head_dim, max_len)
        self.rope_k = Rope(self.n_kv*self.head_dim, max_len)
    def forward(self, x):
        B,T,C = x.shape
        q = self.rope_q(self.q_proj(x)).view(B,T,self.n_heads,self.head_dim).transpose(1,2)
        k = self.rope_k(self.k_proj(x)).view(B,T,self.n_kv,self.head_dim).transpose(1,2)
        v = self.v_proj(x).view(B,T,self.n_kv,self.head_dim).transpose(1,2)
        expand = self.n_heads // self.n_kv
        k = k.repeat_interleave(expand, dim=1)
        v = v.repeat_interleave(expand, dim=1)
        attn = torch.softmax((q @ k.transpose(-2,-1))/math.sqrt(self.head_dim), dim=-1)
        out = attn @ v
        out = out.transpose(1,2).contiguous().view(B,T,C)
        return self.o_proj(out)

class MLP(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.fc1 = nn.Linear(d_model, 2*d_ff, bias=False)
        self.fc2 = nn.Linear(d_ff, d_model, bias=False)
    def forward(self,x):
        up, gate = self.fc1(x).chunk(2, dim=-1)
        return self.fc2(up * F.silu(gate))

class Block(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.rms1 = RMSNorm(cfg.hidden_size)
        self.rms2 = RMSNorm(cfg.hidden_size)
        self.attn = GQA(cfg.hidden_size, cfg.num_attention_heads, 2, cfg.max_position_embeddings)
        self.mlp = MLP(cfg.hidden_size, 2*cfg.hidden_size)
    def forward(self,x):
        x = x + self.attn(self.rms1(x))
        x = x + self.mlp(self.rms2(x))
        return x

class GatorModel(PreTrainedModel):
    config_class = GatorConfig
    def __init__(self, config):
        super().__init__(config)
        self.embed = nn.Embedding(config.vocab_size, config.hidden_size)
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_hidden_layers)])
        self.norm = RMSNorm(config.hidden_size)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.lm_head.weight = self.embed.weight
    def forward(self, input_ids):
        h = self.embed(input_ids)
        for blk in self.blocks: h = blk(h)
        h = self.norm(h)
        return {"logits": self.lm_head(h)}
''')

# 8ï¸âƒ£ __init__.py
(save_dir / "__init__.py").write_text('''\
from .configuration_gator import GatorConfig
from .modeling_gator import GatorModel
__all__ = ["GatorConfig", "GatorModel"]
''')

# 9ï¸âƒ£ Push to HF
repo_id = "kunjcr2/GatorGPT2"
create_repo(repo_id, repo_type="model", exist_ok=True)

upload_folder(
    repo_id=repo_id,
    folder_path=str(save_dir),
    repo_type="model",
    commit_message="ðŸš€ Push GatorGPT2 with custom config, model, tokenizer manifest"
)

print(f"âœ… Done! Model pushed to https://huggingface.co/{repo_id}")

"""# Plotting loss and Inference"""

import numpy as np
import matplotlib.pyplot as plt

def compress_losses(train_losses, target_len):
    """
    Reduce a long train_losses list to target_len by averaging chunks.
    """
    values = np.asarray(train_losses, dtype=float)
    chunks = np.array_split(values, target_len)
    return [chunk.mean() for chunk in chunks]

def plot_losses(tokens_seen, train_losses, val_losses):
    # Compress train_losses down to match tokens_seen length
    train_losses_reduced = compress_losses(train_losses, len(tokens_seen))

    plt.figure(figsize=(8, 5))
    plt.plot(tokens_seen, train_losses_reduced, label="Train Loss (avg per chunk)")
    plt.plot(tokens_seen, val_losses, label="Validation Loss")
    plt.xlabel("Tokens Seen")
    plt.ylabel("Loss")
    plt.title("Training vs Validation Loss")
    plt.legend()
    plt.grid(True)
    plt.show()

# Example usage
plot_losses(tokens_seen, train_losses, val_losses)

token_ids_to_text(generate_text_simple(model, text_to_token_ids("Little girl was", tok).to(device), 30, 1024), tok).replace("\n", " ").replace("\r\\", "").replace("\r", "")

import torch
import tiktoken
from transformers import AutoModelForCausalLM

MODEL_ID = "kunjcr2/GatorGPT2"
DEVICE   = "cuda" if torch.cuda.is_available() else "cpu"

tok = tiktoken.get_encoding("p50k_base")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID, trust_remote_code=True, torch_dtype=torch.float32
).to(DEVICE).eval()

def generate_greedy(prompt: str, max_new_tokens: int = 50) -> str:
    ids = tok.encode(prompt)
    x = torch.tensor([ids], device=DEVICE) # Convert to tensor and move to device here
    print(f"Device of initial 'x': {x.device}") # Check the device of x here
    for _ in range(max_new_tokens):
        # x is already on the correct device
        with torch.no_grad():
            out = model(x)
        logits = out["logits"] if isinstance(out, dict) else out.logits
        next_id = int(torch.argmax(logits[0, -1]))
        next_id_tensor = torch.tensor([[next_id]], device=DEVICE) # Ensure the next id is a tensor on the correct device
        x = torch.cat((x, next_id_tensor), dim=1) # Concatenate tensors on the same device
    return tok.decode(x[0].tolist()) # Decode the tensor

