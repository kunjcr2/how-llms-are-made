{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQaIovr3q6Z1",
        "outputId": "51bb4286-605d-4b1c-a40a-0a058a3b953d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Aug 22 16:46:44 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0             43W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfJmSY_0R7Ew"
      },
      "outputs": [],
      "source": [
        "%pip install torchinfo --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjPyS_JDyLWC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchinfo import summary\n",
        "import requests\n",
        "\n",
        "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
        "from torch.cuda.amp import autocast # For GPU acceleration\n",
        "\n",
        "import tiktoken\n",
        "tok = tiktoken.get_encoding(\"p50k_base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRdCoh3LckNL",
        "outputId": "6ceba550-617e-4191-acb1-531e42e37fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading moby_dick.txt...\n",
            "Downloading pride_prejudice.txt...\n",
            "Downloading frankenstein.txt...\n",
            "Downloading alice_wonderland.txt...\n",
            "Downloading christmas_carol.txt...\n",
            "\n",
            "✅ Download complete. `data` now contains the combined text of all books.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# List of books to download (title: URL)\n",
        "books = {\n",
        "    \"moby_dick.txt\": \"https://www.gutenberg.org/files/2701/2701-0.txt\",\n",
        "    \"pride_prejudice.txt\": \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
        "    \"frankenstein.txt\": \"https://www.gutenberg.org/files/84/84-0.txt\",\n",
        "    \"alice_wonderland.txt\": \"https://www.gutenberg.org/files/11/11-0.txt\",\n",
        "    \"christmas_carol.txt\": \"https://www.gutenberg.org/files/98/98-0.txt\",\n",
        "}\n",
        "\n",
        "# Download and store in `data`\n",
        "data = \"\"\n",
        "\n",
        "for name, url in books.items():\n",
        "    print(f\"Downloading {name}...\")\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        text = response.text.strip()\n",
        "        data += text + \"\\n\\n\"\n",
        "    else:\n",
        "        print(f\"Failed to download {name}\")\n",
        "\n",
        "print(\"\\n✅ Download complete. `data` now contains the combined text of all books.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEuM12qn6JAd"
      },
      "outputs": [],
      "source": [
        "class Rope(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(Rope, self).__init__()\n",
        "\n",
        "        # Store the embedding dimension (must be even for proper 2D rotations)\n",
        "        assert d_model % 2 == 0, \"d_mAodel must be even for proper RoPE implementation\"\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Store the maximum sequence length this RoPE can handle\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Create position indices [0, 1, 2, ..., max_len-1] and add dimension for broadcasting\n",
        "            # Shape: (max_len, 1) - each position gets its own row\n",
        "        self.register_buffer('position_ids', torch.arange(max_len).unsqueeze(1))\n",
        "\n",
        "        # Create frequency terms for rotation - these determine how fast each dimension pair rotates\n",
        "            # Uses exponential decay: smaller indices rotate faster, larger indices rotate slower\n",
        "            # torch.arange(0, d_model, 2) creates [0, 2, 4, 6, ...] (even indices only)\n",
        "            # The formula creates frequencies: [1/10000^(0/d), 1/10000^(2/d), 1/10000^(4/d), ...]\n",
        "            # This is equal to (10000 ** (-(2 * i) / d_model) for i in range(d_model // 2))\n",
        "        self.register_buffer('div_term', torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch_size, seq_len, d_model)\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # Get position indices for current sequence length (trim to actual sequence length)\n",
        "            # If input has 100 tokens, this gets positions [0, 1, 2, ..., 99]\n",
        "        position_ids = self.position_ids[:seq_len]  # Shape: (seq_len, 1)\n",
        "\n",
        "        # Calculate rotation angles for each position and frequency based on 2017 paper\n",
        "            # Multiply each position by each frequency term to get rotation angles\n",
        "            # Shape: (seq_len, d_model//2)\n",
        "            # This is basically: pos/(10000^(2i/d_model))\n",
        "        angles = position_ids * self.div_term\n",
        "\n",
        "        # Calculate sine and cosine values for rotation\n",
        "            # Shape: (seq_len, d_model//2)\n",
        "        cos_vals = torch.cos(angles)\n",
        "        sin_vals = torch.sin(angles)\n",
        "\n",
        "        # Reshape input to separate even and odd dimensions for 2D rotation\n",
        "            # Split x into pairs: (x_0, x_1), (x_2, x_3), (x_4, x_5), ...\n",
        "            # Shape: (batch_size, seq_len, d_model//2, 2)\n",
        "        x_pairs = x.view(batch_size, seq_len, d_model // 2, 2)\n",
        "\n",
        "        # Extract even and odd components\n",
        "            # x_even contains x_0, x_2, x_4, ... (first element of each pair)\n",
        "            # x_odd contains x_1, x_3, x_5, ... (second element of each pair)\n",
        "        x_even = x_pairs[..., 0]  # Shape: (batch_size, seq_len, d_model//2)\n",
        "        x_odd = x_pairs[..., 1]   # Shape: (batch_size, seq_len, d_model//2)\n",
        "\n",
        "        # Apply 2D rotation to each pair of dimensions\n",
        "            # Rotation matrix: [[cos, -sin], [sin, cos]]\n",
        "            # For each pair (x_i, x_{i+1}), compute:\n",
        "            # x_i' = x_i * cos - x_{i+1} * sin\n",
        "            # x_{i+1}' = x_i * sin + x_{i+1} * cos\n",
        "        rotated_even = x_even * cos_vals - x_odd * sin_vals\n",
        "        rotated_odd = x_even * sin_vals + x_odd * cos_vals\n",
        "\n",
        "        rotated_pairs = torch.stack([rotated_even, rotated_odd], dim=-1)\n",
        "        rotated_x = rotated_pairs.view(batch_size, seq_len, d_model)\n",
        "\n",
        "        return rotated_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3UfVD5p6RO3"
      },
      "outputs": [],
      "source": [
        "class GQA(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int = 384,\n",
        "                 n_heads: int = 8,\n",
        "                 gqa_groups: int = 2,\n",
        "                 max_len: int = 1024,\n",
        "                ):\n",
        "        super().__init__()  # initialize base Module\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"  # validate head split\n",
        "        assert n_heads % gqa_groups == 0, \"n_heads must be divisible by gqa_groups\"  # validate grouping\n",
        "\n",
        "        self.d_model = d_model  # store model dimension\n",
        "        self.n_heads = n_heads  # store number of query heads\n",
        "        self.gqa_groups = gqa_groups  # store number of groups for GQA\n",
        "        self.head_dim = d_model // n_heads  # compute per-head dimension\n",
        "        self.n_kv_heads = n_heads // gqa_groups  # compute number of K/V heads for GQA\n",
        "        self.max_len = max_len  # store max sequence length\n",
        "\n",
        "        # Define bias-free linear projections\n",
        "        self.q_proj = nn.Linear(d_model, n_heads * self.head_dim, bias=False)  # Q projection: d_model -> H*D\n",
        "        self.k_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)  # K projection: d_model -> H_kv*D\n",
        "        self.v_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)  # V projection: d_model -> H_kv*D\n",
        "        self.o_proj = nn.Linear(n_heads * self.head_dim, d_model, bias=False)  # Output projection: H*D -> d_model\n",
        "\n",
        "        # Instantiate two RoPE modules with the exact composite dims requested\n",
        "        self.rope_q = Rope(d_model=n_heads * self.head_dim, max_len=max_len)  # RoPE for Q (expects (B,T,H*D))\n",
        "        self.rope_k = Rope(d_model=self.n_kv_heads * self.head_dim, max_len=max_len)  # RoPE for K (expects (B,T,H_kv*D))\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,  # (B, T, d_model)\n",
        "                attention_mask: torch.Tensor | None = None  # (B, T) 1=real,0=pad (ignored: no attention bias)\n",
        "                ) -> torch.Tensor:  # returns (B, T, d_model)\n",
        "        B, T, C = x.shape  # unpack input shape\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        q = self.q_proj(x)  # (B, T, H*D)\n",
        "        k = self.k_proj(x)  # (B, T, H_kv*D)\n",
        "        v = self.v_proj(x)  # (B, T, H_kv*D)\n",
        "\n",
        "        # Apply RoPE to Q over the flattened head dimension\n",
        "        q = self.rope_q(q)  # (B, T, H*D) with rotary positional encoding applied\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).permute(0, 2, 1, 3).contiguous()  # (B, H, T, D)\n",
        "\n",
        "        # Apply RoPE to K over the flattened head dimension\n",
        "        k = self.rope_k(k)  # (B, T, H_kv*D) with rotary positional encoding applied\n",
        "        k = k.view(B, T, self.n_kv_heads, self.head_dim).permute(0, 2, 1, 3).contiguous()  # (B, H_kv, T, D)\n",
        "\n",
        "        # Reshape V to heads (no RoPE for V)\n",
        "        v = v.view(B, T, self.n_kv_heads, self.head_dim).permute(0, 2, 1, 3).contiguous()  # (B, H_kv, T, D)\n",
        "\n",
        "        # Expand K and V from n_kv_heads to n_heads via repeat_interleave on the head axis\n",
        "        expand_factor = self.n_heads // self.n_kv_heads  # compute replication factor\n",
        "        k = k.repeat_interleave(expand_factor, dim=1)  # (B, H, T, D)\n",
        "        v = v.repeat_interleave(expand_factor, dim=1)  # (B, H, T, D)\n",
        "        # Above thing converts [1,2,3,4] -> [1,1,1,2,2,2,3,3,3,4,4,4] when expand_factor is 3 and dim=0\n",
        "\n",
        "        # Compute SDPA with purely causal masking (no external attention bias, this uses flash attention\n",
        "        with sdpa_kernel(\n",
        "            backends=SDPBackend.FLASH_ATTENTION,\n",
        "            enable_math=False,\n",
        "            enable_mem_efficient=False,\n",
        "            enable_flash=True\n",
        "        ):\n",
        "            out = F.scaled_dot_product_attention(q, k, v, attn_mask=None, is_causal=True)  # (B, H, T, D)\n",
        "\n",
        "        # Merge heads back to (B, T, H*D)\n",
        "        out = out.permute(0, 2, 1, 3).contiguous().view(B, T, self.n_heads * self.head_dim)  # (B, T, d_model)\n",
        "\n",
        "        # Project to output dimension\n",
        "        out = self.o_proj(out)  # (B, T, d_model)\n",
        "\n",
        "        return out  # return attended representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYijehYT6ZOu"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    SwiGLU MLP for a decoder-only Transformer block.\n",
        "\n",
        "    - d_model: 384\n",
        "    - d_ff: ~768 (≈2.5 × d_model)\n",
        "    - Linear layers are bias-free\n",
        "    - RMSNorm is applied outside this module\n",
        "    - Input/Output shape: (batch, seq_len, d_model)\n",
        "    - BF16-friendly: uses ops that preserve input dtype\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int = 384, d_ff: int = 768):\n",
        "        super().__init__()\n",
        "        # Fused \"up\" + \"gate\" projection to reduce matmuls: d_model -> 2*d_ff\n",
        "        self.w1 = nn.Linear(d_model, 2 * d_ff, bias=False)\n",
        "        # Down projection: d_ff -> d_model\n",
        "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, T, d_model)\n",
        "        up, gate = self.w1(x).chunk(2, dim=-1)  # (B, T, d_ff) each\n",
        "\n",
        "        # We split in two because SwiGLU works like that and it takes -\n",
        "            # First half which is content\n",
        "            # Second half which is how much of content in the context\n",
        "        x = up * F.silu(gate)                   # SwiGLU: up ⊗ swish(gate)\n",
        "        x = self.w2(x)                          # (B, T, d_model)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrlVzq-P6iRb"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int = 384,\n",
        "        n_heads: int = 8,\n",
        "        gqa_groups: int = 2,\n",
        "        max_len: int = 1024,\n",
        "        d_ff: int = 768,\n",
        "        eps: float = 1e-5,\n",
        "        dropout_p: float = 0.0,  # keep 0.0 for pretrain\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.rms1 = nn.modules.normalization.RMSNorm(d_model, eps)\n",
        "        self.rms2 = nn.modules.normalization.RMSNorm(d_model, eps)\n",
        "\n",
        "        self.attn = GQA(d_model, n_heads, gqa_groups, max_len)  # should include proj_out\n",
        "        self.mlp = MLP(d_model, d_ff)  # your SwiGLU MLP (bias-free)\n",
        "\n",
        "        self.drop_attn = nn.Dropout(dropout_p)\n",
        "        self.drop_mlp = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # pre-rms\n",
        "        x = x + self.drop_attn(self.attn(self.rms1(x)))\n",
        "        x = x + self.drop_mlp(self.mlp(self.rms2(x)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGKYFDZA61RN"
      },
      "outputs": [],
      "source": [
        "class GatorGPT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 384,\n",
        "        n_heads: int = 8,\n",
        "        gqa_groups: int = 2,\n",
        "        max_len: int = 1024,\n",
        "        d_ff: int = 768,\n",
        "        eps: float = 1e-5,\n",
        "        dropout_p: float = 0.0,\n",
        "        blocks: int = 10,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.unembed = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        self.final_rms = nn.modules.normalization.RMSNorm(d_model, eps)\n",
        "        self.embed.weight = self.unembed.weight\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                Block(\n",
        "                    d_model=d_model,\n",
        "                    n_heads=n_heads,\n",
        "                    gqa_groups=gqa_groups,\n",
        "                    max_len=max_len,\n",
        "                    d_ff=d_ff,\n",
        "                    eps=eps,\n",
        "                    dropout_p=dropout_p,\n",
        "                ) for _ in range(blocks)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward method that takes in the tokens\n",
        "        \"\"\"\n",
        "      # x: (batch, seq_len) of token ids\n",
        "        h = self.embed(x)                 # (batch, seq_len, d_model)\n",
        "        for block in self.blocks:         # run each transformer block\n",
        "            h = block(h)\n",
        "        h = self.final_rms(h)\n",
        "        logits = self.unembed(h)          # (batch, seq_len, vocab_size)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQ3k_XG69xSP"
      },
      "outputs": [],
      "source": [
        "class GatorGPTDataset(Dataset):\n",
        "    def __init__(self, txt, tokenizer=tok, max_length=1024, stride=512):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            self.input_ids.append(torch.tensor(token_ids[i:i + max_length]))\n",
        "            self.target_ids.append(torch.tensor(token_ids[i + 1: i + max_length + 1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQVYCEWjAtEJ"
      },
      "outputs": [],
      "source": [
        "###################################### TURNS TEXTS TO TOKENS\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "############################################################################\n",
        "\n",
        "###################################### TURN TOKENS TO TEXT\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "  decoded = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
        "  return decoded\n",
        "############################################################################\n",
        "\n",
        "###################################### CREATES DATA LOADERS\n",
        "def create_dataloader(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0): # Changed default to 0\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"p50k_base\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GatorGPTDataset(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers # Pass the potentially modified num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader\n",
        "############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvIlC33GvjJg"
      },
      "outputs": [],
      "source": [
        "###################################### LOSS FOR ONE BATCH\n",
        "# Caluclates loss for a batch\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "    # For other devices\n",
        "    # logits = model(input_batch)\n",
        "    # loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "\n",
        "    # For A100s\n",
        "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "        logits = model(input_batch)\n",
        "        loss = torch.nn.functional.cross_entropy(\n",
        "            logits.flatten(0, 1),\n",
        "            target_batch.flatten()\n",
        "        )\n",
        "\n",
        "    return loss\n",
        "############################################################################\n",
        "\n",
        "###################################### LOSS FOR ENTIRE LOADER\n",
        "# Caluculates loss for ENTIRE data_loader which calls calc_loss_batch function inside itself\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "############################################################################\n",
        "\n",
        "###################################### USE THIS TO EVALUATE MODEL DIRECLTY\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "  # basically returns the losses for training and validation\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    return calc_loss_loader(train_loader, model, device, eval_iter), calc_loss_loader(val_loader, model, device, eval_iter)\n",
        "############################################################################\n",
        "\n",
        "####################################### GENERATING NEW TOKENS\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "\n",
        "  # idx is (batch, n_tokens) array of indices in current context\n",
        "  for _ in range(max_new_tokens):\n",
        "    # If LLM suports only 5 tokens, and the context size is 10, then we only use last 5 toens as context.\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "\n",
        "    # Gettings the predictions\n",
        "    with torch.no_grad():\n",
        "      # Reshape idx_cond to (batch_size, sequence_length, emb_dim)\n",
        "      # idx_cond = idx_cond.unsqueeze(-1).repeat(1 , 1, model.norm1.scale.shape[0]) # Or model.att.d_in to get the embedding dimension\n",
        "      logits = model(idx_cond) # (batch, num_tokens, vocab_size)\n",
        "\n",
        "    # We take the last row. We dont do anything to the batches neither to the last dimension of the vocabularies, but take the last row\n",
        "    logits = logits[:, -1, :] # (batch, vocab_size)\n",
        "\n",
        "    # getting probablities from the logits. We can say something like 50% chances of this, 2% chances of this...\n",
        "    probs = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
        "\n",
        "    # We see the highest value's index\n",
        "    idx_next = torch.argmax(probs, dim=-1, keepdim=True) # (batch, 1)\n",
        "\n",
        "    # Append the predicted token_id generated to the original index\n",
        "    idx = torch.cat((idx, idx_next), dim=1) # (batch, num_tokens+1)\n",
        "\n",
        "  return idx\n",
        "############################################################################\n",
        "\n",
        "###################################### GENERATING AND PRINTING SAMPLES\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context, context_size):\n",
        "  # we print out, what the model is generating right now at the end of each epoch. Also, we print 50 items!\n",
        "  model.eval()\n",
        "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    token_ids = generate_text_simple(\n",
        "        model, encoded, 50, context_size\n",
        "    )\n",
        "\n",
        "  decoded = token_ids_to_text(token_ids, tokenizer)\n",
        "  print(decoded.replace(\"\\n\", \" \"))\n",
        "  model.train()\n",
        "############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ds-jtNuay6R"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer, Patience):\n",
        "    # Initialize tracking lists\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "    best_val_loss = float('inf')\n",
        "    PATIENCE = Patience\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Check dataset size\n",
        "    print(f\"📊 Dataset info:\")\n",
        "    print(f\"   Training batches: {len(train_loader)}\")\n",
        "    print(f\"   Validation batches: {len(val_loader)}\")\n",
        "    print(f\"   Total steps per epoch: {len(train_loader)}\")\n",
        "    print(f\"   Evaluation every {eval_freq} steps\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"🔁 Starting epoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\"*30)\n",
        "        model.train()\n",
        "\n",
        "        epoch_steps = 0\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "\n",
        "            if patience_counter >= PATIENCE:\n",
        "              break\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "            epoch_steps += 1\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Print progress within epoch\n",
        "            if epoch_steps % max(1, len(train_loader) // 5) == 0:  # Print 5 times per epoch\n",
        "                avg_loss = epoch_loss / epoch_steps\n",
        "                progress = epoch_steps / len(train_loader) * 100\n",
        "                print(f\"   Step {epoch_steps}/{len(train_loader)} ({progress:.1f}%) - \"\n",
        "                      f\"Current loss: {loss.item():.3f}, Avg loss: {avg_loss:.3f}, patience level: {patience_counter}/5\")\n",
        "\n",
        "            # Evaluation during training\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "\n",
        "                improvement = \"✅\" if val_loss < best_val_loss else \"❌\"\n",
        "                print(f\"   Eval at step {global_step}: Train {train_loss:.3f}, \"\n",
        "                      f\"Val {val_loss:.3f} {improvement}, patience level: {patience_counter}/5\")\n",
        "\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "        # End of epoch summary\n",
        "        avg_epoch_loss = epoch_loss / epoch_steps if epoch_steps > 0 else float('inf')\n",
        "        print(f\"✅ Epoch {epoch+1} complete: {epoch_steps} steps, avg loss: {avg_epoch_loss:.3f}\")\n",
        "\n",
        "        # Generate sample after each epoch\n",
        "        print(\"🎯 Generated sample:\")\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context, 1024)\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    print(f\"🏁 Training complete! Best validation loss: {best_val_loss:.3f}\")\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blOEz7Q5doin"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = GatorGPT(vocab_size=tok.n_vocab)\n",
        "\n",
        "# To cuda, compiling before running\n",
        "model = model.to(device)\n",
        "model = torch.compile(model)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=0.001, eps=1e-08, weight_decay=0.01)\n",
        "EPOCHS = 5\n",
        "EVAL_FREQ = 543\n",
        "EVAL_ITER = 180\n",
        "CONTEXT = \"We ran across a field that was\"\n",
        "PATIENCE = 5\n",
        "\n",
        "train_loader = create_dataloader(\n",
        "    data[:int(0.9*len(data))],\n",
        "    batch_size=4,\n",
        "    max_length=1024,\n",
        "    stride=128,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0 # Explicitly setting to 0 here as well for clarity\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader(\n",
        "    data[int(0.9*len(data)):],\n",
        "    batch_size=4,\n",
        "    max_length=1024,\n",
        "    stride=128,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0 # Explicitly setting to 0 here as well for clarity\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0UaMJ_hknWg",
        "outputId": "1b49416f-09fd-4922-cf3f-43473ef21010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Dataset info:\n",
            "   Training batches: 1629\n",
            "   Validation batches: 184\n",
            "   Total steps per epoch: 1629\n",
            "   Evaluation every 543 steps\n",
            "==================================================\n",
            "🔁 Starting epoch 1/5\n",
            "------------------------------\n",
            "   Eval at step 0: Train 10.267, Val 10.223 ✅, patience level: 0/5\n",
            "   Step 325/1629 (20.0%) - Current loss: 4.716, Avg loss: 5.713, patience level: 0/5\n",
            "   Eval at step 543: Train 4.757, Val 4.910 ✅, patience level: 0/5\n",
            "   Step 650/1629 (39.9%) - Current loss: 4.673, Avg loss: 5.293, patience level: 0/5\n",
            "   Step 975/1629 (59.9%) - Current loss: 4.446, Avg loss: 5.052, patience level: 0/5\n",
            "   Eval at step 1086: Train 4.374, Val 4.727 ✅, patience level: 0/5\n",
            "   Step 1300/1629 (79.8%) - Current loss: 4.193, Avg loss: 4.876, patience level: 0/5\n",
            "   Step 1625/1629 (99.8%) - Current loss: 4.448, Avg loss: 4.738, patience level: 0/5\n",
            "✅ Epoch 1 complete: 1629 steps, avg loss: 4.736\n",
            "🎯 Generated sample:\n",
            " the same time, and the same time, and the same, and\n",
            "==================================================\n",
            "🔁 Starting epoch 2/5\n",
            "------------------------------\n",
            "   Eval at step 1629: Train 4.059, Val 4.627 ✅, patience level: 0/5\n",
            "   Step 325/1629 (20.0%) - Current loss: 3.928, Avg loss: 3.999, patience level: 0/5\n",
            "   Eval at step 2172: Train 3.812, Val 4.622 ✅, patience level: 0/5\n",
            "   Step 650/1629 (39.9%) - Current loss: 3.691, Avg loss: 3.926, patience level: 0/5\n",
            "   Step 975/1629 (59.9%) - Current loss: 3.526, Avg loss: 3.851, patience level: 0/5\n",
            "   Eval at step 2715: Train 3.451, Val 4.608 ✅, patience level: 0/5\n",
            "   Step 1300/1629 (79.8%) - Current loss: 3.420, Avg loss: 3.780, patience level: 0/5\n",
            "   Step 1625/1629 (99.8%) - Current loss: 3.336, Avg loss: 3.703, patience level: 0/5\n",
            "✅ Epoch 2 complete: 1629 steps, avg loss: 3.702\n",
            "🎯 Generated sample:\n",
            " to be rehearsed from the Specksnyder. Literally\n",
            "==================================================\n",
            "🔁 Starting epoch 3/5\n",
            "------------------------------\n",
            "   Eval at step 3258: Train 3.196, Val 4.706 ❌, patience level: 0/5\n",
            "   Step 325/1629 (20.0%) - Current loss: 3.511, Avg loss: 3.153, patience level: 1/5\n",
            "   Eval at step 3801: Train 2.870, Val 4.904 ❌, patience level: 1/5\n",
            "   Step 650/1629 (39.9%) - Current loss: 3.011, Avg loss: 3.076, patience level: 2/5\n",
            "   Step 975/1629 (59.9%) - Current loss: 2.940, Avg loss: 2.990, patience level: 2/5\n",
            "   Eval at step 4344: Train 2.519, Val 5.101 ❌, patience level: 2/5\n",
            "   Step 1300/1629 (79.8%) - Current loss: 2.583, Avg loss: 2.903, patience level: 3/5\n",
            "   Step 1625/1629 (99.8%) - Current loss: 2.430, Avg loss: 2.811, patience level: 3/5\n",
            "✅ Epoch 3 complete: 1629 steps, avg loss: 2.810\n",
            "🎯 Generated sample:\n",
            " often been studying the open to me, and I have made you\n",
            "==================================================\n",
            "🔁 Starting epoch 4/5\n",
            "------------------------------\n",
            "   Eval at step 4887: Train 2.148, Val 5.360 ❌, patience level: 3/5\n",
            "   Step 325/1629 (20.0%) - Current loss: 2.173, Avg loss: 2.110, patience level: 4/5\n",
            "   Eval at step 5430: Train 1.822, Val 5.726 ❌, patience level: 4/5\n",
            "✅ Epoch 4 complete: 544 steps, avg loss: 2.062\n",
            "🎯 Generated sample:\n",
            " wear, for the trial of the trial--there was no one of any\n",
            "==================================================\n",
            "🔁 Starting epoch 5/5\n",
            "------------------------------\n",
            "✅ Epoch 5 complete: 0 steps, avg loss: inf\n",
            "🎯 Generated sample:\n",
            " wear, for the trial of the trial--there was no one of any\n",
            "==================================================\n",
            "🏁 Training complete! Best validation loss: 4.608\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([10.266812176174588,\n",
              "  4.756670006116232,\n",
              "  4.374165822399987,\n",
              "  4.058816957473755,\n",
              "  3.812153563234541,\n",
              "  3.4509832514656913,\n",
              "  3.1961449914508395,\n",
              "  2.869881663057539,\n",
              "  2.5189720312754313,\n",
              "  2.147828308078978,\n",
              "  1.8217950297726526],\n",
              " [10.223125457763672,\n",
              "  4.909924348195394,\n",
              "  4.727024560504489,\n",
              "  4.627366911040412,\n",
              "  4.621872064802382,\n",
              "  4.608041501045227,\n",
              "  4.7057045261065165,\n",
              "  4.904332576857673,\n",
              "  5.101007869508531,\n",
              "  5.35958661503262,\n",
              "  5.726420248879327],\n",
              " [4096,\n",
              "  2228224,\n",
              "  4452352,\n",
              "  6676480,\n",
              "  8900608,\n",
              "  11124736,\n",
              "  13348864,\n",
              "  15572992,\n",
              "  17797120,\n",
              "  20021248,\n",
              "  22245376])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_model_simple(model, train_loader, val_loader, optim, device, EPOCHS, EVAL_FREQ, EVAL_ITER, CONTEXT, tok, PATIENCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "jIQ-AhHsoXeo",
        "outputId": "f04cda26-7093-468c-bad4-fd77a03be15b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Fascinated by the blackium, and the coffin-ends of the Pequod’s company, and the nearest. The Chase made itself, and the seamen went by the purchasing house air, and the rolling ship, and the rolling'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_ids_to_text(generate_text_simple(model, text_to_token_ids(\"Fascinated by the black\", tok).to(device), 50, 1024), tok).replace(\"\\n\", \" \").replace(\"\\r\\\\\", \"\").replace(\"\\r\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiL1bDDN02Iz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
