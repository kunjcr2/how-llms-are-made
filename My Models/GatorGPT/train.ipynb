{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5504d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchinfo --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "import requests\n",
    "\n",
    "from torch.cuda.amp import autocast # For GPU acceleration\n",
    "\n",
    "import tiktoken\n",
    "tok = tiktoken.get_encoding(\"p50k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# List of books to download (title: URL)\n",
    "books = {\n",
    "    \"moby_dick.txt\": \"https://www.gutenberg.org/files/2701/2701-0.txt\",\n",
    "    \"pride_prejudice.txt\": \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
    "    \"frankenstein.txt\": \"https://www.gutenberg.org/files/84/84-0.txt\",\n",
    "    \"alice_wonderland.txt\": \"https://www.gutenberg.org/files/11/11-0.txt\",\n",
    "    \"christmas_carol.txt\": \"https://www.gutenberg.org/files/98/98-0.txt\",\n",
    "}\n",
    "\n",
    "# Download and store in `data`\n",
    "data = \"\"\n",
    "\n",
    "for name, url in books.items():\n",
    "    print(f\"Downloading {name}...\")\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        text = response.text.strip()\n",
    "        data += text + \"\\n\\n\"\n",
    "    else:\n",
    "        print(f\"Failed to download {name}\")\n",
    "\n",
    "print(\"\\nâœ… Download complete. `data` now contains the combined text of all books.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf2ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rope(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(Rope, self).__init__()\n",
    "\n",
    "        # Store the embedding dimension (must be even for proper 2D rotations)\n",
    "        assert d_model % 2 == 0, \"d_mAodel must be even for proper RoPE implementation\"\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Store the maximum sequence length this RoPE can handle\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Create position indices [0, 1, 2, ..., max_len-1] and add dimension for broadcasting\n",
    "            # Shape: (max_len, 1) - each position gets its own row\n",
    "        self.register_buffer('position_ids', torch.arange(max_len).unsqueeze(1))\n",
    "\n",
    "        # Create frequency terms for rotation - these determine how fast each dimension pair rotates\n",
    "            # Uses exponential decay: smaller indices rotate faster, larger indices rotate slower\n",
    "            # torch.arange(0, d_model, 2) creates [0, 2, 4, 6, ...] (even indices only)\n",
    "            # The formula creates frequencies: [1/10000^(0/d), 1/10000^(2/d), 1/10000^(4/d), ...]\n",
    "            # This is equal to (10000 ** (-(2 * i) / d_model) for i in range(d_model // 2))\n",
    "        self.register_buffer('div_term', torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, d_model)\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        # Get position indices for current sequence length (trim to actual sequence length)\n",
    "            # If input has 100 tokens, this gets positions [0, 1, 2, ..., 99]\n",
    "        position_ids = self.position_ids[:seq_len]  # Shape: (seq_len, 1)\n",
    "\n",
    "        # Calculate rotation angles for each position and frequency based on 2017 paper\n",
    "            # Multiply each position by each frequency term to get rotation angles\n",
    "            # Shape: (seq_len, d_model//2)\n",
    "            # This is basically: pos/(10000^(2i/d_model))\n",
    "        angles = position_ids * self.div_term\n",
    "\n",
    "        # Calculate sine and cosine values for rotation\n",
    "            # Shape: (seq_len, d_model//2)\n",
    "        cos_vals = torch.cos(angles)\n",
    "        sin_vals = torch.sin(angles)\n",
    "\n",
    "        # Reshape input to separate even and odd dimensions for 2D rotation\n",
    "            # Split x into pairs: (x_0, x_1), (x_2, x_3), (x_4, x_5), ...\n",
    "            # Shape: (batch_size, seq_len, d_model//2, 2)\n",
    "        x_pairs = x.view(batch_size, seq_len, d_model // 2, 2)\n",
    "\n",
    "        # Extract even and odd components\n",
    "            # x_even contains x_0, x_2, x_4, ... (first element of each pair)\n",
    "            # x_odd contains x_1, x_3, x_5, ... (second element of each pair)\n",
    "        x_even = x_pairs[..., 0]  # Shape: (batch_size, seq_len, d_model//2)\n",
    "        x_odd = x_pairs[..., 1]   # Shape: (batch_size, seq_len, d_model//2)\n",
    "\n",
    "        # Apply 2D rotation to each pair of dimensions\n",
    "            # Rotation matrix: [[cos, -sin], [sin, cos]]\n",
    "            # For each pair (x_i, x_{i+1}), compute:\n",
    "            # x_i' = x_i * cos - x_{i+1} * sin\n",
    "            # x_{i+1}' = x_i * sin + x_{i+1} * cos\n",
    "        rotated_even = x_even * cos_vals - x_odd * sin_vals\n",
    "        rotated_odd = x_even * sin_vals + x_odd * cos_vals\n",
    "\n",
    "        rotated_pairs = torch.stack([rotated_even, rotated_odd], dim=-1)\n",
    "        rotated_x = rotated_pairs.view(batch_size, seq_len, d_model)\n",
    "\n",
    "        return rotated_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc9ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
