{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "with open(\"the_verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()"
      ],
      "metadata": {
        "id": "SDkHBXrl1dCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "rBQ370rvaT84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "4b981ff5-c216-44f9-ed00-d14cd4698ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "yrQt2sOJkTMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading gpt2 tokenizer"
      ],
      "metadata": {
        "id": "uFOWMFF9lMtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating input output pairs\n",
        "\n",
        "we're implemeting data loaders using sliding window approach"
      ],
      "metadata": {
        "id": "JQDpFjjWhC4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loaders are efficient and structured way of using datasets\n",
        "we're using pytorch's inbubilt stuff"
      ],
      "metadata": {
        "id": "R7reKAPRmL8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch"
      ],
      "metadata": {
        "id": "-Vz-Dyt0k6LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            self.input_ids.append(torch.tensor(token_ids[i:i + max_length]))\n",
        "            self.target_ids.append(torch.tensor(token_ids[i + 1: i + max_length + 1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "WZCaIuRFu1H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "6oYFtXmtyEJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_2 = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)"
      ],
      "metadata": {
        "id": "4mdFXkJw4Hcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iter_2 = iter(dataloader_2)"
      ],
      "metadata": {
        "id": "UqWoj1ht43S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(data_iter_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrqnYSf75EFq",
        "outputId": "46baaace-718e-43d3-8418-5266c4649efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[10970, 33310,    35, 18379],\n",
              "         [  198, 15749, 40417,   628],\n",
              "         [  198,    40,   550,  1464],\n",
              "         [ 1807,  3619,   402,   271],\n",
              "         [10899,  2138,   257,  7026],\n",
              "         [15632,   438,  2016,   257],\n",
              "         [  198, 11274,  5891,  1576],\n",
              "         [  438,   568,   340,   373]]),\n",
              " tensor([[33310,    35, 18379,   198],\n",
              "         [15749, 40417,   628,   198],\n",
              "         [   40,   550,  1464,  1807],\n",
              "         [ 3619,   402,   271, 10899],\n",
              "         [ 2138,   257,  7026, 15632],\n",
              "         [  438,  2016,   257,   198],\n",
              "         [11274,  5891,  1576,   438],\n",
              "         [  568,   340,   373,   645]])]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets trynna make token embeddings now\n",
        "\n",
        "Lets take a simplest form of example with just vocab of size 6 and vector embedding of size 3"
      ],
      "metadata": {
        "id": "ZD4nHXmRdzK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "_TMqazTf46IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "embed_1 = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "rl51Pg9eezmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomly initialized embedding layer weights"
      ],
      "metadata": {
        "id": "MXZHbeqvfQjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_1.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J08dz967e82f",
        "outputId": "f4cf9598-4b39-4170-aeaf-547edae835b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.6590,  1.8272,  0.9967],\n",
              "        [ 0.7668,  0.0812, -0.2805],\n",
              "        [ 0.3666, -0.4789, -0.9839],\n",
              "        [ 0.9342,  1.6583, -0.8979],\n",
              "        [ 0.0170, -1.2282,  0.9828],\n",
              "        [-1.0764, -0.1559,  1.5065]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically when we train model. The weights of this particular embedding layer that is in the start PLUS the weights of the actual neurons in the neural netowrk are tuned and hence used later on in predicting next word."
      ],
      "metadata": {
        "id": "d1Ck3C6Afiid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embed_1(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD0kbBoafEii",
        "outputId": "3f53ab66-bf33-4789-ae16-51e30fa304ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3666, -0.4789, -0.9839],\n",
            "        [ 0.9342,  1.6583, -0.8979],\n",
            "        [-1.0764, -0.1559,  1.5065],\n",
            "        [ 0.7668,  0.0812, -0.2805]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional encodings"
      ],
      "metadata": {
        "id": "I6Hbu67Vtc1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "embed = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "W4_VnRqDib5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False\n",
        ")\n",
        "data_itr = iter(dataloader)"
      ],
      "metadata": {
        "id": "XClln8SztnRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets = next(data_itr)"
      ],
      "metadata": {
        "id": "ocW5_VS-vGtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_embedding = embed(inputs)"
      ],
      "metadata": {
        "id": "kbxE6MlCvKmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_embedding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cUDM_Envc9t",
        "outputId": "a1c94b47-bcf8-4782-8a29-fbc3788c40ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we add positional embedding"
      ],
      "metadata": {
        "id": "uo2QgygXwjnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We have context_length of 4 as we JUST want the positional embeddings for positions 0,1,2,3 and that's why we have context_length of 4 only rather than being of 50257.\n",
        "\n",
        "- Also, the what we are going to do is basically add this positional embeddings to the inputs embeddings and have our final input ready for torch to have."
      ],
      "metadata": {
        "id": "jGxviNOJzGhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "embed_pos = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "FMWgO-JjwaUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = embed_pos(torch.arange(max_length))"
      ],
      "metadata": {
        "id": "Zv85N4gbx_EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rkloz_Dbyh-W",
        "outputId": "a00e9a1a-7da6-4f5a-a3ba-7257bb000390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4138,  0.1417, -1.0336,  ...,  1.0254,  1.0297,  0.3817],\n",
              "        [ 0.5235, -0.4544, -0.3201,  ...,  0.8458, -2.0020,  0.8202],\n",
              "        [ 0.2843, -0.6244, -1.6005,  ...,  0.2893,  1.2007,  1.5052],\n",
              "        [ 1.1805, -1.9126,  0.9538,  ..., -1.1906, -1.4974,  0.0035]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_embeddings = pos_embeddings + token_embedding"
      ],
      "metadata": {
        "id": "vzHm-k9IzEOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v2QD8dL0fHd",
        "outputId": "82567520-e9b4-4679-e1ae-fe455efd7fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ganH5DKx4mdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}