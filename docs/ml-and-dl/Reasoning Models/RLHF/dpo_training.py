# -*- coding: utf-8 -*-
"""DPO Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1euezmDyO2TC7Xh_jGaFMBp_sVzmbDW3y

Huggingface repo at
    https://huggingface.co/kunjcr2/qwen2.5-0.5b-sft-dpo/tree/main
"""

# pip install trl --quiet

import os, torch, random, numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
from trl import SFTTrainer, SFTConfig, DPOTrainer, DPOConfig
from peft import LoraConfig

import pandas as pd

os.environ["WANDB_PROJECT"] = "qwen05b-sft"
torch.backends.cuda.matmul.allow_tf32 = True
torch.set_float32_matmul_precision("high")

DTYPE = torch.bfloat16
BASE_ID = "Qwen/Qwen2.5-0.5B-Instruct"
MAXLEN = 1024

ATTN_IMPL = "flash_attention_2"
try:
    import flash_attn  # noqa
except ImportError:
    ATTN_IMPL = "eager"

SEED = 42
set_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

tok = AutoTokenizer.from_pretrained(BASE_ID, use_fast=True)
if tok.pad_token is None: tok.pad_token = tok.eos_token
tok.padding_side = "right"

raw = load_dataset("HuggingFaceH4/ultrachat_200k", split="train_sft[:100000]").train_test_split(test_size=0.02, seed=SEED)

def to_text(ex):
    # keep only examples that END with an assistant message
    msgs = ex["messages"]
    if not msgs or msgs[-1]["role"] != "assistant":
        return {"text": None}
    return {
        "text": tok.apply_chat_template(
            msgs,
            tokenize=False,
            add_generation_prompt=False  # learn full assistant turn(s)
        )
    }

train = raw["train"].map(to_text, remove_columns=raw["train"].column_names, num_proc=4)
evald = raw["test"].map(to_text, remove_columns=raw["test"].column_names, num_proc=4)

policy = AutoModelForCausalLM.from_pretrained(
    BASE_ID,
    device_map="auto",
    torch_dtype=DTYPE,
    attn_implementation=ATTN_IMPL,
)
policy.config.use_cache = False
policy.gradient_checkpointing_enable()

# --- LoRA: include MLP projections for better quality
peft_cfg = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
)

# --- Training config
sft_cfg = SFTConfig(
    output_dir="ckpt_sft_merged_qwen05b",
    max_length=MAXLEN,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=8,  # effective microbatching
    num_train_epochs=1,
    learning_rate=2e-5,
    logging_steps=10,
    report_to=["wandb"],
    bf16=True,
    tf32=True,
    packing=False,  # safer for dialogue structure; turn on if you must
    dataset_text_field="text",
    group_by_length=True,
    dataloader_num_workers=4,
    dataloader_pin_memory=True,
    gradient_checkpointing=True,
    optim="adamw_torch",
    lr_scheduler_type="cosine",
    warmup_ratio=0.05,
    max_grad_norm=1.0,
    seed=SEED,

    # eval & saving
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=200,
    save_total_limit=3,
)

trainer = SFTTrainer(
    model=policy,
    args=sft_cfg,
    peft_config=peft_cfg,
    train_dataset=train,
    eval_dataset=evald,
    processing_class=tok,
)

trainer.train()

trainer.model.merge_and_unload()
trainer.model.save_pretrained("ckpt_sft_merged_qwen05b/final_model")
tok.save_pretrained("ckpt_sft_merged_qwen05b/final_model")

del policy

def prep_dpo(ex):
    if not ex.get("chosen") or not ex.get("rejected"):
        return None

    raw = ex["prompt"]
    # keep raw chat if it's a list; else wrap as one user turn
    prompt = raw if isinstance(raw, list) else [{"role": "user", "content": str(raw)}]

    return {
        "prompt": prompt,                 # NOT templated
        "chosen": str(ex["chosen"]),
        "rejected": str(ex["rejected"]),
    }

prefs = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="train_prefs")

prefs = prefs.map(prep_dpo, num_proc=4)

prefs = prefs.train_test_split(test_size=0.1, seed=SEED)

train_prefs = prefs["train"]
eval_prefs = prefs["test"]

policy_dpo = AutoModelForCausalLM.from_pretrained(
    "ckpt_sft_merged_qwen05b/final_model", device_map="auto", torch_dtype=DTYPE,
    attn_implementation=ATTN_IMPL
)
policy_dpo.config.use_cache = False
policy_dpo.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False})

peft_cfg_dpo = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
)

train_prefs = train_prefs.remove_columns(["prompt_id", "messages", "score_chosen", "score_rejected"])
eval_prefs = eval_prefs.remove_columns(["prompt_id", "messages", "score_chosen", "score_rejected"])

dpo_cfg = DPOConfig(
    # I/O
    output_dir="ckpt_dpo_qwen05b_refrozen",

    # Core DPO
    beta=0.1,                           # keep
    f_divergence_type="reverse_kl",     # default; good starting point
    reference_free=False,               # standard DPO with ref

    # Batching
    per_device_train_batch_size=16,
    gradient_accumulation_steps=16,
    per_device_eval_batch_size=16,

    # Schedule & LR
    num_train_epochs=1,
    max_steps=-1,                       # honor num_train_epochs
    learning_rate=1e-5,                 # LoRA-DPO sweet spot
    lr_scheduler_type="cosine",
    warmup_ratio=0.05,                  # match SFT
    weight_decay=0.0,

    # Lengths
    max_length=MAXLEN,                  # same as SFT
    max_prompt_length=512,              # default is fine; keep explicit
    truncation_mode="keep_end",

    # Logging / Eval / Save (mirror SFT cadence)
    logging_strategy="steps",
    logging_steps=5,                    # match SFT
    report_to=["wandb"],

    eval_strategy="steps",
    eval_steps=100,                      # match SFT
    save_strategy="steps",
    save_steps=100,                      # match SFT
    save_total_limit=3,
    save_safetensors=True,

    # Precision & perf
    bf16=True,
    tf32=True,
    gradient_checkpointing=True,        # match SFT
    dataloader_num_workers=4,
    dataloader_pin_memory=True,
    group_by_length=False,               # match SFT
    remove_unused_columns=False,         # default good

    # Optimizer (match SFT unless you’re in 4/8-bit)
    optim="adamw_torch",                # if using 4/8-bit, switch to "paged_adamw_torch"
    max_grad_norm=1.0,

    # Repro
    seed=SEED,

    # Nice-to-haves (explicit)
    disable_dropout=True,               # default; keeps eval-style stability during DPO
    label_pad_token_id=-100,
    average_tokens_across_devices=True
)

# No need to pass in the seprate Ref model since DPOTrainer makes their own !
dpo_tr = DPOTrainer(
    model=policy_dpo,
    args=dpo_cfg,
    train_dataset=train_prefs,
    eval_dataset=eval_prefs,
    processing_class=tok,
    peft_config=peft_cfg_dpo
)

dpo_tr.train()
dpo_tr.model.save_pretrained("ckpt_dpo_qwen05b_refrozen")
tok.save_pretrained("ckpt_dpo_qwen05b_refrozen")

from pathlib import Path
from huggingface_hub import create_repo, login

login()
USERNAME = "kunjcr2"   # or org
REPO_ID = f"{USERNAME}/qwen2.5-0.5b-sft-dpo"  # single repo

SFT_LOCAL = "ckpt_sft_merged_qwen05b/final_model"
DPO_LOCAL = "ckpt_dpo_qwen05b_refrozen"

assert Path(SFT_LOCAL).exists()
assert Path(DPO_LOCAL).exists()

create_repo(REPO_ID, repo_type="model", private=False, exist_ok=True)

import shutil, os
from huggingface_hub import HfApi

STAGING = "hub_upload"
if os.path.exists(STAGING):
    shutil.rmtree(STAGING)

# copy backbone (root)
shutil.copytree(SFT_LOCAL, STAGING)

# add adapters subfolder
ADAPT_SUBFOLDER = os.path.join(STAGING, "dpo_adapters")
shutil.copytree(DPO_LOCAL, ADAPT_SUBFOLDER)

# push to hub
api = HfApi()
api.upload_folder(
    repo_id=REPO_ID,
    folder_path=STAGING,
    commit_message="Upload SFT backbone + DPO adapters in one repo",
)
print("✅ pushed to", REPO_ID)

