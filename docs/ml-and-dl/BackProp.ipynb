{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51cae586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b0776",
   "metadata": {},
   "source": [
    "### Explanation of Key Concepts and Classes\n",
    "\n",
    "#### Gradient (`grad`)\n",
    "- The `grad` attribute in the `Value` class represents the gradient of the output with respect to that value. Gradients are essential for updating parameters during training using optimization algorithms like gradient descent.\n",
    "\n",
    "#### Backpropagation (`backward`)\n",
    "- Backpropagation is the process of computing gradients for all parameters in the network by traversing the computation graph in reverse order. The `backward()` method in the `Value` class implements this by building a topological order of nodes and applying the chain rule to propagate gradients backward through the graph.\n",
    "\n",
    "#### Classes Overview\n",
    "- **Value**: Represents a scalar value in the computation graph, supports basic arithmetic operations, and tracks gradients for automatic differentiation.\n",
    "- **Neuron**: Models a single artificial neuron with weights and bias, computes the output using a tanh activation function.\n",
    "- **Layer**: Comprises multiple neurons, processes input data through each neuron, and aggregates their outputs.\n",
    "- **MLP (Multi-Layer Perceptron)**: Stacks multiple layers to form a feedforward neural network, enabling complex function approximation.\n",
    "\n",
    "#### Training Loop\n",
    "- The training loop repeatedly feeds input data through the network, computes the loss (difference between predictions and targets), performs backpropagation to calculate gradients, and updates the parameters using gradient descent. This iterative process allows the network to learn from data and improve its predictions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bbccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  \n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self.grad = 0.0\n",
    "    self._backward = lambda: None\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  def __add__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += 1.0 * out.grad\n",
    "      other.grad += 1.0 * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "      \n",
    "    return out\n",
    "  \n",
    "  def __pow__(self, other):\n",
    "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def __rmul__(self, other): # other * self\n",
    "    return self * other\n",
    "\n",
    "  def __truediv__(self, other): # self / other\n",
    "    return self * other**-1\n",
    "\n",
    "  def __neg__(self): # -self\n",
    "    return self * -1\n",
    "\n",
    "  def __sub__(self, other): # self - other\n",
    "    return self + (-other)\n",
    "\n",
    "  def __radd__(self, other): # other + self\n",
    "    return self + other\n",
    "\n",
    "  def tanh(self):\n",
    "    x = self.data\n",
    "    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "    out = Value(t, (self, ), 'tanh')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += (1 - t**2) * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  def exp(self):\n",
    "    x = self.data\n",
    "    out = Value(math.exp(x), (self, ), 'exp')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += out.data * out.grad # NOTE: in the video I incorrectly used = instead of +=. Fixed here.\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  def backward(self):\n",
    "    \n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          build_topo(child)\n",
    "        topo.append(v)\n",
    "    build_topo(self) \n",
    "    \n",
    "    self.grad = 1.0\n",
    "    for node in reversed(topo):\n",
    "      node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dace89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  \n",
    "  def __init__(self, nin):\n",
    "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "    self.b = Value(random.uniform(-1,1))\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # w * x + b\n",
    "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "    out = act.tanh()\n",
    "    return out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "  \n",
    "  def __init__(self, nin, nout):\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    outs = [n(x) for n in self.neurons]\n",
    "    return outs[0] if len(outs) == 1 else outs\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "  \n",
    "  def __init__(self, nin, nouts):\n",
    "    sz = [nin] + nouts\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb439d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(value, indent=0):\n",
    "    prefix = \" \" * indent\n",
    "    try:\n",
    "        # Print the operation and its result\n",
    "        print(f\"{prefix}{'[op:'+value._op+']'}: {value.data:.2f} (grad: {value.grad:.2f})\")\n",
    "    except AttributeError:\n",
    "        print(f\"{prefix}{'[no_op]'}: {value:.2f} (grad: : None)\")\n",
    "    for child in value._prev:\n",
    "        print_tree(child, indent + 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b51d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(3, [4, 4, 1])\n",
    "\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6935d8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss: 4.97816241\n",
      "epoch 20, loss: 4.22803703\n",
      "epoch 30, loss: 3.90322286\n",
      "epoch 40, loss: 3.57281627\n",
      "epoch 50, loss: 2.58125884\n",
      "epoch 60, loss: 1.00777397\n",
      "epoch 70, loss: 0.48989850\n",
      "epoch 80, loss: 0.29139758\n",
      "epoch 90, loss: 0.19592383\n",
      "epoch 100, loss: 0.14277657\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    step += 1\n",
    "    out = [mlp(x) for x in xs]\n",
    "    loss = sum((o - y)**2 for y, o in zip(ys, out)) # MSE loss\n",
    "\n",
    "    for p in mlp.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "\n",
    "    for p in mlp.parameters():\n",
    "        p.data -= 0.01 * p.grad\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"epoch {epoch+1}, loss: {loss.data:.8f}\")\n",
    "        step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe954d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
