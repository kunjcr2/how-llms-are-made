{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzmVbpW1SQVu"
      },
      "source": [
        "# **Creating GPT model architecture**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6v9p_oriTHz"
      },
      "source": [
        "## GPT-2-124M Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLBpX68pSY5J"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,  # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 768,  # Embedding dimension\n",
        "    \"n_layers\": 12,  # Number of transformer blocks\n",
        "    \"n_heads\": 12,  # Number of attention heads per transformer block\n",
        "    \"drop_rate\": 0.1,  # Dropout rate\n",
        "    \"qkv_bias\": False,  # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0TKKjdDSVeL"
      },
      "source": [
        "## Dummy GPT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2S6h2cxTU2p"
      },
      "source": [
        "this is just a placehoder thingy. No working stuff. Dont worry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvDWbJqCSNFG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        # Token embeddings as well as positional embedding lookup tables\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        # Using Dummy transformer block, total of n_layers times\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        # Using a dummy class for LayerNorm\n",
        "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "\n",
        "        # Getting token embeddings and then positional embeddings and then adding those for input embeddings\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "\n",
        "        # Applying dropout, passing through transformer block and then final normalization\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        # getting probability matrix\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # A simple placeholder\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This block does nothing and just returns its input.\n",
        "        return x\n",
        "\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        # The parameters here are just to mimic the LayerNorm interface.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This layer does nothing and just returns its input.\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXzM2Lmqa9mj"
      },
      "source": [
        "## Layer normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOUXhPIga_U8"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nOeKMnWdxsp"
      },
      "source": [
        "just using sequential and linear layers to get the weights to let the batches pass through once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lP9X08qbA9_",
        "outputId": "424ca7b5-aced-4ae2-d28b-0b562bd2a0e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0000, 0.7230, 0.2197, 0.0000, 0.8547, 0.0000],\n",
            "        [0.0000, 0.6109, 0.1887, 0.0141, 1.1073, 0.0000]],\n",
            "       grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "batch_example = torch.rand(2, 5)\n",
        "layer = torch.nn.Sequential(torch.nn.Linear(5, 6), torch.nn.ReLU())\n",
        "output = layer(batch_example)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQm8ESWKdviC"
      },
      "source": [
        "getting mean and variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tsnkaKubN7_",
        "outputId": "8b90d8d6-f47e-41a6-8a7f-f987d4bed577"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0.2996],\n",
              "         [0.3202]], grad_fn=<MeanBackward1>),\n",
              " tensor([[0.1526],\n",
              "         [0.2040]], grad_fn=<VarBackward0>))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean = output.mean(dim=-1, keepdim=True)\n",
        "var = output.var(dim=-1, keepdim=True)\n",
        "\n",
        "mean, var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGqDa4lQdqa5"
      },
      "source": [
        "we're doing x-u/sqrt(var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97Ab4IpPdMm_"
      },
      "outputs": [],
      "source": [
        "res = (output - mean) / torch.sqrt(var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82gFiFcrdhsT",
        "outputId": "fca14320-d38e-4d6a-8cb9-b103312d014b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-5.9605e-08, grad_fn=<MeanBackward0>)\n",
            "tensor(1.0000, grad_fn=<VarBackward0>)\n",
            "tensor(4.9671e-08, grad_fn=<MeanBackward0>)\n",
            "tensor(1., grad_fn=<VarBackward0>)\n"
          ]
        }
      ],
      "source": [
        "for i in res:\n",
        "  print(i.mean())\n",
        "  print(i.var())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI_xMn-Hee-U"
      },
      "source": [
        "THE CLASSSSSSSSSSSSSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIoyT1dudnfQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    # we do +self.eps, to let the var not be 0 and division by 0 SHOULD not be done\n",
        "    norm_x = (x-mean)/torch.sqrt(var + self.eps)\n",
        "\n",
        "    # We use scale and shift for better training and they are trainable also !!!\n",
        "    return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT7eq9Q4iPKt"
      },
      "source": [
        "Heck yeah !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKAcVC32fxaZ",
        "outputId": "b2157016-6c53-4fa7-d350-ba11a554d49d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[-2.3842e-08],\n",
              "         [ 3.5763e-07]], grad_fn=<MeanBackward1>),\n",
              " tensor([[0.9999],\n",
              "         [0.9996]], grad_fn=<VarBackward0>))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln = ln(batch_example)\n",
        "out_ln.mean(dim=-1, keepdim=True), out_ln.var(dim=-1, keepdim=True, unbiased=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGB3fbri7GHx"
      },
      "source": [
        "## GeLU activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jsijBTHBSwX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re930vKA7JEH"
      },
      "outputs": [],
      "source": [
        "class GeLU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  # Better version of ReLU()\n",
        "  def forward(self, x):\n",
        "    return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2/torch.pi))* (x + 0.044715*x**3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIOyTHqZBPvi"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    # Feed forward network with GeLU between 2 linear\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
        "        GeLU(),\n",
        "        nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozdk20lIEdPy",
        "outputId": "4d70e44f-9fc8-4ec7-ed05-d788ffcb143f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[0.1575, 0.4331, 0.1776,  ..., 0.3492, 0.0244, 0.2455],\n",
              "          [0.9566, 0.5933, 0.6393,  ..., 0.6167, 0.0435, 0.2261],\n",
              "          [0.4319, 0.2730, 0.2293,  ..., 0.6945, 0.5290, 0.0227]],\n",
              " \n",
              "         [[0.9675, 0.9138, 0.5699,  ..., 0.9252, 0.4692, 0.9113],\n",
              "          [0.7298, 0.9616, 0.9957,  ..., 0.1108, 0.0734, 0.1066],\n",
              "          [0.5184, 0.4737, 0.3395,  ..., 0.4596, 0.9735, 0.7594]]]),\n",
              " tensor([[[ 0.1558,  0.0788,  0.0144,  ..., -0.1708,  0.0179, -0.0577],\n",
              "          [ 0.0983,  0.0398, -0.0635,  ..., -0.1152,  0.0316, -0.1472],\n",
              "          [ 0.1592,  0.0591, -0.0759,  ..., -0.1235, -0.1080, -0.0653]],\n",
              " \n",
              "         [[ 0.2360,  0.0097, -0.0355,  ..., -0.0814,  0.0767, -0.0155],\n",
              "          [ 0.1177, -0.0355, -0.0075,  ..., -0.0553, -0.0595, -0.1363],\n",
              "          [ 0.1997,  0.0751,  0.0651,  ..., -0.1827,  0.0676, -0.1640]]],\n",
              "        grad_fn=<ViewBackward0>))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ff = FeedForward(GPT_CONFIG_124M)\n",
        "ones = torch.rand(2, 3, 768)\n",
        "out_ff = ff(ones)\n",
        "ones, out_ff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UazkAXthIZvk"
      },
      "source": [
        "## ShortCut Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHJ9bfArKd_f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAu2TWk-Zrjk"
      },
      "outputs": [],
      "source": [
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "    def __init__(self, layer_sizes, use_shortcut):\n",
        "        super().__init__()\n",
        "        self.use_shortcut = use_shortcut\n",
        "\n",
        "        # The list of all the layers for this test class\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GeLU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GeLU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GeLU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GeLU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GeLU())\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            # Compute the output of the current layer\n",
        "            layer_output = layer(x)\n",
        "            # Check if shortcut can be applied, if yes, we add input into output, else just the ouput\n",
        "            if self.use_shortcut and x.shape == layer_output.shape:\n",
        "                x = x + layer_output\n",
        "            else:\n",
        "                x = layer_output\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9LnKn46Z4G5",
        "outputId": "4c92d129-cdce-43be-fa20-6ea348c98b79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0.2493]], grad_fn=<MulBackward0>),\n",
              " tensor([[-0.0259]], grad_fn=<MulBackward0>))"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "layers = [3,3,3,3,3,1]\n",
        "dnn_1 = ExampleDeepNeuralNetwork(layers,  use_shortcut=False)\n",
        "dnn_2 = ExampleDeepNeuralNetwork(layers,  use_shortcut=True)\n",
        "r = torch.rand(1, 3)\n",
        "dnn_1(r), dnn_2(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZz0LaT97T66"
      },
      "source": [
        "## **Complete transformer block**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgM9GQVZjEbH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,  # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 768,  # Embedding dimension\n",
        "    \"n_layers\": 12,  # Number of\n",
        "    \"n_heads\": 12,  # Number of attention heads per transformer block\n",
        "    \"drop_rate\": 0.1,  # Dropout rate\n",
        "    \"qkv_bias\": False,  # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq8UX3mu71JT"
      },
      "outputs": [],
      "source": [
        "# Multiheaded attetion mechanism. Dude, this shit was fire !\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_token, d_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "    keys = keys.view(b, num_token, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_token, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_token, self.num_heads, self.head_dim)\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    attn_scores = queries @ keys.transpose(2,3) # we get (..., num_token, num_token)\n",
        "    masked_bool = self.mask.bool()[:num_token, :num_token]\n",
        "    attn_scores.masked_fill(masked_bool, -torch.inf)\n",
        "    attn_scores = attn_scores / keys.shape[-1]**0.5\n",
        "    attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = (attn_weights @ values).transpose(1,2)\n",
        "    context_vec = context_vec.contiguous().view(b, num_token, self.d_out)\n",
        "\n",
        "    return context_vec\n",
        "\n",
        "# We normalize the layer at the last dim with mean near to 0 and variance near to 1\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    # we do +self.eps, to let the var not be 0 and division by 0 SHOULD not be done\n",
        "    norm_x = (x-mean)/torch.sqrt(var + self.eps)\n",
        "\n",
        "    # We use scale and shift for better training and they are trainable also !!!\n",
        "    return self.scale * norm_x + self.shift\n",
        "\n",
        "# GeLU function activation\n",
        "class GeLU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  # Better version of ReLU()\n",
        "  def forward(self, x):\n",
        "    return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2/torch.pi))* (x + 0.044715*x**3)))\n",
        "\n",
        "# The classic feed froward neura network\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    # Feed forward network with GeLU between 2 linear\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
        "        GeLU(),\n",
        "        nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN9ehj4290ac"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    # Dude, no need for the comments here. You already know. It is a transformer block bro !\n",
        "    self.att = MultiHeadAttention(\n",
        "        d_in=cfg[\"emb_dim\"],\n",
        "        d_out=cfg[\"emb_dim\"],\n",
        "        context_length=cfg[\"context_length\"],\n",
        "        dropout=cfg[\"drop_rate\"],\n",
        "        num_heads=cfg[\"n_heads\"],\n",
        "        qkv_bias=cfg[\"qkv_bias\"]\n",
        "    )\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self,x):\n",
        "    # creating shortcut from x to the first dropout layer\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    # creating shortcut from first dropout to the second dropout\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiqSst12CuJU",
        "outputId": "e793d41f-9d06-4ab7-db8e-52e7c7f21b9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[0.8630, 0.3479, 0.8217,  ..., 0.6449, 0.2349, 0.7564],\n",
              "          [0.8813, 0.8789, 0.8641,  ..., 0.1024, 0.1728, 0.0935],\n",
              "          [0.8460, 0.5911, 0.9952,  ..., 0.0910, 0.1392, 0.5239]],\n",
              " \n",
              "         [[0.0131, 0.6674, 0.7115,  ..., 0.3334, 0.0476, 0.3675],\n",
              "          [0.4079, 0.4333, 0.5122,  ..., 0.9597, 0.1936, 0.6188],\n",
              "          [0.0487, 0.6183, 0.3731,  ..., 0.3049, 0.3739, 0.1200]]]),\n",
              " tensor([[[ 1.6255, -0.1657,  0.8310,  ...,  0.5347,  0.2914,  0.8870],\n",
              "          [ 1.3251,  0.8969,  1.1177,  ...,  0.1688,  0.3840,  0.5984],\n",
              "          [ 1.4692,  0.4756,  1.3227,  ..., -0.1366,  0.4886,  1.3945]],\n",
              " \n",
              "         [[ 0.2987,  0.9146,  0.3924,  ...,  0.0164, -0.1391, -0.0574],\n",
              "          [ 0.6332,  0.7505,  0.0119,  ...,  0.6402,  0.2357, -0.0370],\n",
              "          [ 0.0024,  0.7039,  0.0350,  ...,  0.6196,  0.2175, -0.5970]]],\n",
              "        grad_fn=<AddBackward0>))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = torch.rand(2,3,768)\n",
        "trfmblck = TransformerBlock(GPT_CONFIG_124M)\n",
        "input, trfmblck(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WydrnHFkDzHe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "O6v9p_oriTHz",
        "n0TKKjdDSVeL",
        "xGB3fbri7GHx",
        "UazkAXthIZvk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
