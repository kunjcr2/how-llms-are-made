GPT_CONFIG_124M = {
    "vocab_size": 50257,  # Vocabulary size
    "context_length": 256,  # Context length
    "emb_dim": 810,  # Embedding dimension
    "n_layers": 18,  # Number of
    "n_heads": 18,  # Number of attention heads per transformer block
    "drop_rate": 0.2,  # Dropout rate
    "qkv_bias": False,  # Query-Key-Value bias
}