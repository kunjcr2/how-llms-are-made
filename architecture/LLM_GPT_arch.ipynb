{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oqe50lsvju_R",
        "xZz0LaT97T66",
        "i9KNnDC1LF0H"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Architecture**"
      ],
      "metadata": {
        "id": "1yW9ZgzCjedU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Params"
      ],
      "metadata": {
        "id": "oqe50lsvju_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0-wTo87kDIr",
        "outputId": "d1ec2372-ad7b-4d82-a8b7-741ee6a270ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,  # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 768,  # Embedding dimension\n",
        "    \"n_layers\": 12,  # Number of\n",
        "    \"n_heads\": 12,  # Number of attention heads per transformer block\n",
        "    \"drop_rate\": 0.1,  # Dropout rate\n",
        "    \"qkv_bias\": False,  # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "M7a4EoGAj1x0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete transformer block"
      ],
      "metadata": {
        "id": "xZz0LaT97T66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiheaded attetion mechanism. Dude, this shit was fire !\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_token, d_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "    keys = keys.view(b, num_token, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_token, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_token, self.num_heads, self.head_dim)\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    attn_scores = queries @ keys.transpose(2,3) # we get (..., num_token, num_token)\n",
        "    masked_bool = self.mask.bool()[:num_token, :num_token]\n",
        "    attn_scores.masked_fill(masked_bool, -torch.inf)\n",
        "    attn_scores = attn_scores / keys.shape[-1]**0.5\n",
        "    attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = (attn_weights @ values).transpose(1,2)\n",
        "    context_vec = context_vec.contiguous().view(b, num_token, self.d_out)\n",
        "\n",
        "    return context_vec\n",
        "\n",
        "# We normalize the layer at the last dim with mean near to 0 and variance near to 1\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    # we do +self.eps, to let the var not be 0 and division by 0 SHOULD not be done\n",
        "    norm_x = (x-mean)/torch.sqrt(var + self.eps)\n",
        "\n",
        "    # We use scale and shift for better training and they are trainable also !!!\n",
        "    return self.scale * norm_x + self.shift\n",
        "\n",
        "# GeLU function activation\n",
        "class GeLU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  # Better version of ReLU()\n",
        "  def forward(self, x):\n",
        "    return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2/torch.pi))* (x + 0.044715*x**3)))\n",
        "\n",
        "# The classic feed froward neura network\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    # Feed forward network with GeLU between 2 linear\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
        "        GeLU(),\n",
        "        nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "Fq8UX3mu71JT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    # Dude, no need for the comments here. You already know. It is a transformer block bro !\n",
        "    self.att = MultiHeadAttention(\n",
        "        d_in=cfg[\"emb_dim\"],\n",
        "        d_out=cfg[\"emb_dim\"],\n",
        "        context_length=cfg[\"context_length\"],\n",
        "        dropout=cfg[\"drop_rate\"],\n",
        "        num_heads=cfg[\"n_heads\"],\n",
        "        qkv_bias=cfg[\"qkv_bias\"]\n",
        "    )\n",
        "    self.emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x = self.emb(x)\n",
        "    # creating shortcut from x to the first dropout layer\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    # creating shortcut from first dropout to the second dropout\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "gN9ehj4290ac"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerBlock(GPT_CONFIG_124M)"
      ],
      "metadata": {
        "id": "HorsO6d1kFnC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating new Output Tokens"
      ],
      "metadata": {
        "id": "i9KNnDC1LF0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "\n",
        "  # idx is (batch, n_tokens) array of indices in current context\n",
        "  for _ in range(max_new_tokens):\n",
        "    # If LLM suports only 5 tokens, and the context size is 10, then we only use last 5 toens as context.\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "\n",
        "    # Gettings the predictions\n",
        "    with torch.no_grad():\n",
        "      # Reshape idx_cond to (batch_size, sequence_length, emb_dim)\n",
        "      # idx_cond = idx_cond.unsqueeze(-1).repeat(1, 1, model.norm1.scale.shape[0]) # Or model.att.d_in to get the embedding dimension\n",
        "      logits = model(idx_cond) # (batch, num_tokens, vocab_size)\n",
        "\n",
        "    # We take the last row. We dont do anything to the batches neither to the last dimension of the vocabularies, but take the last row\n",
        "    logits = logits[:, -1, :] # (batch, vocab_size)\n",
        "\n",
        "    # getting probablities from the logits. We can say something like 50% chances of this, 2% chances of this...\n",
        "    probs = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
        "\n",
        "    # We see the highest value's index\n",
        "    idx_next = torch.argmax(probs, dim=-1, keepdim=True) # (batch, 1)\n",
        "\n",
        "    # Append the predicted token_id generated to the original index\n",
        "    idx = torch.cat((idx, idx_next), dim=1) # (batch, num_tokens+1)\n",
        "\n",
        "  return idx"
      ],
      "metadata": {
        "id": "Y0eNJ7R3LKov"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = \"Hello, I am \"\n",
        "encoded_start = tokenizer.encode(start)\n",
        "encoded_tensor = torch.tensor(encoded_start).unsqueeze(0)\n",
        "\n",
        "encoded_start, encoded_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1lhR-nMR4an",
        "outputId": "f2ce5964-7702-4548-a2e3-32fb262b9afd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([15496, 11, 314, 716, 220], tensor([[15496,    11,   314,   716,   220]]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # This puts model in evaluation thingy. It will bypass LayerNormaliztion and such.\n",
        "output = generate_text_simple(\n",
        "    model = model,\n",
        "    idx=encoded_tensor,\n",
        "    max_new_tokens=6,\n",
        "    context_size=GPT_CONFIG_124M['context_length']\n",
        ")\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyAltrjgSVC3",
        "outputId": "b1c8f294-0f5e-473f-910e-6f08f710dc31"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[15496,    11,   314,   716,   220,   454,     8,   427,   744,   367,\n",
              "           737]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(output.squeeze().tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "id": "33y3imMaUmEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b83cd281-48e2-4ac0-f75d-e31a8fd44eeb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am our) shround H).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ffFPd1yZrBB"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}