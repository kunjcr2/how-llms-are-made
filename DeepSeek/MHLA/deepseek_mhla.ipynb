{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW-IwADn9sRh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hlwjxkiv-J-c"
      },
      "outputs": [],
      "source": [
        "class RopelessMLA(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, kv_latent_dim):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model # Dimension of embeddings\n",
        "    self.n_heads = n_heads # Number of heads\n",
        "    self.dh = d_model // n_heads # dimensions of heads\n",
        "\n",
        "    self.W_q = nn.Linear(d_model, d_model, qkv_bias = False) # Wuery projection\n",
        "    self.W_dkv = nn.Linear(d_model, kv_latent_dim, qkv_bias=False) # Down projection\n",
        "    self.W_uk = nn.Linear(kv_latent_dim, d_model, qkv_bias = False) # Up projection to Keys\n",
        "    self.W_uv = nn.Linear(kv_latent_dim, d_model, qkv_bias = False) # Up projection to values\n",
        "    self.W_o = nn.Linear(d_model, d_model, qkv_bias = False) # Output projection\n",
        "\n",
        "    self.ln = nn.LayerNorm(kv_latent_dim) # Layer norm\n",
        "    self.register_buffer('absorbed_k', None) # Holds W_q @ W_uk\n",
        "\n",
        "  def forward(self, x, kv_cache=None, past_length=0):\n",
        "    B, S, D = x.size()\n",
        "\n",
        "    # Computing absorbed query once: W_q @ W_uk.T, Shape: (D, kv_latent_dim)\n",
        "      # Absorbed query matrix\n",
        "    if self.absorbed_k is None:\n",
        "      # Matmul directly transposes the second weight matrix\n",
        "      absorbed = torch.matmul(self.W_q.weight, self.W_uk.weight) # dim: (D, kv)\n",
        "      self.absorbed_k = absorbed.view(self.n_heads, self.dh, -1) # (num_heads, head_dim, latent_dim)\n",
        "\n",
        "    # Calculating kv_cache for new token\n",
        "      # If we dont have kv_cache, we assign new_kv_cache to variable c_kv\n",
        "    new_c_kv = self.ln(self.W_dkv(x)) # (B, S, kv_latent_dim)\n",
        "    if kv_cache is None:\n",
        "      c_kv = new_c_kv\n",
        "    else: # If we have alod cache, we join them\n",
        "      c_kv = torch.cat([kv_cache, new_c_kv], dim=1) # (B, s_full, kv_latent_dim)\n",
        "\n",
        "    S_full = c_kv.size(1)\n",
        "\n",
        "    # Working on values matrix\n",
        "    v_full = self.W_uv(c_kv) # (B, S_full, D)\n",
        "    v = v_full.view(B, S_full, self.n_heads, self.dh) # (B, S_full, num_heads, head_dim)\n",
        "\n",
        "    # Breaking input x since W_q is absorbed\n",
        "    q = x.view(B, S, self.n_heads, self.dh) # (B, S, num_heads, head_dim)\n",
        "\n",
        "    # Computing attention scores for the last token ONLY\n",
        "    attn_scores = torch.zeroes(B, self.n_heads, S, S_full, devoce=x.device)\n",
        "    # We first multiply first head of input with first head of absorbed query\n",
        "      # Then we multiply the product with transpose of c_kv to get the attention scores\n",
        "    for h in range(self.n_heads):\n",
        "      tmp = torch.matmul(q[:, :, h, :], self.absorbed_k[h, :, :]) # (B, S, kv_latent_dim)\n",
        "      attn_scores[:, h, :, :] = torch.bmm(tmp, c_kv.transpose(1,2)) # (B, S, kv_latent_dim)@(B, kv_latent_dim, s_full)=(B, S, S_full)\n",
        "\n",
        "    attn_scores = attn_scores / (self.dh**0.5) # variance near 1\n",
        "    mask = torch.tril(torch.ones((S, S_full), device=x.device), diagonal=past_length) # (S, S_full)\n",
        "    attn_scores = attn_scores.masked_fill(mask.view(1, 1, S, S_full) == 0, float('-inf'))\n",
        "\n",
        "    # Softmax on scores to get weights\n",
        "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    # Applying weights to each head o V sepratey\n",
        "    out_heads = []\n",
        "    for h in range(self.n_heads):\n",
        "      context_h = torch.matmul(attn_weights[:, h, :, :], v[:, :, h, :])\n",
        "      out_heads.append(context_h)\n",
        "\n",
        "    # concating all the out put heads together\n",
        "    out = torch.cat(out_heads, dim=-1)\n",
        "\n",
        "    # Returning after output projection\n",
        "    return self.W_o(out), c_kv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
