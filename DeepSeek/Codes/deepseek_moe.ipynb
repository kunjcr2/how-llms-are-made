{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57k8vY2QbsA-",
        "outputId": "7575b2de-b308-4bd2-9b57-3ea89ee2fba6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78c477fbd3f0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(69)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vujI75J_dofr"
      },
      "source": [
        "Downloading shakespear dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QdYlC6pb0oc",
        "outputId": "1448decf-55fa-4756-9c35-85095b694b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-11 22:28:42--  https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-07-11 22:28:42 (17.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qao9mE5QdtYq"
      },
      "source": [
        "## Starting with coding out FFNN and their experts individually.\n",
        "\n",
        "- We are using ReLU() as an activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wwWjQFiNddzy"
      },
      "outputs": [],
      "source": [
        "class Expert(nn.Module):\n",
        "  \"\"\"\n",
        "  Expert networkA simple MLP with a linear layer followed by a ReLU activation for each experts.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, embed_dim, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(embed_dim, 4*embed_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*embed_dim, embed_dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uROCsf00mYey"
      },
      "source": [
        "So now we need a routing matrix, which helps us in routing the input to various experts, along the entire MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AjCkrKGpr5Gh"
      },
      "outputs": [],
      "source": [
        "class NoisyTopK(nn.Module):\n",
        "  \"\"\"\n",
        "  This is the class for the routing matrix and getting the top k experts.\n",
        "  \"\"\"\n",
        "  def __init__(self, n_embed, n_experts, top_k):\n",
        "    super().__init__()\n",
        "    # This is the routing matrix which goes from embedding dim to number of experts and topk\n",
        "    self.linear = nn.Linear(n_embed, n_experts)\n",
        "    self.top_k = top_k\n",
        "\n",
        "    # A bit of noise\n",
        "    self.noise = nn.Linear(n_embed, n_experts)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Getting the expert selector matrix and then getting topk results from each dimensions\n",
        "    logits = self.linear(x)\n",
        "    noise_logits = self.noise(x)\n",
        "    noisy_logits = logits + noise_logits\n",
        "\n",
        "    topk_logits, topk_indices = torch.topk(noisy_logits, k=self.top_k, dim=2)\n",
        "\n",
        "    # we create a same shaped matrix with all being -inf and then wherever the indices are for topk, we leave that and make others -inf\n",
        "    zeroes = torch.full_like(noisy_logits, float('-inf'))\n",
        "    sparse_logits = zeroes.scatter(-1, topk_indices, topk_logits)\n",
        "    router_output = F.softmax(sparse_logits, dim=-1)\n",
        "\n",
        "    return router_output, topk_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sakyYiRyTg2u"
      },
      "source": [
        "We make something we call it as NosiyTopK gating which is basically to add some noise (Gaussian noise) to the Selector matrix for load balancing. Just a small addition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWa9PRvzWfay"
      },
      "source": [
        "## And finally sparse mixture of experts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bavOrkcRVTBx"
      },
      "outputs": [],
      "source": [
        "class SparseMoE(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, n_experts, top_k):\n",
        "    super().__init__()\n",
        "    self.router = NoisyTopK(embed_dim, n_experts, top_k)\n",
        "    self.experts = nn.ModuleList([Expert(embed_dim) for _ in range(n_experts)])\n",
        "    self.topk = top_k\n",
        "\n",
        "  def forward(self, x):\n",
        "    gating_output , indices = self.router(x)\n",
        "    final_output = torch.zeros_like(x)\n",
        "\n",
        "    # Reshaping for batch processing\n",
        "    flat_x = x.view(-1, x.size(-1)) # [batch, seq, emb] -> [batch*seq, emb]\n",
        "    flat_gatting_output = gating_output.view(-1, gating_output.size(-1)) # [batch, seq, n_experts] -> [batch*seq, n_experts]\n",
        "\n",
        "    # Processing each expert in parellel\n",
        "    for i, expert in enumerate(self.experts):\n",
        "      # Creating a mask where each token is routed to expert i\n",
        "        # For example, expert_mask = [True, False, False, True, ...]\n",
        "          # Shape: [batch, seq_len] — one True/False per token\n",
        "      expert_mask = (indices == i).any(dim=-1) # [batch, seq_len]\n",
        "\n",
        "      # Flattened to [batch * seq_len] so it matches flat_x\n",
        "      flat_mask = expert_mask.view(-1) # [batch * seq_len]\n",
        "\n",
        "      if flat_mask.any():\n",
        "        # WHERVER we have TRUE in flat_mask, we take those tokens from flat_x, we pass them through expert and we save those tokens\n",
        "          # At the exact places in final_output where we have true in corespondance to flat_mask\n",
        "        expert_input = flat_x[flat_mask]\n",
        "        expert_output = expert(expert_input)\n",
        "\n",
        "        # Extracting and applying gating scores\n",
        "        gating_scores = flat_gatting_output[flat_mask, i].unsqueeze(1)\n",
        "        weighted_expert_output = gating_scores * expert_output\n",
        "\n",
        "        # putting in weighted expert outputs to the final output matrix\n",
        "        final_output[expert_mask] += weighted_expert_output.squeeze(1)\n",
        "\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNIEiTC-f33l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
