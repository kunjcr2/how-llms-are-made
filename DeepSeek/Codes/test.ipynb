{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64d838c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import Rope "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5c29467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RopeAttention(nn.Module):\n",
    "\n",
    "    \"\"\"A placeholder for the RopeAttention module.\n",
    "    This module currently does not implement any functionality.\n",
    "    It is intended to be a stub for future development.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_heads, kv_latent_dim):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimension of embeddings\n",
    "        self.n_heads = n_heads # Number of heads\n",
    "        self.dh = d_model // n_heads # dimensions of heads\n",
    "\n",
    "        self.rope = Rope.Rope(d_model, 20000) # RoPE instance\n",
    "\n",
    "        self.W_dq = nn.Linear(d_model, kv_latent_dim, bias = False) # Query down projection\n",
    "        self.W_dkv = nn.Linear(d_model, kv_latent_dim, bias=False) # Down projection\n",
    "        \n",
    "        self.W_uk = nn.Linear(kv_latent_dim, d_model, bias = False) # Up projection to Keys\n",
    "        self.W_uv = nn.Linear(kv_latent_dim, d_model, bias = False) # Up projection to values\n",
    "        self.W_uq = nn.Linear(kv_latent_dim, d_model, bias = False) # Up projection to queries\n",
    "        \n",
    "        self.W_qr = nn.Linear(kv_latent_dim, d_model, bias = False) # Query projection for RoPE\n",
    "        self.W_kr = nn.Linear(d_model, self.dh, bias = False) # Key projection for RoPE\n",
    "        \n",
    "        self.W_o = nn.Linear(d_model, d_model, bias = False) # Output projection\n",
    "        self.ln = nn.LayerNorm(kv_latent_dim) # Layer norm\n",
    "\n",
    "    def forward(self, x, kv_cache=None, kr_cache=None, past_length=0):\n",
    "        B, S, D = x.size() # Batch size, sequence length, and embedding dimension\n",
    "\n",
    "        # Query down projection and attention scores\n",
    "        c_q = self.ln(self.W_dq(x)) # (B, S, kv_latent_dim) - down projection\n",
    "\n",
    "        #### WITHOUT ROPE ####\n",
    "\n",
    "        # queries up projection, first \n",
    "        q_c = self.W_uq(c_q).view(B, S, self.n_heads, self.dh) # (B, S, num_heads, head_dim)\n",
    "        attn_scores = torch.zeros(B, S, self.n_heads, self.dh, device=x.device) # (B, S, num_heads)\n",
    "\n",
    "        # Keys and values down projection\n",
    "        new_c_kv = self.ln(self.W_dkv(x)) # (B, S, kv_latent_dim) - down projection\n",
    "\n",
    "        # update cache\n",
    "        if kv_cache is None:\n",
    "            c_kv = new_c_kv\n",
    "        else:\n",
    "            c_kv = torch.cat([kv_cache, new_c_kv], dim=1)\n",
    "        \n",
    "        S_full = c_kv.size(1) # number of tokens in total cache\n",
    "\n",
    "        # keys and values up projection\n",
    "        k_c = self.W_uk(c_kv).view(B, S_full, self.n_heads, self.dh) # (B, S_full, num_heads, head_dim)\n",
    "        v_c = self.W_uv(c_kv).view(B, S_full, self.n_heads, self.dh) # (B, S_full, num_heads, head_dim)\n",
    "\n",
    "        #### WITH ROPE ####\n",
    "\n",
    "        # queries up projection\n",
    "        q_r = self.rope(self.W_qr(c_q)).view(B, S, self.n_heads, self.dh) # (B, S, num_heads, head_dim)\n",
    "\n",
    "        # Keys up projection\n",
    "        new_kr_cache = self.ln(self.W_kr(x)) # (B, S, dh) - down projection\n",
    "        if kr_cache is None:\n",
    "            k_r = new_kr_cache\n",
    "        else:\n",
    "            k_r = torch.cat([kr_cache, new_kr_cache], dim=1)\n",
    "\n",
    "        # Multiple heads for keys\n",
    "        k_r = torch.stack([k_r] * self.n_heads, dim=2) # (B, S_full, num_heads, head_dim)\n",
    "\n",
    "        #### JOINING ####\n",
    "        # Concatenate queries and keys\n",
    "        q = torch.cat([q_c, q_r], dim=3) # (B, S, num_heads, head_dim)\n",
    "        k = torch.cat([k_c, k_r], dim=3) # (B, S_full, num_heads, head_dim)\n",
    "\n",
    "        # Compute attention scores\n",
    "        for i in range(self.n_heads):\n",
    "            attn_scores[:, i, :, :] = torch.matmul(q[:, :, i, :], k[:, :, i, :].transpose(2, 3))\n",
    "\n",
    "        # Mask, softmax, and dropout\n",
    "        attn_scores = attn_scores / (self.dh ** 0.5)\n",
    "        mask = torch.tril(torch.ones((S, S_full), device=x.device), diagonal=past_length)\n",
    "        attn_scores = attn_scores.masked_fill(mask.view(1, 1, S, S_full) == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Context vector using attention weights and values\n",
    "        out_heads = []\n",
    "        for i in range(self.n_heads):\n",
    "            context_h = torch.matmul(attn_weights[:, :, i, :], v_c[:, :, i, :])\n",
    "            out_heads.append(context_h)\n",
    "        \n",
    "        out_heads = torch.stack(out_heads, dim=2) # (B, S, num_heads, head_dim)\n",
    "        out = out_heads.view(B, S, self.d_model)\n",
    "\n",
    "        out = self.W_o(out) # (B, S, d_model) - output projection\n",
    "\n",
    "        return out, c_kv, k_r  # Return output, key-value cache, and key RoPE cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29898e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = RopeAttention(d_model=32, n_heads=4, kv_latent_dim=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15fc1eea",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[16], expected input with shape [*, 16], but got input of size[2, 16, 8]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kunjs\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kunjs\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[48], line 62\u001b[0m, in \u001b[0;36mRopeAttention.forward\u001b[1;34m(self, x, kv_cache, kr_cache, past_length)\u001b[0m\n\u001b[0;32m     59\u001b[0m q_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_qr(c_q))\u001b[38;5;241m.\u001b[39mview(B, S, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdh) \u001b[38;5;66;03m# (B, S, num_heads, head_dim)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Keys up projection\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m new_kr_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_kr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, S, dh) - down projection\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kr_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     k_r \u001b[38;5;241m=\u001b[39m new_kr_cache\n",
      "File \u001b[1;32mc:\\Users\\kunjs\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kunjs\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kunjs\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kunjs\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\functional.py:2910\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2902\u001b[0m         layer_norm,\n\u001b[0;32m   2903\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2908\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2909\u001b[0m     )\n\u001b[1;32m-> 2910\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2911\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2912\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given normalized_shape=[16], expected input with shape [*, 16], but got input of size[2, 16, 8]"
     ]
    }
   ],
   "source": [
    "test(torch.randn(2, 16, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d4be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
