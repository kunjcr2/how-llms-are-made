{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "poR_yjThirkR",
        "a_3NiVpvAvA8",
        "v-AJ1moaioQJ",
        "mMs1httjNzrV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We're using simplfied attention mechanism and we're using a 3 dimensional vector for all these."
      ],
      "metadata": {
        "id": "HagcEZ_rOG_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attention Mechanism**"
      ],
      "metadata": {
        "id": "ZEB5e7QLirpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simplified Attention**"
      ],
      "metadata": {
        "id": "poR_yjThirkR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf2lqSzkKEuq"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [0.43, 0.15, 0.89], # Your\n",
        "        [0.55, 0.87, 0.66], # Journey\n",
        "        [0.57, 0.85, 0.64], # starts\n",
        "        [0.22, 0.58, 0.33], # with\n",
        "        [0.77, 0.25, 0.1],  # one\n",
        "        [0.05, 0.80, 0.55]  # step\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "sHgS9o27OoGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DAMNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN**\n",
        "\n",
        "It was the dot product"
      ],
      "metadata": {
        "id": "nsO6GgQYUBXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "  attn_scores_2[i] = torch.dot(x_i, query)\n",
        "\n",
        "attn_scores_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrHx--mQPD4y",
        "outputId": "ba884aad-9c37-41a4-ba4d-b2cdbc25b574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing"
      ],
      "metadata": {
        "id": "fyYjh_PqWK-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not a greate way tbh. Let's do Softmax. Why ? IN NOTES !!!!\n",
        "\n",
        "Let's skip naive softmax and just do pytorch softmax"
      ],
      "metadata": {
        "id": "efYu3r4wWMSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "attn_weights_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxZQDoQUWBTH",
        "outputId": "586b9eb8-67fa-45b9-874b-c74f88313ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i, attn_weight in enumerate(attn_weights_2):\n",
        "  context_vec_2 += attn_weight * inputs[i]"
      ],
      "metadata": {
        "id": "Jcqe3aHjYiJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cC7sioocmiB",
        "outputId": "8fb1ca68-24ad-401d-82ec-58c9379b27f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4419, 0.6515, 0.5683])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.empty(6,6)\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "  for j, x_j in enumerate(inputs):\n",
        "    attn_scores[i][j] = torch.dot(x_i, x_j)\n",
        "\n",
        "attn_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URFUi77Zcrqb",
        "outputId": "026a8592-5986-44c7-a024-68124ceac891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
              "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
              "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
              "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
              "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
              "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the above is VERY EXPENSIVE computationally.\n",
        "\n",
        "And hnce we use MUCH MORE EFFICIENT thing"
      ],
      "metadata": {
        "id": "odM-l-C-fBgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = inputs @ inputs.T\n",
        "attn_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvbdZrxZfE0e",
        "outputId": "640ce417-fe25-4c14-8588-c971c9f1c870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
              "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
              "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
              "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
              "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
              "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHfGs1v4eFSV",
        "outputId": "d6d29201-7f50-4d82-e368-6095a62e6d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
              "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
              "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
              "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
              "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
              "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec = attn_weights @ inputs\n",
        "context_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTCWApvpeKwA",
        "outputId": "40f147f4-2ed7-4d9c-9354-b6ed64c1403b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4421, 0.5931, 0.5790],\n",
              "        [0.4419, 0.6515, 0.5683],\n",
              "        [0.4431, 0.6496, 0.5671],\n",
              "        [0.4304, 0.6298, 0.5510],\n",
              "        [0.4671, 0.5910, 0.5266],\n",
              "        [0.4177, 0.6503, 0.5645]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moving to self-attention with trainable weights"
      ],
      "metadata": {
        "id": "a_3NiVpvAvA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "crrldk_HDXrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [0.43, 0.15, 0.89], # Your\n",
        "        [0.55, 0.87, 0.66], # Journey\n",
        "        [0.57, 0.85, 0.64], # starts\n",
        "        [0.22, 0.58, 0.33], # with\n",
        "        [0.77, 0.25, 0.1],  # one\n",
        "        [0.05, 0.80, 0.55]  # step\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "hGxVILJgeeNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have query, key and weight matrices in random values\n",
        "\n",
        "- Parameter is basically to tell PyTorch that this tensor is \"trainable\" tensor. More on that, ask GPT."
      ],
      "metadata": {
        "id": "FT4piEe0dHbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets do queries, keys and values matrices for x_2 (Journey)."
      ],
      "metadata": {
        "id": "A7g19xx6e6r0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get the query of all the inputs"
      ],
      "metadata": {
        "id": "K5sDaYLnfd5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets get the attention scores matrix of all inputs with every other inputs.\n",
        "- we just multiply queries (6x2) with keys (2x6) to get the nice trainable matrix of attention scores that is (6x6)"
      ],
      "metadata": {
        "id": "Iwcm7kHzkYTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets start by getting the attention weights.\n",
        "\n",
        "We divide the atention scores with sqrt(d_of_keys) and then apply softmax to get the attention weights."
      ],
      "metadata": {
        "id": "wh_4wZnxpKnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets implement the Self Attention classes."
      ],
      "metadata": {
        "id": "0H6Kvgl92Wpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super().__init__()\n",
        "    self.w_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.w_key = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.w_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "  def forward(self, x):\n",
        "    queries = x @ self.w_query\n",
        "    keys = x @ self.w_key\n",
        "    values = x @ self.w_value\n",
        "\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "Bpu5rSYS1wc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_1 = SelfAttention_v1(inputs.shape[-1], 2)\n",
        "att_1(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZIsB2mU4wLo",
        "outputId": "9e7c246f-57dd-41ea-d99b-2a65358b78c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7966, 0.5731],\n",
              "        [0.8082, 0.5837],\n",
              "        [0.8076, 0.5832],\n",
              "        [0.7848, 0.5610],\n",
              "        [0.7835, 0.5592],\n",
              "        [0.7921, 0.5685]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use nn.Linear, instead of nn.Parameter() as it automatically initializes the weight matrices (internally call nn.Parameter()) which is good for computations and also, it is much stable and effective at model training."
      ],
      "metadata": {
        "id": "gjj14daA8rgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super().__init__()\n",
        "    self.w_query = nn.Linear(d_in, d_out, bias=False)\n",
        "    self.w_key = nn.Linear(d_in, d_out, bias=False)\n",
        "    self.w_value = nn.Linear(d_in, d_out, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys = self.w_key(x)\n",
        "    queries = self.w_query(x)\n",
        "    values = self.w_value(x)\n",
        "\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "1gTuO9Lf44gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_2 = SelfAttention_v2(inputs.shape[-1], 2)\n",
        "att_2(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yepqu8cs9jmv",
        "outputId": "11ed2719-7ac1-4c8c-9189-4637e2796897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2334, -0.1073],\n",
              "        [ 0.2312, -0.1064],\n",
              "        [ 0.2314, -0.1064],\n",
              "        [ 0.2305, -0.1062],\n",
              "        [ 0.2339, -0.1065],\n",
              "        [ 0.2291, -0.1061]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Causal Attention**"
      ],
      "metadata": {
        "id": "v-AJ1moaioQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "MJQBP9sjIeIo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [0.43, 0.15, 0.89], # Your\n",
        "        [0.55, 0.87, 0.66], # Journey\n",
        "        [0.57, 0.85, 0.64], # starts\n",
        "        [0.22, 0.58, 0.33], # with\n",
        "        [0.77, 0.25, 0.1],  # one\n",
        "        [0.05, 0.80, 0.55]  # step\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "7PyRX_wJyWN0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get triangular matrix using torch.tril and then NEVERMIND\n",
        "\n",
        "WE PASS IN THE ATTENTION WEIGHTS TO TORCH.TRIL TO GET MASK. HAHAHA, HE MISSED IT."
      ],
      "metadata": {
        "id": "IBWgG1e6psfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing"
      ],
      "metadata": {
        "id": "jS0TjHCZrdXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We make 1s and 0s in mask tensor with upper side of diagonal with 1s and lower with 0s.\n",
        "We than convert the att_scores part to -inf wherever there is 1 in mask."
      ],
      "metadata": {
        "id": "KjMfe5C21F9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now softmax"
      ],
      "metadata": {
        "id": "NBN4hv821ewg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create a class now"
      ],
      "metadata": {
        "id": "03FsOw6IIjO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets do batches"
      ],
      "metadata": {
        "id": "KwKgECRfLG6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CasualAttention(nn.Module):\n",
        "\n",
        "  # We define output dimension, context_length (length of one item in the batch), dropout rate\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "\n",
        "    # key, query and value weight matrices as well as dropout layer\n",
        "    self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # We initialize a buffer which is basically a upper traingular matrix\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # getting batch, number of tokens or context length as well as the embedding vector size\n",
        "    batch, num_token, d_in = x.shape # Batches b\n",
        "\n",
        "    # keys, values and queries matrices\n",
        "    keys = self.w_key(x)\n",
        "    values = self.w_value(x)\n",
        "    queries = self.w_query(x)\n",
        "\n",
        "    # Getting attention scores by transposing keys and using last two dimensions\n",
        "    attn_scores = queries @ keys.transpose(1,2)\n",
        "\n",
        "    # Changing 1s in the attention scores matrix to -inf\n",
        "    # we also use [:num_token, :num_token], because what if we ONLY need 4x4 as the number of tokens are less than the context_size\n",
        "    attn_scores = attn_scores.masked_fill(self.mask.bool()[:num_token, :num_token], -torch.inf)\n",
        "\n",
        "    # Applying softmax to zero out the -infs and basically hiding the future tokens by creating attn_scores of those = 0\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights) # Applying dropout\n",
        "\n",
        "    context_vector = attn_weights @ values  # Context vectors\n",
        "\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "GlnsmtlTKUBd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs))"
      ],
      "metadata": {
        "id": "HZraF6Aly2Q_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cas_attn = CasualAttention(\n",
        "  d_in=3,\n",
        "  d_out=2,\n",
        "  context_length=6,\n",
        "  dropout=0\n",
        ")"
      ],
      "metadata": {
        "id": "kHgAmkl7HqHx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cas_attn(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDC6_rCZyNOv",
        "outputId": "810f886a-2c8c-4cd4-9b71-d98b5ced3ae4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.3541, -0.2838],\n",
              "         [ 0.1350, -0.2300],\n",
              "         [ 0.0593, -0.2145],\n",
              "         [ 0.0101, -0.1688],\n",
              "         [ 0.0072, -0.1969],\n",
              "         [-0.0232, -0.1566]],\n",
              "\n",
              "        [[ 0.3541, -0.2838],\n",
              "         [ 0.1350, -0.2300],\n",
              "         [ 0.0593, -0.2145],\n",
              "         [ 0.0101, -0.1688],\n",
              "         [ 0.0072, -0.1969],\n",
              "         [-0.0232, -0.1566]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MULTIHEAD ATTENTION MECHANISM**\n"
      ],
      "metadata": {
        "id": "mMs1httjNzrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "we create multiple instances of casual self attention mechanism and concat them."
      ],
      "metadata": {
        "id": "0bY4Q8w0S9GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "\n",
        "    # we create a list of classes and store it in heads using nn.Module.\n",
        "    # there are basically num_heads number of heads or CasualAttention classes initialized\n",
        "    self.heads = nn.ModuleList(\n",
        "        [CasualAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "  # We concat all the context vectors we get after passing x to each head in heads\n",
        "  def forward(self, x):\n",
        "    return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "8UQwKIHRysw_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well the above thing is very inefficient as we are initializing one vector and that getting context for 1.\n",
        "Then other. then other. then other.\n",
        "\n",
        "But we will make it better on next lecture cuz that shit is official as fck. That is what openai used for gpts."
      ],
      "metadata": {
        "id": "vBLr-SF9TR0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ma = MultiHeadAttentionWrapper(3, 2, 6, 0000, 2)"
      ],
      "metadata": {
        "id": "AMgw9vF_Ox7L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ma(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGNNf_yKO3y_",
        "outputId": "e5f3dd9e-792b-4889-dd01-f97038777765"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3051,  0.5048,  0.0693,  0.4790],\n",
              "         [-0.4397,  0.5333,  0.0808,  0.3125],\n",
              "         [-0.4840,  0.5410,  0.0903,  0.2603],\n",
              "         [-0.4409,  0.4772,  0.0673,  0.1920],\n",
              "         [-0.4328,  0.4419,  0.1307,  0.2116],\n",
              "         [-0.4174,  0.4301,  0.0840,  0.1605]],\n",
              "\n",
              "        [[-0.3051,  0.5048,  0.0693,  0.4790],\n",
              "         [-0.4397,  0.5333,  0.0808,  0.3125],\n",
              "         [-0.4840,  0.5410,  0.0903,  0.2603],\n",
              "         [-0.4409,  0.4772,  0.0673,  0.1920],\n",
              "         [-0.4328,  0.4419,  0.1307,  0.2116],\n",
              "         [-0.4174,  0.4301,  0.0840,  0.1605]]], grad_fn=<CatBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Better MultiHeadAttention"
      ],
      "metadata": {
        "id": "2PKppPLCSirc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        # Basically a thing between if-else and try-catch block\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        # we keep a output dimension, number of heads and dimension of single head's context_vec\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        # Weights for q, k, v\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        # Dropout as well as a buffer for masked triangular matrix\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_token, d_in = x.shape\n",
        "\n",
        "    # getting k, q, v\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    # (b, num_token, d_out) -> (b, num_token, num_head, head_dim) for queries, keys, values\n",
        "    keys = keys.view(b, num_token, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_token, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_token, self.num_heads, self.head_dim)\n",
        "\n",
        "    # (b, num_token, num_head, head_dim) -> (b, num_head, num_token, head_dim) to get a better hand on doing multiplications\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "\n",
        "    # The main multiplication happens between (..., num_token, head_dim) and (..., head_dim, num_tokens)\n",
        "    attn_scores = queries @ keys.transpose(2,3) # we get (..., num_token, num_token)\n",
        "\n",
        "    # MASKINGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG and getting attention weights\n",
        "    masked_bool = self.mask.bool()[:num_token, :num_token]\n",
        "    attn_scores.masked_fill(masked_bool, -torch.inf)\n",
        "    attn_scores = attn_scores / keys.shape[-1]**0.5\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    # After getting context_vec, we do (b, num_head, num_token, head_dim) -> (b, num_token, num_head, head_dim)\n",
        "    context_vec = (attn_weights @ values).transpose(1,2)\n",
        "\n",
        "    # And finally we eat last two dimensions to (b, num_token, num_head, head_dim) -> (b, num_token, d_out)\n",
        "    context_vec = context_vec.contiguous().view(b, num_token, self.d_out)\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "3ZDy1crTO7KK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ma = MultiHeadAttention(3, 2, 6, 0.0, 2)"
      ],
      "metadata": {
        "id": "ytwPX4j0mZah"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ma(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_SV46U7mfa3",
        "outputId": "ca5a0bfa-14d9-4471-f235-b84fb29b9913"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0137,  0.5634],\n",
              "         [-0.0137,  0.5643],\n",
              "         [-0.0137,  0.5643],\n",
              "         [-0.0136,  0.5647],\n",
              "         [-0.0137,  0.5645],\n",
              "         [-0.0136,  0.5647]],\n",
              "\n",
              "        [[-0.0137,  0.5634],\n",
              "         [-0.0137,  0.5643],\n",
              "         [-0.0137,  0.5643],\n",
              "         [-0.0136,  0.5647],\n",
              "         [-0.0137,  0.5645],\n",
              "         [-0.0136,  0.5647]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRYmQkZ5mg3B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}