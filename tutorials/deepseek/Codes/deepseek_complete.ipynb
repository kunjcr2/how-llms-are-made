{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> # **STILL NEED TO WORK ON THIS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AsWF8oe2peVY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHaxfiSYpms8",
        "outputId": "60ab2294-14b2-4717-c0cb-f084007eb818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-11 22:41:29--  https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-07-11 22:41:29 (12.4 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "euX40PU_pokU"
      },
      "outputs": [],
      "source": [
        "class Expert(nn.Module):\n",
        "  \"\"\"\n",
        "  Expert networkA simple MLP with a linear layer followed by a ReLU activation for each experts.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, embed_dim, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(embed_dim, 4*embed_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*embed_dim, embed_dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class NoisyTopK(nn.Module):\n",
        "  \"\"\"\n",
        "  This is the class for the routing matrix and getting the top k experts.\n",
        "  \"\"\"\n",
        "  def __init__(self, n_embed, n_experts, top_k):\n",
        "    super().__init__()\n",
        "    # This is the routing matrix which goes from embedding dim to number of experts and topk\n",
        "    self.linear = nn.Linear(n_embed, n_experts)\n",
        "    self.top_k = top_k\n",
        "\n",
        "    # A bit of noise\n",
        "    self.noise = nn.Linear(n_embed, n_experts)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Getting the expert selector matrix and then getting topk results from each dimensions\n",
        "    logits = self.linear(x)\n",
        "    noise_logits = self.noise(x)\n",
        "    noisy_logits = logits + noise_logits\n",
        "\n",
        "    topk_logits, topk_indices = torch.topk(noisy_logits, k=self.top_k, dim=2)\n",
        "\n",
        "    # we create a same shaped matrix with all being -inf and then wherever the indices are for topk, we leave that and make others -inf\n",
        "    zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "    sparse_logits = zeros.scatter(-1, topk_indices, topk_logits)\n",
        "    router_output = F.softmax(sparse_logits, dim=-1)\n",
        "\n",
        "    return router_output, topk_indices\n",
        "\n",
        "class SparseMoE(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, n_experts, top_k):\n",
        "    super().__init__()\n",
        "    self.router = NoisyTopK(embed_dim, n_experts, top_k)\n",
        "    self.experts = nn.ModuleList([Expert(embed_dim) for _ in range(n_experts)])\n",
        "    self.topk = top_k\n",
        "\n",
        "  def forward(self, x):\n",
        "    gating_output , indices = self.router(x)\n",
        "    final_output = torch.zeros_like(x)\n",
        "\n",
        "    # Reshaping for batch processing\n",
        "    flat_x = x.view(-1, x.size(-1)) # [batch, seq, emb] -> [batch*seq, emb]\n",
        "    flat_gatting_output = gating_output.view(-1, gating_output.size(-1)) # [batch, seq, n_experts] -> [batch*seq, n_experts]\n",
        "\n",
        "    # Processing each expert in parellel\n",
        "    for i, expert in enumerate(self.experts):\n",
        "      # Creating a mask where each token is routed to expert i\n",
        "        # For example, expert_mask = [True, False, False, True, ...]\n",
        "          # Shape: [batch, seq_len] — one True/False per token\n",
        "      expert_mask = (indices == i).any(dim=-1) # [batch, seq_len]\n",
        "\n",
        "      # Flattened to [batch * seq_len] so it matches flat_x\n",
        "      flat_mask = expert_mask.view(-1) # [batch * seq_len]\n",
        "\n",
        "      if flat_mask.any():\n",
        "        # WHERVER we have TRUE in flat_mask, we take those tokens from flat_x, we pass them through expert and we save those tokens\n",
        "          # At the exact places in final_output where we have true in corespondance to flat_mask\n",
        "        expert_input = flat_x[flat_mask]\n",
        "        expert_output = expert(expert_input)\n",
        "\n",
        "        # Extracting and applying gating scores\n",
        "        gating_scores = flat_gatting_output[flat_mask, i].unsqueeze(1)\n",
        "        weighted_expert_output = gating_scores * expert_output\n",
        "\n",
        "        # putting in weighted expert outputs to the final output matrix\n",
        "        final_output[expert_mask] += weighted_expert_output.squeeze(1)\n",
        "\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "p88YtDrQpvDM"
      },
      "outputs": [],
      "source": [
        "class MultiheadLatentAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, kv_latent_dim):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model # Dimension of embeddings\n",
        "    self.n_heads = n_heads # Number of heads\n",
        "    self.dh = d_model // n_heads # dimensions of heads\n",
        "\n",
        "    self.W_q = nn.Linear(d_model, d_model) # Wuery projection\n",
        "    self.W_dkv = nn.Linear(d_model, kv_latent_dim) # Down projection\n",
        "    self.W_uk = nn.Linear(kv_latent_dim, d_model) # Up projection to Keys\n",
        "    self.W_uv = nn.Linear(kv_latent_dim, d_model) # Up projection to values\n",
        "    self.W_o = nn.Linear(d_model, d_model) # Output projection\n",
        "\n",
        "    self.ln = nn.LayerNorm(kv_latent_dim) # Layer norm\n",
        "    self.register_buffer('absorbed_k', None) # Holds W_q @ W_uk\n",
        "\n",
        "  def forward(self, x, kv_cache=None, past_length=0):\n",
        "    B, S, D = x.size()\n",
        "\n",
        "    # Computing absorbed query once: W_q @ W_uk.T, Shape: (D, kv_latent_dim)\n",
        "      # Absorbed query matrix\n",
        "    if self.absorbed_k is None:\n",
        "      # Matmul directly transposes the second weight matrix\n",
        "      absorbed = torch.matmul(self.W_q.weight, self.W_uk.weight) # dim: (D, kv)\n",
        "      self.absorbed_k = absorbed.view(self.n_heads, self.dh, -1) # (num_heads, head_dim, latent_dim)\n",
        "\n",
        "    # Calculating kv_cache for new token\n",
        "      # If we dont have kv_cache, we assign new_kv_cache to variable c_kv\n",
        "    new_c_kv = self.ln(self.W_dkv(x)) # (B, S, kv_latent_dim)\n",
        "    if kv_cache is None:\n",
        "      c_kv = new_c_kv\n",
        "    else: # If we have alod cache, we join them\n",
        "      c_kv = torch.cat([kv_cache, new_c_kv], dim=1) # (B, s_full, kv_latent_dim)\n",
        "\n",
        "    S_full = c_kv.size(1)\n",
        "\n",
        "    # Working on values matrix\n",
        "    v_full = self.W_uv(c_kv) # (B, S_full, D)\n",
        "    v = v_full.view(B, S_full, self.n_heads, self.dh) # (B, S_full, num_heads, head_dim)\n",
        "\n",
        "    # Breaking input x since W_q is absorbed\n",
        "    q = x.view(B, S, self.n_heads, self.dh) # (B, S, num_heads, head_dim)\n",
        "\n",
        "    # Computing attention scores for the last token ONLY\n",
        "    attn_scores = torch.zeros(B, self.n_heads, S, S_full, device=x.device)\n",
        "    # We first multiply first head of input with first head of absorbed query\n",
        "      # Then we multiply the product with transpose of c_kv to get the attention scores\n",
        "    for h in range(self.n_heads):\n",
        "      tmp = torch.matmul(q[:, :, h, :], self.absorbed_k[h, :, :]) # (B, S, kv_latent_dim)\n",
        "      attn_scores[:, h, :, :] = torch.bmm(tmp, c_kv.transpose(1,2)) # (B, S, kv_latent_dim)@(B, kv_latent_dim, s_full)=(B, S, S_full)\n",
        "\n",
        "    attn_scores = attn_scores / (self.dh**0.5) # variance near 1\n",
        "    mask = torch.tril(torch.ones((S, S_full), device=x.device), diagonal=past_length) # (S, S_full)\n",
        "    attn_scores = attn_scores.masked_fill(mask.view(1, 1, S, S_full) == 0, float('-inf'))\n",
        "\n",
        "    # Softmax on scores to get weights\n",
        "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    # Applying weights to each head o V sepratey\n",
        "    out_heads = []\n",
        "    for h in range(self.n_heads):\n",
        "      context_h = torch.matmul(attn_weights[:, h, :, :], v[:, :, h, :])\n",
        "      out_heads.append(context_h)\n",
        "\n",
        "    # concating all the out put heads together\n",
        "    out = torch.cat(out_heads, dim=-1)\n",
        "\n",
        "    # Returning after output projection\n",
        "    return self.W_o(out), c_kv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XCwSg2IYq1IW"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head, kv_latent_dim, n_experts, top_k):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiheadLatentAttention(n_embed, n_head, kv_latent_dim)\n",
        "    self.smoe = SparseMoE(n_embed, n_experts, top_k)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    sa_output, kv_cache = self.sa(self.ln1(x))\n",
        "    x = x + sa_output\n",
        "    x = x + self.smoe(self.ln2(x))\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYZNyjUfv8zR"
      },
      "outputs": [],
      "source": [
        "class We_have_deepseek_at_home(nn.Module):\n",
        "    def __init__(self, emb_dim, n_head, kv_latent_dim, n_experts, top_k):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.pos_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.drop_emb = nn.Dropout(0.1)\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[Block(emb_dim, n_head, kv_latent_dim, n_experts, top_k) for _ in range(12)])\n",
        "\n",
        "        self.final_norm = LayerNorm(emb_dim)\n",
        "        self.out_head = nn.Linear(\n",
        "            emb_dim, vocab_size, bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
