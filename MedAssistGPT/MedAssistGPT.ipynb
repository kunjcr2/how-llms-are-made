{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQzvIP6uCSTZ"
      },
      "source": [
        "MedAssist-GPT: Complete Medical LLM Pretraining Script\n",
        "=====================================================\n",
        "Modern architecture with RoPE, GQA, SwiGLU, RMSNorm\n",
        "Optimized for A100 GPU with Flash Attention\n",
        "Automatic checkpointing and HuggingFace uploads\n",
        "\n",
        "PRETRAINED WEIGHTS AT: https://huggingface.co/kunjcr2/MedAssistGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSugAirc_7-E"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGI4c6FtCimw"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YPBumROsB2Qw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from bs4 import BeautifulSoup, NavigableString\n",
        "import tiktoken\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import math\n",
        "\n",
        "import pickle\n",
        "import gc\n",
        "from pathlib import Path\n",
        "import re\n",
        "import html\n",
        "from bs4 import BeautifulSoup, NavigableString\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import autocast\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import multiprocessing as mp\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from functools import partial\n",
        "\n",
        "import tiktoken\n",
        "import wandb\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from huggingface_hub import login, create_repo, upload_folder, HfApi\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97a6jSgCZ87"
      },
      "source": [
        "## CONFIGURATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "-ZMmrguuCNxw"
      },
      "outputs": [],
      "source": [
        "MODEL_CONFIG = {\n",
        "    \"vocab_size\": 50281,\n",
        "    \"d_model\": 512,\n",
        "    \"n_heads\": 16,\n",
        "    \"gqa_groups\": 4,\n",
        "    \"max_len\": 1024,\n",
        "    \"d_ff\": 2048,           # 4x hidden dimension\n",
        "    \"eps\": 1e-5,\n",
        "    \"dropout_p\": 0.1,       # No dropout during pretraining\n",
        "    \"blocks\": 16,           # ~500M parameters\n",
        "}\n",
        "\n",
        "TRAINING_CONFIG = {\n",
        "    \"batch_size\": 64,\n",
        "    \"max_length\": 1024,\n",
        "    \"stride\": 1024,\n",
        "    \"gradient_accumulation_steps\": 2,  # Effective batch size: 128\n",
        "    \"learning_rate\": 3e-4,\n",
        "    \"weight_decay\": 0.1,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.95,\n",
        "    \"eps\": 1e-8,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"max_steps\": 50000,\n",
        "    \"eval_freq\": 500,\n",
        "    \"eval_iter\": 100,\n",
        "    \"save_freq\": 1000,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"num_workers\": 4,\n",
        "    \"seed\": 1496,\n",
        "}\n",
        "\n",
        "DATA_CONFIG = {\n",
        "    \"dataset_name\": \"Hack90/europe_pmc_articles_part_2\",  # Your dataset\n",
        "    \"text_column\": \"full_text\",  # Column with text\n",
        "    \"max_length\": 1024,  # Sequence length\n",
        "    \"stride\": 1024,  # Window stride\n",
        "    \"train_split\": 0.95,  # 95% train, 5% val\n",
        "    \"max_train_samples\": 1_000_000,  # Total documents to use\n",
        "    \"chunk_size\": 10000,  # Process 5K docs at a time\n",
        "    \"use_clean\": True,  # Use your clean() function\n",
        "}\n",
        "\n",
        "WANDB_CONFIG = {\n",
        "    \"project\": \"MedAssist-GPT-Pretraining\",\n",
        "    \"entity\": \"kunjcr2-dreamable\",  # Your wandb username\n",
        "    \"name\": \"medassist-86M\",\n",
        "}\n",
        "\n",
        "HF_CONFIG = {\n",
        "    \"repo_id\": \"kunjcr2/MedAssist-GPT-125M\",  # Change this!\n",
        "    \"upload_checkpoints\": True,\n",
        "    \"upload_frequency\": 5000,  # Upload every N steps\n",
        "}\n",
        "\n",
        "INFERENCE_CONFIG = {\n",
        "    \"max_new_tokens\": 100,\n",
        "    \"temperature\": 0.5,\n",
        "    \"prompt_text\": \"To live a good life\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ducwfpL3ClaG"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6cXKVVC1CmjN"
      },
      "outputs": [],
      "source": [
        "class RoPE(nn.Module):\n",
        "    \"\"\"Rotary Position Embeddings (RoPE)\"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        assert d_model % 2 == 0, \"d_model must be even for RoPE\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Position indices - tensor (0,1,2,...,max_len) of size (max_len, 1)\n",
        "        self.register_buffer('position_ids', torch.arange(max_len).unsqueeze(1))\n",
        "\n",
        "        # Frequency terms\n",
        "        self.register_buffer(\n",
        "            'div_term',\n",
        "            torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "            # e^(2i*(-log(10000))/d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # Get positions\n",
        "        position_ids = self.position_ids[:seq_len] # (seq_len, 1)\n",
        "\n",
        "        # Calculate angles\n",
        "        angles = position_ids * self.div_term # (seq_len, d_model/2)\n",
        "        cos_vals = torch.cos(angles)\n",
        "        sin_vals = torch.sin(angles)\n",
        "\n",
        "        # Reshape for rotation\n",
        "        x_pairs = x.view(batch_size, seq_len, d_model // 2, 2) # (b, s, d//2, 2)\n",
        "        x_even = x_pairs[..., 0] # (b, s, d//2)\n",
        "        x_odd = x_pairs[..., 1] # (b, s, d//2)\n",
        "\n",
        "        # Apply rotation\n",
        "        rotated_even = x_even * cos_vals - x_odd * sin_vals\n",
        "        rotated_odd = x_even * sin_vals + x_odd * cos_vals\n",
        "\n",
        "        # Reconstruct\n",
        "        rotated_pairs = torch.stack([rotated_even, rotated_odd], dim=-1) # (b, s, d//2, 2)\n",
        "        rotated_x = rotated_pairs.view(batch_size, seq_len, d_model) # (b, s, d)\n",
        "\n",
        "        return rotated_x\n",
        "\n",
        "\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"Grouped Query Attention (GQA) with RoPE\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int = 512,\n",
        "        n_heads: int = 8,\n",
        "        gqa_groups: int = 2,\n",
        "        max_len: int = 1024,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        assert n_heads % gqa_groups == 0, \"n_heads must be divisible by gqa_groups\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.gqa_groups = gqa_groups\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.n_kv_heads = n_heads // gqa_groups\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Projections (bias-free)\n",
        "        self.q_proj = nn.Linear(d_model, n_heads * self.head_dim, bias=False)\n",
        "        self.k_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.o_proj = nn.Linear(n_heads * self.head_dim, d_model, bias=False)\n",
        "\n",
        "        # RoPE for Q and K\n",
        "        self.rope_q = RoPE(d_model=n_heads * self.head_dim, max_len=max_len)\n",
        "        self.rope_k = RoPE(d_model=self.n_kv_heads * self.head_dim, max_len=max_len)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Project Q, K, V\n",
        "        q = self.q_proj(x)  # (B, T, H*D)\n",
        "        k = self.k_proj(x)  # (B, T, H_kv*D)\n",
        "        v = self.v_proj(x)  # (B, T, H_kv*D)\n",
        "\n",
        "        # Apply RoPE\n",
        "        q = self.rope_q(q)\n",
        "        k = self.rope_k(k)\n",
        "\n",
        "        # Reshape to heads\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # (B, H, T, D)\n",
        "        k = k.view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)  # (B, H_kv, T, D)\n",
        "        v = v.view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)  # (B, H_kv, T, D)\n",
        "\n",
        "        # Expand K and V for GQA\n",
        "        expand_factor = self.n_heads // self.n_kv_heads\n",
        "        k = k.repeat_interleave(expand_factor, dim=1)  # (B, H, T, D)\n",
        "        v = v.repeat_interleave(expand_factor, dim=1)  # (B, H, T, D)\n",
        "\n",
        "        # Scaled dot-product attention with Flash Attention if available\n",
        "        out = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            attn_mask=None,\n",
        "            dropout_p=0.0,\n",
        "            is_causal=True\n",
        "        )\n",
        "\n",
        "        # Merge heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, self.n_heads * self.head_dim)\n",
        "\n",
        "        # Output projection\n",
        "        out = self.o_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SwiGLU_MLP(nn.Module):\n",
        "    \"\"\"SwiGLU Feed-Forward Network\"\"\"\n",
        "    def __init__(self, d_model: int = 512, d_ff: int = 2048):\n",
        "        super().__init__()\n",
        "        # Fused up + gate projection\n",
        "        self.w1 = nn.Linear(d_model, 2 * d_ff, bias=False)\n",
        "        # Down projection\n",
        "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        up, gate = self.w1(x).chunk(2, dim=-1) # breaks it into 2 parts - (b, s, d_ff)\n",
        "        x = up * F.silu(gate)  # SwiGLU activation - (b,s,d_ff) * (b,s,d_ff) = (b,s,d_ff)\n",
        "        x = self.w2(x)  # (b,s,d_model)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with pre-norm and residual connections\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "        self.rms1 = nn.RMSNorm(config[\"d_model\"], eps=config[\"eps\"])\n",
        "        self.rms2 = nn.RMSNorm(config[\"d_model\"], eps=config[\"eps\"])\n",
        "\n",
        "        self.attn = GroupedQueryAttention(\n",
        "            d_model=config[\"d_model\"],\n",
        "            n_heads=config[\"n_heads\"],\n",
        "            gqa_groups=config[\"gqa_groups\"],\n",
        "            max_len=config[\"max_len\"]\n",
        "        )\n",
        "\n",
        "        self.mlp = SwiGLU_MLP(\n",
        "            d_model=config[\"d_model\"],\n",
        "            d_ff=config[\"d_ff\"]\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(config[\"dropout_p\"])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Pre-norm attention\n",
        "        x = x + self.dropout(self.attn(self.rms1(x)))\n",
        "        # Pre-norm MLP\n",
        "        x = x + self.dropout(self.mlp(self.rms2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class MedAssistGPT(nn.Module):\n",
        "    \"\"\"Main model class\"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Token embeddings\n",
        "        self.embed = nn.Embedding(config[\"vocab_size\"], config[\"d_model\"])\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(config) for _ in range(config[\"blocks\"])\n",
        "        ])\n",
        "\n",
        "        # Final RMSNorm\n",
        "        self.final_rms = nn.RMSNorm(config[\"d_model\"], eps=config[\"eps\"])\n",
        "\n",
        "        # Language model head (weight-tied with embeddings)\n",
        "        self.lm_head = nn.Linear(config[\"d_model\"], config[\"vocab_size\"], bias=False)\n",
        "        self.lm_head.weight = self.embed.weight\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        # input_ids: (batch, seq_len)\n",
        "        h = self.embed(input_ids)  # (batch, seq_len, d_model)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.blocks:\n",
        "            h = block(h)\n",
        "\n",
        "        # Final normalization\n",
        "        h = self.final_rms(h)\n",
        "\n",
        "        # Language model head\n",
        "        logits = self.lm_head(h)  # (batch, seq_len, vocab_size)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def count_parameters(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_y_cRARCtMu"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PER5B_NY_i5d"
      },
      "source": [
        "### clean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N7YNaXjO_kbl"
      },
      "outputs": [],
      "source": [
        "def clean(xml_str: str) -> str:\n",
        "    \"\"\"\n",
        "    YOUR ORIGINAL XML CLEANING FUNCTION\n",
        "    Converts JATS/PMC XML to readable plain text\n",
        "    Removes tables, figures, citations, keeps narrative text\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(xml_str, \"lxml-xml\")\n",
        "\n",
        "    # 1) Remove whole non-narrative blocks\n",
        "    drop_whole = [\n",
        "        \"ref-list\", \"fig\", \"fig-group\", \"table-wrap\", \"table\", \"thead\", \"tbody\",\n",
        "        \"tr\", \"td\", \"th\", \"graphic\", \"media\", \"supplementary-material\", \"back\",\n",
        "        \"sec-meta\", \"table-wrap-foot\", \"caption\"\n",
        "    ]\n",
        "    for name in drop_whole:\n",
        "        for tag in soup.find_all(name):\n",
        "            tag.decompose()\n",
        "\n",
        "    # 2) Remove cross-references entirely (citations, table/fig pointers)\n",
        "    for tag in soup.find_all(\"xref\"):\n",
        "        tag.decompose()\n",
        "\n",
        "    # 3) Preserve disp-quote as plain paragraphs\n",
        "    for dq in soup.find_all(\"disp-quote\"):\n",
        "        txt = dq.get_text(\" \", strip=True)\n",
        "        dq.replace_with(NavigableString((\"\\n\" + txt + \"\\n\") if txt else \"\"))\n",
        "\n",
        "    # 4) Turn <title> into clean section headers\n",
        "    for t in soup.find_all(\"title\"):\n",
        "        title_txt = t.get_text(\" \", strip=True)\n",
        "        t.replace_with(NavigableString(\"\\n\\n\" + title_txt + \"\\n\"))\n",
        "\n",
        "    # 5) Ensure paragraphs end cleanly; unwrap inline tags\n",
        "    inline_unwrap = [\n",
        "        \"italic\", \"bold\", \"underline\", \"sc\", \"em\", \"strong\", \"sup\", \"sub\",\n",
        "        \"styled-content\", \"inline-formula\", \"monospace\"\n",
        "    ]\n",
        "    for p in soup.find_all(\"p\"):\n",
        "        for name in inline_unwrap:\n",
        "            for it in p.find_all(name):\n",
        "                it.unwrap()\n",
        "        p.insert_after(NavigableString(\"\\n\\n\"))\n",
        "        p.unwrap()\n",
        "\n",
        "    # 6) Unwrap remaining structural containers\n",
        "    for name in [\"sec\", \"body\", \"front\", \"article\", \"abstract\", \"boxed-text\", \"list\", \"list-item\"]:\n",
        "        for tag in soup.find_all(name):\n",
        "            tag.unwrap()\n",
        "\n",
        "    # 7) Extract text and clean up\n",
        "    text = soup.get_text()\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # Post-processing cleanup\n",
        "    text = re.sub(r\"\\(\\s*(?:\\d+\\s*(?:[-‚Äì]\\s*\\d+)?\\s*(?:[,;]\\s*)?)+\\)\", \"\", text)\n",
        "    text = re.sub(r\"\\(\\s*(?:[Ff]ig(?:ure)?\\.?\\s*\\d+|[Tt]able\\s*\\d+)\\s*\\)\", \"\", text)\n",
        "    text = re.sub(r\"\\(\\s*\\)\", \"\", text)\n",
        "    text = re.sub(r\"\\[\\s*\\]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", text)\n",
        "    text = re.sub(r\"([,.;:!?])\\s*\\1+\", r\"\\1 \", text)\n",
        "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "\n",
        "    text = \"\\n\".join(line.rstrip() for line in text.splitlines())\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir1N-wXM_qBo"
      },
      "source": [
        "### MemortMappedDataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vxKxg6Dj_vtS"
      },
      "outputs": [],
      "source": [
        "class MemoryMappedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that uses memory-mapped files for ZERO RAM overhead!\n",
        "\n",
        "    HOW IT WORKS:\n",
        "    1. Data is stored on disk in binary format (.npy files)\n",
        "    2. When you access data[i], OS loads ONLY that piece into RAM\n",
        "    3. OS automatically evicts old data when RAM gets full\n",
        "    4. You get 100GB+ dataset working with 5GB RAM!\n",
        "    \"\"\"\n",
        "    def __init__(self, cache_dir: Path):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "\n",
        "        # Load metadata (tiny file, ~1KB)\n",
        "        with open(self.cache_dir / \"metadata.pkl\", \"rb\") as f:\n",
        "            self.metadata = pickle.load(f)\n",
        "\n",
        "        # Memory-map the input/target arrays\n",
        "        # mode='r' = read-only, mmap_mode='r' = memory-mapped read\n",
        "        # CRITICAL: np.load with mmap_mode DOES NOT load data into RAM!\n",
        "        # It just maps the file, OS loads pages on-demand\n",
        "        self.inputs_mmap = np.load(\n",
        "            self.cache_dir / \"inputs.npy\",\n",
        "            mmap_mode='r'  # ‚Üê THIS IS THE MAGIC! OS manages memory\n",
        "        )\n",
        "        self.targets_mmap = np.load(\n",
        "            self.cache_dir / \"targets.npy\",\n",
        "            mmap_mode='r'\n",
        "        )\n",
        "\n",
        "        print(f\"üìÇ Memory-mapped dataset: {len(self.inputs_mmap):,} samples\")\n",
        "        print(f\"üíæ RAM overhead: ~0 MB (OS manages it)\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.inputs_mmap)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # OS loads ONLY this row into RAM (4KB per sample)\n",
        "        # When RAM is full, OS evicts least-recently-used data\n",
        "        input_ids = torch.from_numpy(self.inputs_mmap[idx].copy()).long()\n",
        "        target_ids = torch.from_numpy(self.targets_mmap[idx].copy()).long()\n",
        "\n",
        "        return input_ids, target_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4a_G8lD_xIy"
      },
      "source": [
        "### process_single_chunk()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "o0dvOYVp_29Z"
      },
      "outputs": [],
      "source": [
        "def process_single_chunk(\n",
        "    doc_batch: List[Dict],\n",
        "    tokenizer_name: str,  # Can't pickle tokenizer, so pass name\n",
        "    max_length: int,\n",
        "    stride: int,\n",
        "    use_clean: bool,\n",
        "    text_column: str,\n",
        "    chunk_id: int,\n",
        ") -> Tuple[List[np.ndarray], List[np.ndarray], int, int]:\n",
        "    \"\"\"\n",
        "    Worker function that processes one chunk\n",
        "\n",
        "    WHY SEPARATE FUNCTION:\n",
        "    - ProcessPoolExecutor spawns new Python processes\n",
        "    - Each process needs its own tokenizer instance\n",
        "    - Can't share tokenizer objects across processes (pickling issues)\n",
        "\n",
        "    WHAT IT DOES:\n",
        "    1. Creates its own tokenizer\n",
        "    2. Processes its batch of documents\n",
        "    3. Returns results to main process\n",
        "    \"\"\"\n",
        "    import tiktoken\n",
        "\n",
        "    # Each worker creates its own tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(tokenizer_name)\n",
        "    vocab_size = tokenizer.n_vocab\n",
        "\n",
        "    chunk_tokens = []\n",
        "    docs_processed = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    # Process each document in this chunk\n",
        "    for doc in doc_batch:\n",
        "        # Extract text\n",
        "        if text_column in doc:\n",
        "            text = doc[text_column]\n",
        "        elif 'text' in doc:\n",
        "            text = doc['text']\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if not text or len(text) < 100:\n",
        "            continue\n",
        "\n",
        "        # Apply clean function if enabled\n",
        "        if use_clean:\n",
        "            try:\n",
        "                text = clean(text)  # Your clean function\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if not text or len(text) < 100:\n",
        "            continue\n",
        "\n",
        "        # Tokenize\n",
        "        try:\n",
        "            tokens = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "            tokens = [t for t in tokens if 0 <= t < vocab_size]\n",
        "\n",
        "            if len(tokens) < 10:\n",
        "                continue\n",
        "\n",
        "            chunk_tokens.extend(tokens)\n",
        "            total_tokens += len(tokens)\n",
        "            docs_processed += 1\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Create sliding windows from all tokens in this chunk\n",
        "    samples = create_sliding_windows(chunk_tokens, max_length, stride, vocab_size)\n",
        "\n",
        "    if samples:\n",
        "        inputs, targets = zip(*samples)\n",
        "        return list(inputs), list(targets), docs_processed, total_tokens\n",
        "    else:\n",
        "        return [], [], docs_processed, total_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1frbZzZJ_4LQ"
      },
      "source": [
        "### process_dataset_in_chunks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "M7_WGmi9__th"
      },
      "outputs": [],
      "source": [
        "def process_dataset_in_chunks(\n",
        "    dataset_name: str,\n",
        "    tokenizer,\n",
        "    cache_dir: Path,\n",
        "    max_length: int = 1024,\n",
        "    stride: int = 1024,\n",
        "    max_samples: int = None,\n",
        "    chunk_size: int = 5000,\n",
        "    use_clean: bool = True,\n",
        "    text_column: str = \"full_text\"\n",
        "):\n",
        "    \"\"\"\n",
        "    PARALLEL VERSION - Drop-in replacement for your original function\n",
        "\n",
        "    HOW PARALLELIZATION WORKS:\n",
        "    1. Load documents in batches (streaming)\n",
        "    2. Distribute batches to worker processes\n",
        "    3. Each worker: clean ‚Üí tokenize ‚Üí create windows\n",
        "    4. Main process: collect results and save\n",
        "\n",
        "    NUM WORKERS:\n",
        "    - Uses all CPU cores by default\n",
        "    - On 8-core: 8 chunks processed simultaneously\n",
        "    - On Colab: ~2 cores (still 2x speedup!)\n",
        "    \"\"\"\n",
        "\n",
        "    cache_dir = Path(cache_dir)\n",
        "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Determine number of workers\n",
        "    num_workers = min(mp.cpu_count(), 8)  # Max 8 workers (diminishing returns)\n",
        "\n",
        "    print(f\"üî• Processing dataset: {dataset_name}\")\n",
        "    print(f\"üìä Chunk size: {chunk_size} documents\")\n",
        "    print(f\"üßπ Cleaning enabled: {use_clean}\")\n",
        "    print(f\"‚ö° Parallel workers: {num_workers}\")\n",
        "\n",
        "    # Load dataset in streaming mode\n",
        "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
        "\n",
        "    if max_samples:\n",
        "        dataset = dataset.take(max_samples)\n",
        "\n",
        "    # Accumulate results from all workers\n",
        "    all_inputs = []\n",
        "    all_targets = []\n",
        "    total_docs_processed = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    print(\"üîÑ Loading documents into batches...\")\n",
        "\n",
        "    # Collect documents into batches\n",
        "    # WHY: Can't parallelize streaming iterator directly\n",
        "    # So we batch first, then parallelize batch processing\n",
        "    doc_batches = []\n",
        "    current_batch = []\n",
        "\n",
        "    for doc in tqdm(dataset, desc=\"Batching docs\"):\n",
        "        current_batch.append(doc)\n",
        "\n",
        "        if len(current_batch) >= chunk_size:\n",
        "            doc_batches.append(current_batch)\n",
        "            current_batch = []\n",
        "\n",
        "    # Don't forget the last batch\n",
        "    if current_batch:\n",
        "        doc_batches.append(current_batch)\n",
        "\n",
        "    print(f\"üì¶ Created {len(doc_batches)} batches of ~{chunk_size} documents each\")\n",
        "    print(f\"‚ö° Processing with {num_workers} parallel workers...\")\n",
        "\n",
        "    # PARALLEL PROCESSING STARTS HERE!\n",
        "    # ProcessPoolExecutor creates worker processes\n",
        "    # Each worker gets its own batch to process\n",
        "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "\n",
        "        # Submit all batches to workers\n",
        "        # partial() pre-fills the arguments that are same for all workers\n",
        "        worker_fn = partial(\n",
        "            process_single_chunk,\n",
        "            tokenizer_name=\"p50k_base\",  # Pass tokenizer name, not object\n",
        "            max_length=max_length,\n",
        "            stride=stride,\n",
        "            use_clean=use_clean,\n",
        "            text_column=text_column,\n",
        "        )\n",
        "\n",
        "        # Submit all jobs\n",
        "        futures = []\n",
        "        for chunk_id, doc_batch in enumerate(doc_batches):\n",
        "            future = executor.submit(worker_fn, doc_batch, chunk_id=chunk_id)\n",
        "            futures.append(future)\n",
        "\n",
        "        # Collect results as they complete\n",
        "        # as_completed() returns futures as soon as they finish (not in order)\n",
        "        # This gives us progress updates in real-time!\n",
        "        try:\n",
        "          for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing chunks\"):\n",
        "            try:\n",
        "                # Get results from this worker\n",
        "                inputs, targets, docs_proc, tokens_proc = future.result()\n",
        "\n",
        "                # Accumulate\n",
        "                all_inputs.extend(inputs)\n",
        "                all_targets.extend(targets)\n",
        "                total_docs_processed += docs_proc\n",
        "                total_tokens += tokens_proc\n",
        "\n",
        "                # Show progress\n",
        "                if len(inputs) > 0:\n",
        "                    print(f\"‚úÖ Chunk done: {len(inputs)} samples, {docs_proc} docs, {tokens_proc:,} tokens\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Worker failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        except KeyboardInterrupt as e:\n",
        "            print(\"‚ö†Ô∏è  Interrupted by user\")\n",
        "\n",
        "    print(f\"\\nüìä Parallel processing complete!\")\n",
        "    print(f\"   Documents processed: {total_docs_processed:,}\")\n",
        "    print(f\"   Total tokens: {total_tokens:,}\")\n",
        "    print(f\"   Training samples: {len(all_inputs):,}\")\n",
        "\n",
        "    # Save to disk (same as before)\n",
        "    print(\"üíæ Saving to disk as memory-mapped arrays...\")\n",
        "\n",
        "    inputs_array = np.array(all_inputs, dtype=np.int32)\n",
        "    targets_array = np.array(all_targets, dtype=np.int32)\n",
        "\n",
        "    np.save(cache_dir / \"inputs.npy\", inputs_array)\n",
        "    np.save(cache_dir / \"targets.npy\", targets_array)\n",
        "\n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        \"num_samples\": len(all_inputs),\n",
        "        \"max_length\": max_length,\n",
        "        \"stride\": stride,\n",
        "        \"vocab_size\": tokenizer.n_vocab,\n",
        "        \"docs_processed\": total_docs_processed,\n",
        "        \"total_tokens\": total_tokens,\n",
        "    }\n",
        "\n",
        "    with open(cache_dir / \"metadata.pkl\", \"wb\") as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    print(f\"‚úÖ Saved to {cache_dir}\")\n",
        "\n",
        "    # Clear RAM\n",
        "    del all_inputs, all_targets, inputs_array, targets_array\n",
        "    gc.collect()\n",
        "\n",
        "    return cache_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD3wJU-rABeN"
      },
      "source": [
        "### create_sliding_windows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PSeR1suNAGH9"
      },
      "outputs": [],
      "source": [
        "def create_sliding_windows(\n",
        "    tokens: List[int],\n",
        "    max_length: int,\n",
        "    stride: int,\n",
        "    vocab_size: int\n",
        ") -> List[Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Create sliding window samples from token list\n",
        "\n",
        "    EXAMPLE:\n",
        "    tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "    max_length = 4, stride = 2\n",
        "\n",
        "    Samples:\n",
        "    input:  [1, 2, 3, 4]  target: [2, 3, 4, 5]\n",
        "    input:  [3, 4, 5, 6]  target: [4, 5, 6, 7]\n",
        "    input:  [5, 6, 7, 8]  target: [6, 7, 8, 9]\n",
        "    etc.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    tokens = np.array(tokens, dtype=np.int32)\n",
        "\n",
        "    # Verify tokens are in valid range\n",
        "    if len(tokens) == 0:\n",
        "        return samples\n",
        "\n",
        "    # Clip to valid range (safety)\n",
        "    tokens = np.clip(tokens, 0, vocab_size - 1)\n",
        "\n",
        "    # Create sliding windows\n",
        "    for i in range(0, len(tokens) - max_length, stride):\n",
        "        input_ids = tokens[i:i+max_length]\n",
        "        target_ids = tokens[i+1:i+max_length+1]\n",
        "\n",
        "        # Ensure both are exactly max_length\n",
        "        if len(input_ids) == max_length and len(target_ids) == max_length:\n",
        "            samples.append((input_ids, target_ids))\n",
        "\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tUUoVFCAHNy"
      },
      "source": [
        "### prepare_mdeical_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "q-ainOtvAMlX"
      },
      "outputs": [],
      "source": [
        "def prepare_medical_data(\n",
        "    config: Dict[str, Any],\n",
        "    tokenizer\n",
        ") -> Tuple[Path, Path]:\n",
        "    \"\"\"\n",
        "    Main function: Prepares training and validation data efficiently\n",
        "\n",
        "    WHAT THIS DOES:\n",
        "    1. Checks if cache exists (skip if already processed)\n",
        "    2. Splits dataset into train/val\n",
        "    3. Processes each split in chunks\n",
        "    4. Saves as memory-mapped files\n",
        "    5. Returns paths to cached data\n",
        "\n",
        "    FIRST RUN: ~30 minutes (processes and caches)\n",
        "    SUBSEQUENT RUNS: ~1 second (loads from cache)\n",
        "    \"\"\"\n",
        "\n",
        "    train_cache = Path(\"./data_cache/train\")\n",
        "    val_cache = Path(\"./data_cache/val\")\n",
        "\n",
        "    # Check if already cached\n",
        "    train_exists = (train_cache / \"metadata.pkl\").exists()\n",
        "    val_exists = (val_cache / \"metadata.pkl\").exists()\n",
        "\n",
        "    if train_exists and val_exists:\n",
        "        print(\"‚úÖ Found cached data! Skipping processing.\")\n",
        "        print(f\"   Train cache: {train_cache}\")\n",
        "        print(f\"   Val cache: {val_cache}\")\n",
        "        return train_cache, val_cache\n",
        "\n",
        "    # Calculate split sizes\n",
        "    total_samples = config.get(\"max_train_samples\", 1_000_000)\n",
        "    train_size = int(total_samples * config.get(\"train_split\", 0.95))\n",
        "    val_size = total_samples - train_size\n",
        "\n",
        "    print(f\"üìä Dataset split:\")\n",
        "    print(f\"   Training: {train_size:,} documents\")\n",
        "    print(f\"   Validation: {val_size:,} documents\")\n",
        "\n",
        "    # Process training data\n",
        "    if not train_exists:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"PROCESSING TRAINING DATA\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Create temporary streaming dataset for train split\n",
        "        dataset = load_dataset(\n",
        "            config[\"dataset_name\"],\n",
        "            split=\"train\",\n",
        "            streaming=True\n",
        "        )\n",
        "        train_dataset = dataset.take(train_size)\n",
        "\n",
        "        # Process and cache\n",
        "        # We create a temporary generator to process\n",
        "        def train_generator():\n",
        "            for item in train_dataset:\n",
        "                yield item\n",
        "\n",
        "        # Process using your dataset directly\n",
        "        process_dataset_in_chunks(\n",
        "            dataset_name=config[\"dataset_name\"],\n",
        "            tokenizer=tokenizer,\n",
        "            cache_dir=train_cache,\n",
        "            max_length=config.get(\"max_length\", 1024),\n",
        "            stride=config.get(\"stride\", 1024),\n",
        "            max_samples=train_size,\n",
        "            chunk_size=config.get(\"chunk_size\", 5000),\n",
        "            use_clean=config.get(\"use_clean\", True),\n",
        "            text_column=config.get(\"text_column\", \"full_text\")\n",
        "        )\n",
        "\n",
        "    # Process validation data\n",
        "    if not val_exists:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"PROCESSING VALIDATION DATA\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Create dataset that skips training samples\n",
        "        dataset = load_dataset(\n",
        "            config[\"dataset_name\"],\n",
        "            split=\"train\",\n",
        "            streaming=True\n",
        "        )\n",
        "        val_dataset = dataset.skip(train_size).take(val_size)\n",
        "\n",
        "        # Process and cache\n",
        "        # For validation, we need to handle the skip\n",
        "        # Easier to just process with offset\n",
        "        temp_config = config.copy()\n",
        "        temp_config[\"dataset_offset\"] = train_size\n",
        "\n",
        "        process_dataset_in_chunks(\n",
        "            dataset_name=config[\"dataset_name\"],\n",
        "            tokenizer=tokenizer,\n",
        "            cache_dir=val_cache,\n",
        "            max_length=config.get(\"max_length\", 1024),\n",
        "            stride=config.get(\"stride\", 1024),\n",
        "            max_samples=val_size,\n",
        "            chunk_size=config.get(\"chunk_size\", 5000),\n",
        "            use_clean=config.get(\"use_clean\", True),\n",
        "            text_column=config.get(\"text_column\", \"full_text\")\n",
        "        )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚úÖ DATA PREPARATION COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return train_cache, val_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjLV-c66ANvw"
      },
      "source": [
        "### create_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "df1BRpySCu_L"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(\n",
        "    cache_dir: Path,\n",
        "    batch_size: int,\n",
        "    shuffle: bool = True,\n",
        "    num_workers: int = 2,  # Lower for memory-mapped files\n",
        ") -> DataLoader:\n",
        "    \"\"\"\n",
        "    Create DataLoader from cached data\n",
        "\n",
        "    NUM_WORKERS = 2:\n",
        "    - Memory-mapped files don't benefit from many workers\n",
        "    - OS handles the parallel I/O better than Python\n",
        "    - 2 workers is sweet spot for prefetching\n",
        "    \"\"\"\n",
        "\n",
        "    dataset = MemoryMappedDataset(cache_dir)\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,  # Faster GPU transfer\n",
        "        drop_last=True,\n",
        "        persistent_workers=True,  # Keep workers alive between epochs\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivOe383gCxwC"
      },
      "source": [
        "## Training Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-iqYukinC0Vu"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(\n",
        "    input_batch: torch.Tensor,\n",
        "    target_batch: torch.Tensor,\n",
        "    model: nn.Module,\n",
        "    device: torch.device\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Calculate loss for a single batch\"\"\"\n",
        "    input_batch = input_batch.to(device, non_blocking=True)\n",
        "    target_batch = target_batch.to(device, non_blocking=True)\n",
        "\n",
        "    with autocast(\"cuda\", torch.bfloat16):\n",
        "        logits = model(input_batch)\n",
        "        loss = F.cross_entropy(\n",
        "            logits.flatten(0, 1),\n",
        "            target_batch.flatten()\n",
        "        )\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    device: torch.device,\n",
        "    num_batches: int = 100\n",
        ") -> float:\n",
        "    \"\"\"Evaluate model on validation set\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    model.train()\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "def save_checkpoint(\n",
        "    model: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: Any,\n",
        "    step: int,\n",
        "    loss: float,\n",
        "    save_dir: Path,\n",
        "    config: Dict[str, Any]\n",
        "):\n",
        "    \"\"\"Save model checkpoint\"\"\"\n",
        "    checkpoint = {\n",
        "        'step': step,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': loss,\n",
        "        'config': config,\n",
        "    }\n",
        "\n",
        "    save_path = save_dir / f\"checkpoint_step_{step}.pt\"\n",
        "    torch.save(checkpoint, save_path)\n",
        "    print(f\"üíæ Checkpoint saved: {save_path}\")\n",
        "\n",
        "    return save_path\n",
        "\n",
        "\n",
        "def upload_to_huggingface(\n",
        "    model: nn.Module,\n",
        "    save_dir: Path,\n",
        "    repo_id: str,\n",
        "    config: Dict[str, Any],\n",
        "    step: int\n",
        "):\n",
        "    \"\"\"Upload model to HuggingFace Hub\"\"\"\n",
        "    try:\n",
        "        # Save model weights\n",
        "        torch.save(model.state_dict(), save_dir / \"pytorch_model.bin\")\n",
        "\n",
        "        # Save config\n",
        "        with open(save_dir / \"config.json\", \"w\") as f:\n",
        "            json.dump(config, f, indent=2)\n",
        "\n",
        "        # Upload to HF\n",
        "        api = HfApi()\n",
        "        api.upload_folder(\n",
        "            folder_path=str(save_dir),\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\",\n",
        "            commit_message=f\"Training checkpoint at step {step}\"\n",
        "        )\n",
        "\n",
        "        print(f\"‚òÅÔ∏è  Uploaded to HuggingFace: {repo_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Failed to upload to HuggingFace: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TOcqHG5C3qx"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Yet8kHbjC5kj"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: Any,\n",
        "    device: torch.device,\n",
        "    config: Dict[str, Any],\n",
        "    save_dir: Path,\n",
        "    hf_repo_id: str = None\n",
        "):\n",
        "    \"\"\"Main training loop with all optimizations\"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üöÄ STARTING MEDICAL LLM PRETRAINING\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"üìä Model: {model.count_parameters():,} parameters\")\n",
        "    print(f\"üìä Training batches: {len(train_loader):,}\")\n",
        "    print(f\"üìä Max steps: {config['max_steps']:,}\")\n",
        "    print(f\"üìä Effective batch size: {config['batch_size'] * config['gradient_accumulation_steps']}\")\n",
        "    print(f\"üìä Device: {device}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    model.train()\n",
        "    global_step = 0\n",
        "    tokens_seen = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    grad_accum = config[\"gradient_accumulation_steps\"]\n",
        "\n",
        "    try:\n",
        "        for epoch in range(100):  # Virtually unlimited epochs\n",
        "            epoch_loss = 0.0\n",
        "            epoch_steps = 0\n",
        "\n",
        "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "            for batch_idx, (input_batch, target_batch) in enumerate(progress_bar):\n",
        "                # Forward pass\n",
        "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "                # Scale loss for gradient accumulation\n",
        "                loss = loss / grad_accum\n",
        "                loss.backward()\n",
        "\n",
        "                # Accumulate\n",
        "                if (batch_idx + 1) % grad_accum == 0 or (batch_idx + 1) == len(train_loader):\n",
        "                    # Clip gradients\n",
        "                    torch.nn.utils.clip_grad_norm_(\n",
        "                        model.parameters(),\n",
        "                        max_norm=config[\"grad_clip\"]\n",
        "                    )\n",
        "\n",
        "                    # Optimizer step\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Update counters\n",
        "                    global_step += 1\n",
        "                    tokens_seen += input_batch.numel() * grad_accum\n",
        "\n",
        "                    # Log training loss\n",
        "                    train_losses.append(loss.item() * grad_accum)\n",
        "                    epoch_loss += loss.item() * grad_accum\n",
        "                    epoch_steps += 1\n",
        "\n",
        "                    # Update progress bar\n",
        "                    progress_bar.set_postfix({\n",
        "                        'loss': f\"{loss.item() * grad_accum:.4f}\",\n",
        "                        'lr': f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
        "                        'step': global_step\n",
        "                    })\n",
        "\n",
        "                    # Evaluation\n",
        "                    if global_step % config[\"eval_freq\"] == 0:\n",
        "                        val_loss = evaluate_model(\n",
        "                            model, val_loader, device, config[\"eval_iter\"]\n",
        "                        )\n",
        "                        val_losses.append(val_loss)\n",
        "\n",
        "                        # Log to wandb\n",
        "                        wandb.log({\n",
        "                            \"val_loss\": val_loss,\n",
        "                            \"learning_rate\": scheduler.get_last_lr()[0],\n",
        "                            \"step\": global_step,\n",
        "                        })\n",
        "\n",
        "                        # Check for improvement\n",
        "                        if val_loss < best_val_loss:\n",
        "                            best_val_loss = val_loss\n",
        "                            print(f\"\\n‚ú® New best validation loss: {val_loss:.4f}\")\n",
        "\n",
        "                    # Save checkpoint\n",
        "                    if global_step % config[\"save_freq\"] == 0:\n",
        "                        save_checkpoint(\n",
        "                            model, optimizer, scheduler,\n",
        "                            global_step, train_losses[-1],\n",
        "                            save_dir, MODEL_CONFIG\n",
        "                        )\n",
        "\n",
        "                        # Upload to HuggingFace\n",
        "                        if hf_repo_id and config.get(\"upload_checkpoints\", False):\n",
        "                            if global_step % config.get(\"upload_frequency\", 1000) == 0:\n",
        "                                upload_to_huggingface(\n",
        "                                    model, save_dir / \"hf_upload\",\n",
        "                                    hf_repo_id, MODEL_CONFIG, global_step\n",
        "                                )\n",
        "\n",
        "                    # Check if max steps reached\n",
        "                    if global_step >= config[\"max_steps\"]:\n",
        "                        print(f\"\\nüéâ Reached max steps ({config['max_steps']})\")\n",
        "                        raise StopIteration\n",
        "\n",
        "                    wandb.log({\n",
        "                        \"train_loss\": loss.item() * grad_accum,\n",
        "                        \"tokens_seen\": tokens_seen\n",
        "                    })\n",
        "\n",
        "            # End of epoch summary\n",
        "            avg_epoch_loss = epoch_loss / epoch_steps if epoch_steps > 0 else float('inf')\n",
        "            print(f\"\\nEpoch {epoch+1} complete - Avg loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    except (KeyboardInterrupt, StopIteration):\n",
        "        print(\"\\n‚ö†Ô∏è  Training stopped\")\n",
        "\n",
        "    # Final checkpoint\n",
        "    print(\"\\nüíæ Saving final checkpoint...\")\n",
        "    save_checkpoint(\n",
        "        model, optimizer, scheduler,\n",
        "        global_step, train_losses[-1] if train_losses else 0,\n",
        "        save_dir, MODEL_CONFIG\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüéâ Training complete!\")\n",
        "    print(f\"üìä Total steps: {global_step:,}\")\n",
        "    print(f\"üìä Total tokens: {tokens_seen:,}\")\n",
        "    print(f\"üìä Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXE9nD2bC86O"
      },
      "source": [
        "## main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "54afbe7a2ce942d390e0e88745d9ece4",
            "8658b4ff5f31491f9ff3e2cd61807c64",
            "4e6ebf5ac4f946d49dac8c0a667faf88",
            "9870f24038a44ca9adb8039e4e4c765d",
            "af0a437faeb449e38d844ff1b8065ee1",
            "9c873dc69e324b3b859f6cb6b2a14ed3",
            "13d81780da124f3e95b4e2d47c55e2c8",
            "05611c4dff5a4d96945997bd83a54725",
            "53381d7a401d4474a946319c168feb24",
            "25aaf39ee7604a8b99481ff9625a72f3",
            "3c263c0123644a6ebf1e498af83fe653",
            "ec4c5f7317944b298935cfb3b73e797f",
            "7bb126813ff04f2d85c04471f31b7671",
            "eb06ec08d68a4c96a741d6c709e0921b",
            "35e0b887b3034ab09d5d61845c18adce",
            "28ca6df30078450cb0c952651cc1e7b4",
            "ab07bff393e847758b55c2912bc70fbf"
          ]
        },
        "collapsed": true,
        "id": "_xVR1-A5C-Qp",
        "outputId": "98bd6b96-c83e-441e-9585-035ed9bc06c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Using device: cuda\n",
            "üîß Loading tokenizer...\n",
            "‚úÖ Found cached data! Skipping processing.\n",
            "   Train cache: data_cache/train\n",
            "   Val cache: data_cache/val\n",
            "üîß Creating dataloaders...\n",
            "üìÇ Memory-mapped dataset: 524,046 samples\n",
            "üíæ RAM overhead: ~0 MB (OS manages it)\n",
            "üìÇ Memory-mapped dataset: 33,328 samples\n",
            "üíæ RAM overhead: ~0 MB (OS manages it)\n",
            "üîß Initializing model...\n",
            "üîß Compiling model...\n",
            "‚úÖ Model has 86,578,176 parameters\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251110_023212-380oe4z1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kunjcr2-dreamable/MedAssist-GPT-Pretraining/runs/380oe4z1' target=\"_blank\">medassist-86M</a></strong> to <a href='https://wandb.ai/kunjcr2-dreamable/MedAssist-GPT-Pretraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kunjcr2-dreamable/MedAssist-GPT-Pretraining' target=\"_blank\">https://wandb.ai/kunjcr2-dreamable/MedAssist-GPT-Pretraining</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kunjcr2-dreamable/MedAssist-GPT-Pretraining/runs/380oe4z1' target=\"_blank\">https://wandb.ai/kunjcr2-dreamable/MedAssist-GPT-Pretraining/runs/380oe4z1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54afbe7a2ce942d390e0e88745d9ece4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  HuggingFace setup failed: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-69114ead-705d2d345910c14b0c0335da;12dc0f30-7093-442c-a008-7dc113ca8e79)\n",
            "\n",
            "Invalid username or password.\n",
            "================================================================================\n",
            "üöÄ STARTING MEDICAL LLM PRETRAINING\n",
            "================================================================================\n",
            "üìä Model: 86,578,176 parameters\n",
            "üìä Training batches: 8,188\n",
            "üìä Max steps: 50,000\n",
            "üìä Effective batch size: 128\n",
            "üìä Device: cuda\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  12%|‚ñà‚ñè        | 1000/8188 [08:29<13:07:57,  6.58s/it, loss=4.8076, lr=3.00e-04, step=500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 4.9242\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  24%|‚ñà‚ñà‚ñç       | 1998/8188 [14:46<38:41,  2.67it/s, loss=3.9421, lr=3.00e-04, step=1000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 3.9290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1:  24%|‚ñà‚ñà‚ñç       | 2000/8188 [15:01<4:33:07,  2.65s/it, loss=3.9421, lr=3.00e-04, step=1000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_1000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 3000/8188 [21:30<3:31:43,  2.45s/it, loss=3.6426, lr=3.00e-04, step=1500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 3.5518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3998/8188 [27:45<26:12,  2.67it/s, loss=3.2987, lr=2.99e-04, step=2000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 3.3488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 4000/8188 [28:00<3:04:48,  2.65s/it, loss=3.2987, lr=2.99e-04, step=2000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_2000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 5000/8188 [34:29<2:10:18,  2.45s/it, loss=3.3058, lr=2.99e-04, step=2500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 3.2273\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 5998/8188 [40:44<13:41,  2.66it/s, loss=3.1478, lr=2.98e-04, step=3000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 3.1449\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 6000/8188 [41:00<1:36:34,  2.65s/it, loss=3.1478, lr=2.98e-04, step=3000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_3000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 7000/8188 [47:29<48:30,  2.45s/it, loss=3.1070, lr=2.97e-04, step=3500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 3.0813\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 7998/8188 [53:45<01:11,  2.66it/s, loss=3.0844, lr=2.96e-04, step=4000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 3.0293\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 8000/8188 [54:00<08:18,  2.65s/it, loss=3.0844, lr=2.96e-04, step=4000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_4000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8188/8188 [55:11<00:00,  2.47it/s, loss=2.8545, lr=2.96e-04, step=4094]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 complete - Avg loss: 3.8584\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2:  10%|‚ñâ         | 812/8188 [05:18<5:01:20,  2.45s/it, loss=3.0777, lr=2.95e-04, step=4500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.9929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2:  22%|‚ñà‚ñà‚ñè       | 1810/8188 [11:34<39:56,  2.66it/s, loss=3.1004, lr=2.94e-04, step=5000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.9633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 2:  22%|‚ñà‚ñà‚ñè       | 1812/8188 [11:49<4:41:08,  2.65s/it, loss=3.1004, lr=2.94e-04, step=5000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_5000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñç      | 2812/8188 [18:19<3:39:27,  2.45s/it, loss=2.9451, lr=2.93e-04, step=5500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.9359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 3810/8188 [24:34<27:21,  2.67it/s, loss=3.0218, lr=2.91e-04, step=6000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.9105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 3812/8188 [24:49<3:13:16,  2.65s/it, loss=3.0218, lr=2.91e-04, step=6000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_6000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 4812/8188 [31:18<2:17:52,  2.45s/it, loss=2.9147, lr=2.89e-04, step=6500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.8890\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 5810/8188 [37:34<15:02,  2.63it/s, loss=2.9481, lr=2.87e-04, step=7000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.8693\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 5812/8188 [37:49<1:47:44,  2.72s/it, loss=2.9481, lr=2.87e-04, step=7000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_7000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 6812/8188 [44:18<56:15,  2.45s/it, loss=2.9766, lr=2.85e-04, step=7500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.8529\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 7810/8188 [50:34<02:21,  2.66it/s, loss=2.9394, lr=2.83e-04, step=8000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.8375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 7812/8188 [50:49<16:35,  2.65s/it, loss=2.9394, lr=2.83e-04, step=8000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_8000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8188/8188 [53:10<00:00,  2.57it/s, loss=2.8984, lr=2.83e-04, step=8188]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 complete - Avg loss: 2.9498\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3:   8%|‚ñä         | 624/8188 [04:08<5:09:16,  2.45s/it, loss=2.8120, lr=2.81e-04, step=8500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.8221\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3:  20%|‚ñà‚ñâ        | 1622/8188 [10:23<41:04,  2.66it/s, loss=2.7628, lr=2.79e-04, step=9000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.8109\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 3:  20%|‚ñà‚ñâ        | 1624/8188 [10:38<4:49:15,  2.64s/it, loss=2.7628, lr=2.79e-04, step=9000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_9000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 2624/8188 [17:08<3:47:15,  2.45s/it, loss=2.8661, lr=2.76e-04, step=9500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.8004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 3622/8188 [23:24<28:32,  2.67it/s, loss=2.8194, lr=2.74e-04, step=1e+4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.7870\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 3624/8188 [23:39<3:21:19,  2.65s/it, loss=2.8194, lr=2.74e-04, step=1e+4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_10000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4624/8188 [30:08<2:25:33,  2.45s/it, loss=2.8632, lr=2.71e-04, step=10500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.7777\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 5622/8188 [36:24<16:02,  2.67it/s, loss=2.8964, lr=2.68e-04, step=11000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.7674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 5624/8188 [36:39<1:53:14,  2.65s/it, loss=2.8964, lr=2.68e-04, step=11000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_11000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 6624/8188 [43:08<1:04:02,  2.46s/it, loss=2.7124, lr=2.65e-04, step=11500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.7577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 7622/8188 [49:24<03:32,  2.67it/s, loss=2.7280, lr=2.62e-04, step=12000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.7499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 3:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 7624/8188 [49:39<24:52,  2.65s/it, loss=2.7280, lr=2.62e-04, step=12000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_12000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8188/8188 [53:11<00:00,  2.57it/s, loss=2.8848, lr=2.60e-04, step=12282]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3 complete - Avg loss: 2.8274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4:   5%|‚ñå         | 436/8188 [02:57<5:16:50,  2.45s/it, loss=2.7571, lr=2.59e-04, step=12500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.7406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4:  18%|‚ñà‚ñä        | 1434/8188 [09:13<42:15,  2.66it/s, loss=2.7287, lr=2.55e-04, step=13000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.7349\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 4:  18%|‚ñà‚ñä        | 1436/8188 [09:28<4:57:05,  2.64s/it, loss=2.7287, lr=2.55e-04, step=13000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_13000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4:  30%|‚ñà‚ñà‚ñâ       | 2436/8188 [15:57<3:54:54,  2.45s/it, loss=2.7125, lr=2.52e-04, step=13500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.7279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 3434/8188 [22:12<29:44,  2.66it/s, loss=2.7483, lr=2.48e-04, step=14000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.7223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 4:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 3436/8188 [22:27<3:29:37,  2.65s/it, loss=2.7483, lr=2.48e-04, step=14000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_14000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 4436/8188 [28:57<2:33:18,  2.45s/it, loss=2.7263, lr=2.45e-04, step=14500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.7135\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 5434/8188 [35:13<17:13,  2.66it/s, loss=2.7985, lr=2.41e-04, step=15000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.7097\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 4:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 5436/8188 [35:28<2:01:25,  2.65s/it, loss=2.7985, lr=2.41e-04, step=15000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_15000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 6436/8188 [41:57<1:11:37,  2.45s/it, loss=2.8543, lr=2.37e-04, step=15500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.7041\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 7435/8188 [48:13<04:11,  3.00it/s, loss=2.6264, lr=2.33e-04, step=16000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.6974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 4:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 7436/8188 [48:28<43:20,  3.46s/it, loss=2.6264, lr=2.33e-04, step=16000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_16000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8188/8188 [53:11<00:00,  2.57it/s, loss=2.7740, lr=2.30e-04, step=16376]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4 complete - Avg loss: 2.7630\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5:   3%|‚ñé         | 248/8188 [01:46<5:24:28,  2.45s/it, loss=2.7209, lr=2.29e-04, step=16500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.6899\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5:  15%|‚ñà‚ñå        | 1246/8188 [08:02<43:24,  2.67it/s, loss=2.6805, lr=2.25e-04, step=17000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.6849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 5:  15%|‚ñà‚ñå        | 1248/8188 [08:17<5:05:35,  2.64s/it, loss=2.6805, lr=2.25e-04, step=17000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_17000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5:  27%|‚ñà‚ñà‚ñã       | 2248/8188 [14:46<4:02:36,  2.45s/it, loss=2.7280, lr=2.21e-04, step=17500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.6794\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5:  40%|‚ñà‚ñà‚ñà‚ñâ      | 3246/8188 [21:02<30:54,  2.66it/s, loss=2.7333, lr=2.17e-04, step=18000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® New best validation loss: 2.6754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 5:  40%|‚ñà‚ñà‚ñà‚ñâ      | 3248/8188 [21:17<3:37:52,  2.65s/it, loss=2.7333, lr=2.17e-04, step=18000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_18000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 4247/8188 [27:34<25:35,  2.57it/s, loss=2.7553, lr=2.12e-04, step=18500]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ö†Ô∏è  Training stopped\n",
            "\n",
            "üíæ Saving final checkpoint...\n",
            "üíæ Checkpoint saved: checkpoints/checkpoint_step_18500.pt\n",
            "\n",
            "üéâ Training complete!\n",
            "üìä Total steps: 18,500\n",
            "üìä Total tokens: 2,424,832,000\n",
            "üìä Best validation loss: 2.6754\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tokens_seen</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train_loss</td><td>‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.00022</td></tr><tr><td>step</td><td>18000</td></tr><tr><td>tokens_seen</td><td>2424700928</td></tr><tr><td>train_loss</td><td>2.62974</td></tr><tr><td>val_loss</td><td>2.67538</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">medassist-86M</strong> at: <a href='https://wandb.ai/kunjcr2-dreamable/MedAssist-GPT-Pretraining/runs/380oe4z1' target=\"_blank\">https://wandb.ai/kunjcr2-dreamable/MedAssist-GPT-Pretraining/runs/380oe4z1</a><br> View project at: <a href='https://wandb.ai/kunjcr2-dreamable/MedAssist-GPT-Pretraining' target=\"_blank\">https://wandb.ai/kunjcr2-dreamable/MedAssist-GPT-Pretraining</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251110_023212-380oe4z1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ All done!\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Set random seeds\n",
        "    torch.manual_seed(TRAINING_CONFIG[\"seed\"])\n",
        "    np.random.seed(TRAINING_CONFIG[\"seed\"])\n",
        "\n",
        "    # Setup device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"üîß Using device: {device}\")\n",
        "\n",
        "    # Create save directory\n",
        "    save_dir = Path(\"./checkpoints\")\n",
        "    save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    print(\"üîß Loading tokenizer...\")\n",
        "    tokenizer = tiktoken.get_encoding(\"p50k_base\")\n",
        "\n",
        "    # Load and prepare data\n",
        "    train_tokens, val_tokens = prepare_medical_data(DATA_CONFIG, tokenizer)\n",
        "\n",
        "    # Create dataloaders\n",
        "    print(\"üîß Creating dataloaders...\")\n",
        "    train_loader = create_dataloader(\n",
        "        Path(\"/content/data_cache/train/\"),\n",
        "        batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
        "        # max_length=TRAINING_CONFIG[\"max_length\"],\n",
        "        # stride=TRAINING_CONFIG[\"stride\"],\n",
        "        shuffle=True,\n",
        "        num_workers=TRAINING_CONFIG[\"num_workers\"]\n",
        "    )\n",
        "\n",
        "    val_loader = create_dataloader(\n",
        "        Path(\"/content/data_cache/val/\"),\n",
        "        batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
        "        # max_length=TRAINING_CONFIG[\"max_length\"],\n",
        "        # stride=TRAINING_CONFIG[\"stride\"],\n",
        "        shuffle=False,\n",
        "        num_workers=TRAINING_CONFIG[\"num_workers\"]\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"üîß Initializing model...\")\n",
        "    model = MedAssistGPT(MODEL_CONFIG)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Compile model (PyTorch 2.0+)\n",
        "    if hasattr(torch, 'compile'):\n",
        "        print(\"üîß Compiling model...\")\n",
        "        model = torch.compile(model, mode=\"default\", fullgraph=False, dynamic=True)\n",
        "\n",
        "    print(f\"‚úÖ Model has {model.count_parameters():,} parameters\")\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=TRAINING_CONFIG[\"learning_rate\"],\n",
        "        weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n",
        "        betas=(TRAINING_CONFIG[\"beta1\"], TRAINING_CONFIG[\"beta2\"]),\n",
        "        eps=TRAINING_CONFIG[\"eps\"]\n",
        "    )\n",
        "\n",
        "    # Initialize scheduler\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=TRAINING_CONFIG[\"learning_rate\"],\n",
        "        total_steps=TRAINING_CONFIG[\"max_steps\"],\n",
        "        pct_start=TRAINING_CONFIG[\"warmup_steps\"] / TRAINING_CONFIG[\"max_steps\"],\n",
        "        anneal_strategy='cos',\n",
        "        div_factor=10,\n",
        "        final_div_factor=100\n",
        "    )\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(\n",
        "        project=WANDB_CONFIG[\"project\"],\n",
        "        entity=WANDB_CONFIG[\"entity\"],\n",
        "        name=WANDB_CONFIG[\"name\"],\n",
        "        config={**MODEL_CONFIG, **TRAINING_CONFIG, **DATA_CONFIG}\n",
        "    )\n",
        "\n",
        "    # Login to HuggingFace (if uploading)\n",
        "    if HF_CONFIG.get(\"upload_checkpoints\", False):\n",
        "        try:\n",
        "            login()\n",
        "            create_repo(HF_CONFIG[\"repo_id\"], repo_type=\"model\", exist_ok=True)\n",
        "            print(f\"‚úÖ HuggingFace repo ready: {HF_CONFIG['repo_id']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  HuggingFace setup failed: {e}\")\n",
        "            HF_CONFIG[\"upload_checkpoints\"] = False\n",
        "\n",
        "\n",
        "    # Train!\n",
        "    train(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        device=device,\n",
        "        config=TRAINING_CONFIG,\n",
        "        save_dir=save_dir,\n",
        "        hf_repo_id=HF_CONFIG[\"repo_id\"] if HF_CONFIG.get(\"upload_checkpoints\") else None\n",
        "    )\n",
        "\n",
        "    wandb.finish()\n",
        "    print(\"\\n‚úÖ All done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-AxM-tUwtH8"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3998a2c"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, inference_config: Dict[str, Any], device: str = 'cuda'):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    text = inference_config[\"prompt_text\"]\n",
        "    max_new_tokens = inference_config.get(\"max_new_tokens\", 50)\n",
        "    temperature = inference_config.get(\"temperature\", 0.8)\n",
        "\n",
        "    # Encode the input text\n",
        "    encoded_input = tokenizer.encode(text)\n",
        "    input_ids = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Take the last 'max_len' tokens if input_ids is longer\n",
        "            current_input_ids = input_ids if input_ids.size(1) <= model.config['max_len'] else input_ids[:, -model.config['max_len']:]\n",
        "\n",
        "            logits = model(current_input_ids) # (1, seq_len, vocab_size)\n",
        "            logits = logits[:, -1, :] # Take the logits for the last token (1, vocab_size)\n",
        "\n",
        "            # Apply temperature\n",
        "            if temperature == 0.0:\n",
        "                next_token = torch.argmax(logits, dim=-1).unsqueeze(1) # Ensure (1, 1) shape\n",
        "            else:\n",
        "                probs = torch.softmax(logits / temperature, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1) # Already (1, 1) shape\n",
        "\n",
        "            # Append the new token to the generated sequence\n",
        "            generated_tokens.append(next_token.item())\n",
        "            input_ids = torch.cat((input_ids, next_token), dim=1) # Removed .unsqueeze(0)\n",
        "\n",
        "            # Stop if endoftext token is generated\n",
        "            if next_token.item() == tokenizer.eot_token:\n",
        "                break\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    decoded_output = tokenizer.decode(generated_tokens)\n",
        "    return text + decoded_output\n",
        "\n",
        "def load_model_from_checkpoint(checkpoint_path: Path, model_config: Dict[str, Any], device: torch.device) -> nn.Module:\n",
        "    \"\"\"Loads a MedAssistGPT model from a checkpoint file.\"\"\"\n",
        "    print(f\"Loading model from {checkpoint_path}...\")\n",
        "    model = MedAssistGPT(model_config)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Fix: Remove '_orig_mod.' prefix from state_dict keys if present\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_orig_mod.'):\n",
        "            new_state_dict[k[len('_orig_mod.'):]] = v\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "\n",
        "    model.load_state_dict(new_state_dict)\n",
        "    model.to(device)\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    print(\"Model loaded successfully.\")\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "# First, set up device and tokenizer as they are needed for both loading and inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = tiktoken.get_encoding(\"p50k_base\")\n",
        "\n",
        "generated_output = []\n",
        "ckpt=1000\n",
        "while ckpt<=18500:\n",
        "    # Specify the path to your saved checkpoint\n",
        "    # Replace 'checkpoints/checkpoint_step_15000.pt' with the actual path if different\n",
        "    checkpoint_file_path = Path(f\"checkpoints/checkpoint_step_{ckpt}.pt\")\n",
        "    ckpt += 1000\n",
        "\n",
        "    # Load the model\n",
        "    loaded_model = load_model_from_checkpoint(checkpoint_file_path, MODEL_CONFIG, device)\n",
        "\n",
        "    # Now you can use the loaded_model with the generate_text function\n",
        "    generated_output.append(generate_text(loaded_model, tokenizer, INFERENCE_CONFIG, device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Maf0l_hKecvT",
        "outputId": "eda9e97f-8396-48b3-b45a-b992b6986546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint1000:\n",
            "To live a good life, the capacity for a person is a major challenge to the health and health system (; ). In addition, the general population has a high prevalence of disability and disability, with a high prevalence of disability, but it is estimated that about 10% of the population is at high risk. The total number of people living with disability in the country is increasing in the United States (; ).  Patients with dementia are at risk of developing dementia, and may also have a high risk of developing dementia\n",
            "\n",
            "\n",
            "Checkpoint2000:\n",
            "To live a good life expectancy, the weight gain in the middle-aged population is also an important factor in the development of the elderly. However, the benefits of weight loss in elderly people are not well understood.  In this study, we investigated the effects of weight loss on the elderly population. We hypothesized that the effects of weight loss on the elderly population, as well as the age-related health outcomes, were assessed.Introduction Hepatocellular carcinoma (HCC) is the most common cancer\n",
            "\n",
            "\n",
            "Checkpoint3000:\n",
            "To live a good life, and to determine its causes and mechanisms, it is necessary to identify the causes and mechanisms involved in the pathogenesis of this disease. The present study was undertaken to identify the risk factors and the associated genes in the pathogenesis of this disease.INTRODUCTION Cerebral palsy is a common cause of the disease and is the most common cause of dementia. It is a progressive neurodegenerative disease that results from a defect in the activity of the extracellular matrix (EC\n",
            "\n",
            "\n",
            "Checkpoint4000:\n",
            "To live a good life, it is possible to obtain a better understanding of the molecular mechanisms of the disease.  The goal of this work was to identify the molecular mechanisms that underlie the pathogenesis of the disease. The aims of this work were to identify the molecular mechanisms that contribute to the pathogenesis of the disease, and to identify the molecular mechanisms that regulate the pathogenesis of this disease.Introduction The incidence of diabetes mellitus (DM) is increasing worldwide, and its incidence has increased substantially in recent years\n",
            "\n",
            "\n",
            "Checkpoint5000:\n",
            "To live a good life style, the majority of the patients are dependent on the social support and support provided by the healthcare system.  In this context, social support as an important mediator of the quality of life of patients with chronic diseases is important to the quality of life of patients. Social support is a key factor in the quality of life of patients with chronic diseases. Social support is a key factor in the quality of life of patients with chronic diseases. Social support is a complex process that includes the activities of daily\n",
            "\n",
            "\n",
            "Checkpoint6000:\n",
            "To live a good life, it is essential to understand the mechanisms that are necessary and sufficient to ensure that the immune system is able to maintain homeostasis.  The innate immune system is an essential component of the immune system and is activated in response to invading pathogens. The innate immune system is able to recognize and respond to invading pathogens and to respond to pathogen-associated molecular patterns (PAMPs) in the presence of pathogen-associated molecular patterns (PAMPs). PAMPs are a family of\n",
            "\n",
            "\n",
            "Checkpoint7000:\n",
            "To live a good life in the world, the World Health Organization (WHO) has recommended the use of probiotic bacteria in the treatment of human diseases.  The use of probiotics in the treatment of various diseases has been growing. The use of probiotics has increased in recent years and this is the most commonly used recommendation in the treatment of various diseases.  The use of probiotics in the treatment of infectious diseases has led to the need for a new approach to treat infectious diseases.  The use of\n",
            "\n",
            "\n",
            "Checkpoint8000:\n",
            "To live a good life, a better understanding of the biology of the human body is essential for developing new treatments.  The development of animal models of human diseases has been recently reviewed (; ). However, the use of animal models is limited to human studies. Animal models can be used to study the pathogenesis of human diseases, such as cancer, diabetes, cardiovascular disease, and neurodegenerative diseases (; ). However, animal models are also used to study the molecular mechanisms of human diseases (; ).  \n",
            "\n",
            "\n",
            "Checkpoint9000:\n",
            "To live a good life, we will have to assume that the adult eukaryotic cells are not able to synthesize the essential nutrients in the form of energy.  In this study, we used a bioluminescence method, which is based on the bioluminescence of a fluorescent dye, in the cell. In this study, we used the bioluminescence method to measure the viability of cells in the presence of a fluorescent dye and also to study the effects of the fluorescent dye\n",
            "\n",
            "\n",
            "Checkpoint10000:\n",
            "To live a good life, we must understand how the CNS is able to respond to changes in the environment. The CNS is composed of neurons, glia, neurons, and glia, and the interactions between them are not fully understood. The CNS is composed of neurons and glia, and it is well-established that the CNS is divided into two regions: the forebrain and the spinal cord. The forebrain is the most important region for the development of the CNS. The forebrain is a complex structure of neurons\n",
            "\n",
            "\n",
            "Checkpoint11000:\n",
            "To live a good life, the host‚Äôs immune system is affected by the environment, the environment, the environment, and the parasite‚Äôs immune system. The immune system of the host is composed of a set of immune cells, which are the immune cells that are present in the body and that are present in the environment. The immune system plays an important role in host defense against infections, and the immune response is the most important factor in the host response against infections. The immune system, which includes the immune\n",
            "\n",
            "\n",
            "Checkpoint12000:\n",
            "To live a good life, the need for the development of new drugs that can be used to treat cancer has been increasing; however, the development of new drugs is still required.  The development of new drugs is a very important goal in the field of cancer research. The development of new drugs is a challenging task, and there are several approaches to develop new drugs. The most commonly used methods for the development of new drugs are the chemical synthesis of new compounds and computational approaches. The chemical synthesis of new drugs, such\n",
            "\n",
            "\n",
            "Checkpoint13000:\n",
            "To live a good life, we need to understand how the immune system reacts to stress and how it is affected. In this regard, the immune system has been studied extensively. In the past few years, the immune system has been studied in different species, including humans and mice. These studies have shown that mice are more susceptible to psychological stress and are more susceptible to stress-induced disorders. In the present study, we assessed the effect of oxidative stress on the immune system in mice. We used the immune system to study the\n",
            "\n",
            "\n",
            "Checkpoint14000:\n",
            "To live a good life, the growth of the organism is required to be tightly controlled. In the case of the yeast Saccharomyces cerevisiae, the cell cycle is tightly regulated by the cell cycle regulatory protein (Cdc6) that activates the activity of the Cdc6 kinase. Cdc6 is a Cdc6-specific protein that interacts with Cdc6 through its N-terminal domain. Cdc6 is a key regulator of the cell cycle. Cdc6 is a major regulator\n",
            "\n",
            "\n",
            "Checkpoint15000:\n",
            "To live a good life, the majority of the time is spent in the public sector, and the workforce is often limited to the public sector. In terms of health care, the public sector is no exception; health care is the most cost-effective way to provide services. The public sector is a significant source of health care, and it is also one of the most important sectors of the health care system. In the present study, we explored the prevalence of obesity among adults aged 20 to 64¬†years in a public hospital\n",
            "\n",
            "\n",
            "Checkpoint16000:\n",
            "To live a good life, this is a very important and challenging task. The nature of the human body, however, can be altered by the environment, such as the presence of a food source or the presence of other diseases. The CNS is not only a homeostatic system, but also a complex system that is affected by several factors including the physical environment, the environment, and the environment. The CNS is a complex system that is composed of several cell types, including neurons, glial cells, microglia,\n",
            "\n",
            "\n",
            "Checkpoint17000:\n",
            "To live a good life, it is crucial to understand the effects of a variety of factors, including social factors, on the development of mental health.  Social capital is defined as a network of social networks and is defined as a set of social relationships that individuals can access in the community and can be either direct or indirect. Social capital is defined as the degree to which individuals perceive themselves to be freely available to them. Social capital is defined as the extent to which individuals have an important role in the community and can be\n",
            "\n",
            "\n",
            "Checkpoint18000:\n",
            "To live a good life, it is crucial to understand the mechanism of the tumor cell invasion.  In the present study, we used the H2O2-induced breast cancer cell line MCF-7 to determine the role of the Wnt/Œ≤-catenin signaling pathway in the invasion of MCF-7 cells. We found that Wnt/Œ≤-catenin signaling pathway is involved in the invasion of MCF-7 cells and that the Wnt/Œ≤-catenin signaling\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i,res in enumerate(generated_output):\n",
        "    print(f\"Checkpoint{(i+1)*1000}:\\n{res.replace(\"\\n\", \" \")}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKcDk4focKxR"
      },
      "source": [
        "## Saving to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctSBRL6Aktf7"
      },
      "outputs": [],
      "source": [
        "import os, json, torch\n",
        "from safetensors.torch import save_file\n",
        "from huggingface_hub import create_repo, upload_folder, login\n",
        "\n",
        "login(token=\"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\")\n",
        "\n",
        "REPO_ID = \"kunjcr2/MedAssistGPT\"\n",
        "CKPT = \"/content/checkpoints/checkpoint_step_17000.pt\"\n",
        "OUT  = \"/content/medassistgpt_repo\"\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "# --- load checkpoint -> state_dict ---\n",
        "sd = torch.load(CKPT, map_location=\"cpu\")\n",
        "if isinstance(sd, dict):\n",
        "    for k in [\"state_dict\",\"model_state_dict\",\"module\",\"model\"]:\n",
        "        if k in sd and isinstance(sd[k], dict):\n",
        "            sd = sd[k]; break\n",
        "if hasattr(sd, \"state_dict\"): sd = sd.state_dict()\n",
        "\n",
        "# --- break shared storages (tied weights) ---\n",
        "def break_shared_storage(state_dict):\n",
        "    def storage_key(t):\n",
        "        # uniquely identify underlying storage\n",
        "        return (t.untyped_storage().data_ptr(), t.dtype, tuple(t.size()))\n",
        "    seen = {}\n",
        "    for name, t in list(state_dict.items()):\n",
        "        if not torch.is_tensor(t):\n",
        "            continue\n",
        "        key = storage_key(t)\n",
        "        if key in seen:\n",
        "            # clone to new storage to satisfy safetensors\n",
        "            state_dict[name] = t.clone()\n",
        "        else:\n",
        "            seen[key] = name\n",
        "    return state_dict\n",
        "\n",
        "sd = break_shared_storage(sd)\n",
        "\n",
        "# save safetensors\n",
        "save_file(sd, f\"{OUT}/model.safetensors\")\n",
        "\n",
        "# minimal configs (your values)\n",
        "import json\n",
        "config = {\n",
        "    \"model_type\": \"medassist_gpt\",\n",
        "    \"architectures\": [\"MedAssistGPTForCausalLM\"],\n",
        "    \"vocab_size\": 50281,\n",
        "    \"hidden_size\": 512,\n",
        "    \"num_attention_heads\": 16,\n",
        "    \"num_key_value_heads\": 4,\n",
        "    \"intermediate_size\": 2048,\n",
        "    \"num_hidden_layers\": 16,\n",
        "    \"max_position_embeddings\": 1024,\n",
        "    \"layer_norm_eps\": 1e-5,\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"attention_dropout_prob\": 0.1,\n",
        "    \"torch_dtype\": \"bfloat16\"\n",
        "}\n",
        "open(f\"{OUT}/config.json\",\"w\").write(json.dumps(config, indent=2))\n",
        "\n",
        "tok = {\"tokenizer_type\": \"tiktoken\", \"tiktoken_model\": \"p50k_base\"}\n",
        "open(f\"{OUT}/tokenizer_config.json\",\"w\").write(json.dumps(tok, indent=2))\n",
        "\n",
        "create_repo(REPO_ID, exist_ok=True, private=False)\n",
        "upload_folder(repo_id=REPO_ID, folder_path=OUT, commit_message=\"weights+config\")\n",
        "print(\"done\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HSugAirc_7-E",
        "QGI4c6FtCimw",
        "ducwfpL3ClaG",
        "Z_y_cRARCtMu",
        "PER5B_NY_i5d",
        "Ir1N-wXM_qBo",
        "L4a_G8lD_xIy",
        "1frbZzZJ_4LQ",
        "wD3wJU-rABeN",
        "2tUUoVFCAHNy",
        "kjLV-c66ANvw",
        "ivOe383gCxwC",
        "7TOcqHG5C3qx",
        "UXE9nD2bC86O"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05611c4dff5a4d96945997bd83a54725": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13d81780da124f3e95b4e2d47c55e2c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "25aaf39ee7604a8b99481ff9625a72f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28ca6df30078450cb0c952651cc1e7b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35e0b887b3034ab09d5d61845c18adce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3c263c0123644a6ebf1e498af83fe653": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e6ebf5ac4f946d49dac8c0a667faf88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_25aaf39ee7604a8b99481ff9625a72f3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3c263c0123644a6ebf1e498af83fe653",
            "value": ""
          }
        },
        "53381d7a401d4474a946319c168feb24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54afbe7a2ce942d390e0e88745d9ece4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8658b4ff5f31491f9ff3e2cd61807c64",
              "IPY_MODEL_4e6ebf5ac4f946d49dac8c0a667faf88",
              "IPY_MODEL_9870f24038a44ca9adb8039e4e4c765d",
              "IPY_MODEL_af0a437faeb449e38d844ff1b8065ee1",
              "IPY_MODEL_9c873dc69e324b3b859f6cb6b2a14ed3"
            ],
            "layout": "IPY_MODEL_13d81780da124f3e95b4e2d47c55e2c8"
          }
        },
        "7bb126813ff04f2d85c04471f31b7671": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8658b4ff5f31491f9ff3e2cd61807c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05611c4dff5a4d96945997bd83a54725",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_53381d7a401d4474a946319c168feb24",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "9870f24038a44ca9adb8039e4e4c765d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_ec4c5f7317944b298935cfb3b73e797f",
            "style": "IPY_MODEL_7bb126813ff04f2d85c04471f31b7671",
            "value": true
          }
        },
        "9c873dc69e324b3b859f6cb6b2a14ed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28ca6df30078450cb0c952651cc1e7b4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ab07bff393e847758b55c2912bc70fbf",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "ab07bff393e847758b55c2912bc70fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af0a437faeb449e38d844ff1b8065ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_eb06ec08d68a4c96a741d6c709e0921b",
            "style": "IPY_MODEL_35e0b887b3034ab09d5d61845c18adce",
            "tooltip": ""
          }
        },
        "eb06ec08d68a4c96a741d6c709e0921b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec4c5f7317944b298935cfb3b73e797f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
